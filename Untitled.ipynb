{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f779db-24c1-4bd3-9dce-bd444604678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\fastapi\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d1c4a0-43d4-4302-9eea-cfb4fa4a250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"all-mini-lm-l6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c84a0d8f-12c3-4ec5-a787-c8312e9e8dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86333b3b-b61d-43e6-ba6b-ca2cc397f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\", server_api=ServerApi('1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "255eb69b-f660-46ea-ba9c-0300ed6a8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileids = [\"\",\"110\",\"5\"]\n",
    "db = client[\"BOOKS\"]\n",
    "col = db[\"PdfStore\"]\n",
    "cursors = [col.find({\"fileid\":fileid}) for fileid in fileids]\n",
    "metadata = []\n",
    "for cursor in cursors:\n",
    "    for post in cursor:\n",
    "        metadata.append(post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7792e5c-18af-492b-bffb-24ad41684bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('66912837a6a51b460ad40199'),\n",
       " 'filename': 'sample.pdf',\n",
       " 'fileid': '110',\n",
       " 'topic': 'sample',\n",
       " 'hash': '36c9b76fb105ed3da937316756d5536c',\n",
       " 'fs_id': ObjectId('66912837a6a51b460ad40197'),\n",
       " 'vector_id': 'a1238853-9bc6-41e7-b8c8-cc332b93b574'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25b4566b-5984-42f5-a195-f07d4b464850",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f480115a-6c41-4b64-813e-3cbbfd8c25e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\fastapi\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedding_name = \"D:\\\\fastapi\\\\embedding_model\\\\all-mini-lm-l6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6df76e0-e11c-4175-a2a7-97ef6d44ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(r\"D:\\fastapi\\vectorstore\",embeddings,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b480854d-ff8d-4363-8726-fb7ccf8663db",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (984120185.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[38], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    db.delete(db.delete(db.index_to_docstore_id[i])\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,db.index.ntotal):\n",
    "    db.delete(db.delete(db.index_to_docstore_id[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce766fc3-22b1-49a9-80cd-e58aabd64e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18c89385-8ba4-4d35-8658-e2de07d15d6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.delete([db.index_to_docstore_id[i] for i in range(1,db.index.ntotal)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b50b039-c80a-4533-ade6-f3f5a41d3284",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FAISS' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m([db\u001b[38;5;241m.\u001b[39mindex_to_docstore_id[\u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FAISS' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "db.get([db.index_to_docstore_id[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb38e58b-d547-4065-92de-a5b149f65a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b2c416e-3dfe-4b82-909f-4f8137871fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(r\"D:\\fastapi\\vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa879b41-760a-41a5-abbc-4b2de9a566a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbbeff36-d237-4663-9453-405288595f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22335f-6f0c-47b2-ab54-8425e2291b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "994b105c-4279-49c4-abf6-06cbd1bf83a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PyPDFLoader(r\"C:\\Users\\itsup\\Downloads\\Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2016, MIT Press) - libgen.lc.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d12cdf-7d6c-4536-84ab-8e2616a01b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"documents/Car.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a28bf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c87e090-3b5f-49ff-ab0f-614dbe88986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PyPDF2 import PdfReader\n",
    "\n",
    "# pdf_reader = PdfReader(r\"C:\\Users\\itsup\\Downloads\\Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2016, MIT Press) - libgen.lc.pdf\")\n",
    "# text = \"\"\n",
    "# for no,page in enumerate(pdf_reader.pages):\n",
    "#     try:\n",
    "#         text+= page.extract_text(x_tolerance=1)+\"\\n\"\n",
    "#     except Exception as exe:\n",
    "#         print(f\"Error in parsing content page: {no} error {exe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "177fe68b-1ae6-4bea-a7b5-f386ac42f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c7eccc-0073-4ec8-8958-7990ffb42ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "124.78852653503418\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import time\n",
    "start = time.time()\n",
    "with pdfplumber.open(r\"temp/deeplearning-ian_goodfellow.pdf\") as pdf:\n",
    "    text = \"\"\n",
    "    \n",
    "    for n,page in enumerate(pdf.pages):\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "        print(n)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "#print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e5be618",
   "metadata": {},
   "outputs": [],
   "source": [
    "textsplitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap=0)\n",
    "doc = Document(page_content = text, metadata = {\"filename\":\"dee.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35e7df79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'filename': 'dee.txt'}, page_content='Deep Learning\\nIan Goodfellow\\nYoshua Bengio\\nAaron Courville\\nContents\\nWebsite vii\\nAcknowledgments viii\\nNotation xi\\n1 Introduction 1\\n1.1 Who Should Read This Book? . . . . . . . . . . . . . . . . . . . . 8\\n1.2 Historical Trends in Deep Learning . . . . . . . . . . . . . . . . . 11\\nI Applied Math and Machine Learning Basics 29\\n2 Linear Algebra 31\\n2.1 Scalars, Vectors, Matrices and Tensors . . . . . . . . . . . . . . . 31\\n2.2 Multiplying Matrices and Vectors . . . . . . . . . . . . . . . . . . 34\\n2.3 Identity and Inverse Matrices . . . . . . . . . . . . . . . . . . . . 36\\n2.4 Linear Dependence and Span . . . . . . . . . . . . . . . . . . . . 37\\n2.5 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n2.6 Special Kinds of Matrices and Vectors . . . . . . . . . . . . . . . 40\\n2.7 Eigendecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n2.8 Singular Value Decomposition . . . . . . . . . . . . . . . . . . . . 44'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='2.9 The Moore-Penrose Pseudoinverse . . . . . . . . . . . . . . . . . . 45\\n2.10 The Trace Operator . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n2.11 The Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n2.12 Example: Principal Components Analysis . . . . . . . . . . . . . 48\\n3 Probability and Information Theory 53\\n3.1 Why Probability? . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\ni\\nCONTENTS\\n3.2 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n3.3 Probability Distributions . . . . . . . . . . . . . . . . . . . . . . . 56\\n3.4 Marginal Probability . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n3.5 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . 59\\n3.6 The Chain Rule of Conditional Probabilities . . . . . . . . . . . . 59\\n3.7 Independence and Conditional Independence . . . . . . . . . . . . 60\\n3.8 Expectation, Variance and Covariance . . . . . . . . . . . . . . . 60'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='3.9 Common Probability Distributions . . . . . . . . . . . . . . . . . 62\\n3.10 Useful Properties of Common Functions . . . . . . . . . . . . . . 67\\n3.11 Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\\n3.12 Technical Details of Continuous Variables . . . . . . . . . . . . . 71\\n3.13 Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 73\\n3.14 Structured Probabilistic Models . . . . . . . . . . . . . . . . . . . 75\\n4 Numerical Computation 80\\n4.1 Overflow and Underflow . . . . . . . . . . . . . . . . . . . . . . . 80\\n4.2 Poor Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . 82\\n4.3 Gradient-Based Optimization . . . . . . . . . . . . . . . . . . . . 82\\n4.4 Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . 93\\n4.5 Example: Linear Least Squares . . . . . . . . . . . . . . . . . . . 96\\n5 Machine Learning Basics 98\\n5.1 Learning Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 99'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='5.2 Capacity, Overfitting and Underfitting . . . . . . . . . . . . . . . 110\\n5.3 Hyperparameters and Validation Sets . . . . . . . . . . . . . . . . 120\\n5.4 Estimators, Bias and Variance . . . . . . . . . . . . . . . . . . . . 122\\n5.5 Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . 131\\n5.6 Bayesian Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n5.7 Supervised Learning Algorithms . . . . . . . . . . . . . . . . . . . 140\\n5.8 Unsupervised Learning Algorithms . . . . . . . . . . . . . . . . . 146\\n5.9 Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . 151\\n5.10 Building a Machine Learning Algorithm . . . . . . . . . . . . . . 153\\n5.11 Challenges Motivating Deep Learning . . . . . . . . . . . . . . . . 155\\nII Deep Networks: Modern Practices 166\\n6 Deep Feedforward Networks 168\\n6.1 Example: Learning XOR . . . . . . . . . . . . . . . . . . . . . . . 171\\n6.2 Gradient-Based Learning . . . . . . . . . . . . . . . . . . . . . . . 177\\nii'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CONTENTS\\n6.3 Hidden Units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\\n6.4 Architecture Design . . . . . . . . . . . . . . . . . . . . . . . . . . 197\\n6.5 Back-Propagation and Other Differentiation Algorithms . . . . . 204\\n6.6 Historical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224\\n7 Regularization for Deep Learning 228\\n7.1 Parameter Norm Penalties . . . . . . . . . . . . . . . . . . . . . . 230\\n7.2 Norm Penalties as Constrained Optimization . . . . . . . . . . . . 237\\n7.3 Regularization and Under-Constrained Problems . . . . . . . . . 239\\n7.4 Dataset Augmentation . . . . . . . . . . . . . . . . . . . . . . . . 240\\n7.5 Noise Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\\n7.6 Semi-Supervised Learning . . . . . . . . . . . . . . . . . . . . . . 243\\n7.7 Multi-Task Learning . . . . . . . . . . . . . . . . . . . . . . . . . 244\\n7.8 Early Stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='7.9 Parameter Tying and Parameter Sharing . . . . . . . . . . . . . . 253\\n7.10 Sparse Representations . . . . . . . . . . . . . . . . . . . . . . . . 254\\n7.11 Bagging and Other Ensemble Methods . . . . . . . . . . . . . . . 256\\n7.12 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\\n7.13 Adversarial Training . . . . . . . . . . . . . . . . . . . . . . . . . 268\\n7.14 Tangent Distance, Tangent Prop, and Manifold Tangent Classifier 270\\n8 Optimization for Training Deep Models 274\\n8.1 How Learning Differs from Pure Optimization . . . . . . . . . . . 275\\n8.2 Challenges in Neural Network Optimization . . . . . . . . . . . . 282\\n8.3 Basic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 294\\n8.4 Parameter Initialization Strategies . . . . . . . . . . . . . . . . . 301\\n8.5 Algorithms with Adaptive Learning Rates . . . . . . . . . . . . . 306\\n8.6 Approximate Second-Order Methods . . . . . . . . . . . . . . . . 310'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='8.7 Optimization Strategies and Meta-Algorithms . . . . . . . . . . . 317\\n9 Convolutional Networks 330\\n9.1 The Convolution Operation . . . . . . . . . . . . . . . . . . . . . 331\\n9.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\n9.3 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\\n9.4 Convolution and Pooling as an Infinitely Strong Prior . . . . . . . 345\\n9.5 Variants of the Basic Convolution Function . . . . . . . . . . . . 347\\n9.6 Structured Outputs . . . . . . . . . . . . . . . . . . . . . . . . . . 358\\n9.7 Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360\\n9.8 Efficient Convolution Algorithms . . . . . . . . . . . . . . . . . . 362\\n9.9 Random or Unsupervised Features . . . . . . . . . . . . . . . . . 363\\niii\\nCONTENTS\\n9.10 The Neuroscientific Basis for Convolutional Networks . . . . . . . 364\\n9.11 Convolutional Networks and the History of Deep Learning . . . . 371'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='10 Sequence Modeling: Recurrent and Recursive Nets 373\\n10.1 Unfolding Computational Graphs . . . . . . . . . . . . . . . . . . 375\\n10.2 Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . 378\\n10.3 Bidirectional RNNs . . . . . . . . . . . . . . . . . . . . . . . . . . 394\\n10.4 Encoder-Decoder Sequence-to-Sequence Architectures . . . . . . . 396\\n10.5 Deep Recurrent Networks . . . . . . . . . . . . . . . . . . . . . . 398\\n10.6 Recursive Neural Networks . . . . . . . . . . . . . . . . . . . . . . 400\\n10.7 The Challenge of Long-Term Dependencies . . . . . . . . . . . . . 401\\n10.8 Echo State Networks . . . . . . . . . . . . . . . . . . . . . . . . . 404\\n10.9 Leaky Units and Other Strategies for Multiple Time Scales . . . . 406\\n10.10 The Long Short-Term Memory and Other Gated RNNs . . . . . . 408\\n10.11 Optimization for Long-Term Dependencies . . . . . . . . . . . . . 413\\n10.12 Explicit Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . 416'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='11 Practical Methodology 421\\n11.1 Performance Metrics . . . . . . . . . . . . . . . . . . . . . . . . . 422\\n11.2 Default Baseline Models . . . . . . . . . . . . . . . . . . . . . . . 425\\n11.3 Determining Whether to Gather More Data . . . . . . . . . . . . 426\\n11.4 Selecting Hyperparameters . . . . . . . . . . . . . . . . . . . . . . 427\\n11.5 Debugging Strategies . . . . . . . . . . . . . . . . . . . . . . . . . 436\\n11.6 Example: Multi-Digit Number Recognition . . . . . . . . . . . . . 440\\n12 Applications 443\\n12.1 Large-Scale Deep Learning . . . . . . . . . . . . . . . . . . . . . . 443\\n12.2 Computer Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . 452\\n12.3 Speech Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . 458\\n12.4 Natural Language Processing . . . . . . . . . . . . . . . . . . . . 461\\n12.5 Other Applications . . . . . . . . . . . . . . . . . . . . . . . . . . 478\\nIII Deep Learning Research 486\\n13 Linear Factor Models 489'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='13.1 Probabilistic PCA and Factor Analysis . . . . . . . . . . . . . . . 490\\n13.2 Independent Component Analysis (ICA) . . . . . . . . . . . . . . 491\\n13.3 Slow Feature Analysis . . . . . . . . . . . . . . . . . . . . . . . . 493\\n13.4 Sparse Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496\\niv\\nCONTENTS\\n13.5 Manifold Interpretation of PCA . . . . . . . . . . . . . . . . . . . 499\\n14 Autoencoders 502\\n14.1 Undercomplete Autoencoders . . . . . . . . . . . . . . . . . . . . 503\\n14.2 Regularized Autoencoders . . . . . . . . . . . . . . . . . . . . . . 504\\n14.3 Representational Power, Layer Size and Depth . . . . . . . . . . . 508\\n14.4 Stochastic Encoders and Decoders . . . . . . . . . . . . . . . . . . 509\\n14.5 Denoising Autoencoders . . . . . . . . . . . . . . . . . . . . . . . 510\\n14.6 Learning Manifolds with Autoencoders . . . . . . . . . . . . . . . 515\\n14.7 Contractive Autoencoders . . . . . . . . . . . . . . . . . . . . . . 521'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='14.8 Predictive Sparse Decomposition . . . . . . . . . . . . . . . . . . 523\\n14.9 Applications of Autoencoders . . . . . . . . . . . . . . . . . . . . 524\\n15 Representation Learning 526\\n15.1 Greedy Layer-Wise Unsupervised Pretraining . . . . . . . . . . . 528\\n15.2 Transfer Learning and Domain Adaptation . . . . . . . . . . . . . 536\\n15.3 Semi-Supervised Disentangling of Causal Factors . . . . . . . . . 541\\n15.4 Distributed Representation . . . . . . . . . . . . . . . . . . . . . . 546\\n15.5 Exponential Gains from Depth . . . . . . . . . . . . . . . . . . . 553\\n15.6 Providing Clues to Discover Underlying Causes . . . . . . . . . . 554\\n16 Structured Probabilistic Models for Deep Learning 558\\n16.1 The Challenge of Unstructured Modeling . . . . . . . . . . . . . . 559\\n16.2 Using Graphs to Describe Model Structure . . . . . . . . . . . . . 563\\n16.3 Sampling from Graphical Models . . . . . . . . . . . . . . . . . . 580'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='16.4 Advantages of Structured Modeling . . . . . . . . . . . . . . . . . 582\\n16.5 Learning about Dependencies . . . . . . . . . . . . . . . . . . . . 582\\n16.6 Inference and Approximate Inference . . . . . . . . . . . . . . . . 584\\n16.7 The Deep Learning Approach to Structured Probabilistic Models 585\\n17 Monte Carlo Methods 590\\n17.1 Sampling and Monte Carlo Methods . . . . . . . . . . . . . . . . 590\\n17.2 Importance Sampling . . . . . . . . . . . . . . . . . . . . . . . . . 592\\n17.3 Markov Chain Monte Carlo Methods . . . . . . . . . . . . . . . . 595\\n17.4 Gibbs Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599\\n17.5 The Challenge of Mixing between Separated Modes . . . . . . . . 599\\n18 Confronting the Partition Function 605\\n18.1 The Log-Likelihood Gradient . . . . . . . . . . . . . . . . . . . . 606\\n18.2 Stochastic Maximum Likelihood and Contrastive Divergence . . . 607\\nv\\nCONTENTS\\n18.3 Pseudolikelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . 615'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='18.4 Score Matching and Ratio Matching . . . . . . . . . . . . . . . . 617\\n18.5 Denoising Score Matching . . . . . . . . . . . . . . . . . . . . . . 619\\n18.6 Noise-Contrastive Estimation . . . . . . . . . . . . . . . . . . . . 620\\n18.7 Estimating the Partition Function . . . . . . . . . . . . . . . . . . 623\\n19 Approximate Inference 631\\n19.1 Inference as Optimization . . . . . . . . . . . . . . . . . . . . . . 633\\n19.2 Expectation Maximization . . . . . . . . . . . . . . . . . . . . . . 634\\n19.3 MAP Inference and Sparse Coding . . . . . . . . . . . . . . . . . 635\\n19.4 Variational Inference and Learning . . . . . . . . . . . . . . . . . 638\\n19.5 Learned Approximate Inference . . . . . . . . . . . . . . . . . . . 651\\n20 Deep Generative Models 654\\n20.1 Boltzmann Machines . . . . . . . . . . . . . . . . . . . . . . . . . 654\\n20.2 Restricted Boltzmann Machines . . . . . . . . . . . . . . . . . . . 656\\n20.3 Deep Belief Networks . . . . . . . . . . . . . . . . . . . . . . . . . 660'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='20.4 Deep Boltzmann Machines . . . . . . . . . . . . . . . . . . . . . . 663\\n20.5 Boltzmann Machines for Real-Valued Data . . . . . . . . . . . . . 676\\n20.6 Convolutional Boltzmann Machines . . . . . . . . . . . . . . . . . 683\\n20.7 Boltzmann Machines for Structured or Sequential Outputs . . . . 685\\n20.8 Other Boltzmann Machines . . . . . . . . . . . . . . . . . . . . . 686\\n20.9 Back-Propagation through Random Operations . . . . . . . . . . 687\\n20.10 Directed Generative Nets . . . . . . . . . . . . . . . . . . . . . . . 692\\n20.11 Drawing Samples from Autoencoders . . . . . . . . . . . . . . . . 711\\n20.12 Generative Stochastic Networks . . . . . . . . . . . . . . . . . . . 714\\n20.13 Other Generation Schemes . . . . . . . . . . . . . . . . . . . . . . 716\\n20.14 Evaluating Generative Models . . . . . . . . . . . . . . . . . . . . 717\\n20.15 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 720\\nBibliography 721\\nIndex 777\\nvi\\nWebsite\\nwww.deeplearningbook.org'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='This book is accompanied by the above website. The website provides a\\nvariety of supplementary material, including exercises, lecture slides, corrections of\\nmistakes, and other resources that should be useful to both readers and instructors.\\nvii\\nAcknowledgments\\nThis book would not have been possible without the contributions of many people.\\nWe would like to thank those who commented on our proposal for the book\\nand helped plan its contents and organization: Guillaume Alain, Kyunghyun Cho,\\nÇağlar Gülçehre, David Krueger, Hugo Larochelle, Razvan Pascanu and Thomas\\nRohée.\\nWe would like to thank the people who offered feedback on the content of the\\nbook itself. Some offered feedback on many chapters: Martín Abadi, Guillaume\\nAlain, Ion Androutsopoulos, Fred Bertsch, Olexa Bilaniuk, Ufuk Can Biçici, Matko\\nBošnjak, John Boersma, Greg Brockman, Alexandre de Brébisson, Pierre Luc\\nCarrier, Sarath Chandar, Pawel Chilinski, Mark Daoust, Oleg Dashevskii, Laurent'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Dinh, Stephan Dreseitl, Jim Fan, Miao Fan, Meire Fortunato, Frédéric Francis,\\nNando de Freitas, Çağlar Gülçehre, Jurgen Van Gael, Javier Alonso García,\\nJonathan Hunt, Gopi Jeyaram, Chingiz Kabytayev, Lukasz Kaiser, Varun Kanade,\\nAsifullah Khan, Akiel Khan, John King, Diederik P. Kingma, Yann LeCun, Rudolf\\nMathey, Matías Mattamala, Abhinav Maurya, Kevin Murphy, Oleg Mürk, Roman\\nNovak, Augustus Q. Odena, Simon Pavlik, Karl Pichotta, Eddie Pierce, Kari Pulli,\\nRoussel Rahman, Tapani Raiko, Anurag Ranjan, Johannes Roith, Mihaela Rosca,\\nHalis Sak, César Salgado, Grigory Sapunov, Yoshinori Sasaki, Mike Schuster,\\nJulian Serban, Nir Shabat, Ken Shirriff, Andre Simpelo, Scott Stanley, David\\nSussillo, Ilya Sutskever, Carles Gelada Sáez, Graham Taylor, Valentin Tolmer,\\nMassimiliano Tomassoli, An Tran, Shubhendu Trivedi, Alexey Umnov, Vincent\\nVanhoucke, Marco Visentini-Scarzanella, Martin Vita, David Warde-Farley, Dustin\\nWebb, Kelvin Xu, Wei Xue, Ke Yang, Li Yao, Zygmunt Zając and Ozan Çağlayan.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='We would also like to thank those who provided us with useful feedback on\\nindividual chapters:\\nNotation: Zhang Yuanhang.\\n•\\nChapter 1, Introduction: Yusuf Akgul, Sebastien Bratieres, Samira Ebrahimi,\\n•\\nviii\\nCONTENTS\\nCharlie Gorichanaz, Brendan Loudermilk, Eric Morris, Cosmin Pârvulescu\\nand Alfredo Solano.\\nChapter 2, Linear Algebra: Amjad Almahairi, Nikola Banić, Kevin Bennett,\\n•\\nPhilippe Castonguay, Oscar Chang, Eric Fosler-Lussier, Andrey Khalyavin,\\nSergey Oreshkov, István Petrás, Dennis Prangle, Thomas Rohée, Gitanjali\\nGulve Sehgal, Colby Toland, Alessandro Vitale and Bob Welland.\\nChapter 3, Probability and Information Theory: John Philip Anderson, Kai\\n•\\nArulkumaran, Vincent Dumoulin, Rui Fa, Stephan Gouws, Artem Oboturov,\\nAntti Rasmus, Alexey Surkov and Volker Tresp.\\nChapter 4, Numerical Computation: Tran Lam AnIan Fischer and Hu\\n•\\nYuhuang.\\nChapter 5, Machine Learning Basics: Dzmitry Bahdanau, Justin Domingue,\\n•\\nNikhil Garg, Makoto Otsuka, Bob Pepin, Philip Popien, Emmanuel Rayner,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Peter Shepard, Kee-Bong Song, Zheng Sun and Andy Wu.\\nChapter 6, Deep Feedforward Networks: Uriel Berdugo, Fabrizio Bottarel,\\n•\\nElizabeth Burl, Ishan Durugkar, Jeff Hlywa, Jong Wook Kim, David Krueger\\nand Aditya Kumar Praharaj.\\nChapter 7, Regularization for Deep Learning: Morten Kolbæk, Kshitij Lauria,\\n•\\nInkyu Lee, Sunil Mohan, Hai Phong Phan and Joshua Salisbury.\\nChapter 8, Optimization for Training Deep Models: Marcel Ackermann, Peter\\n•\\nArmitage, Rowel Atienza, Andrew Brock, Tegan Maharaj, James Martens,\\nKashif Rasul, Klaus Strobl and Nicholas Turner.\\nChapter 9, Convolutional Networks: Martín Arjovsky, Eugene Brevdo, Kon-\\n•\\nstantin Divilov, Eric Jensen, Mehdi Mirza, Alex Paino, Marjorie Sayer, Ryan\\nStout and Wentao Wu.\\nChapter 10, Sequence Modeling: Recurrent and Recursive Nets: Gökçen\\n•\\nEraslan, Steven Hickson, Razvan Pascanu, Lorenzo von Ritter, Rui Rodrigues,\\nDmitriy Serdyuk, Dongyu Shi and Kaiyu Yang.\\nChapter 11, Practical Methodology: Daniel Beckstein.\\n•'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Chapter 12, Applications: George Dahl, Vladimir Nekrasov and Ribana\\n•\\nRoscher.\\nChapter 13, Linear Factor Models: Jayanth Koushik.\\n•\\nix\\nCONTENTS\\nChapter 15, Representation Learning: Kunal Ghosh.\\n•\\nChapter 16, Structured Probabilistic Models for Deep Learning: Minh Lê\\n•\\nand Anton Varfolom.\\nChapter 18, Confronting the Partition Function: Sam Bowman.\\n•\\nChapter 19, Approximate Inference: Yujia Bao.\\n•\\nChapter 20, Deep Generative Models: Nicolas Chapados, Daniel Galvez,\\n•\\nWenming Ma, Fady Medhat, Shakir Mohamed and Grégoire Montavon.\\nBibliography: Lukas Michelbacher and Leslie N. Smith.\\n•\\nWe also want to thank those who allowed us to reproduce images, figures or\\ndata from their publications. We indicate their contributions in the figure captions\\nthroughout the text.\\nWe would like to thank Lu Wang for writing pdf2htmlEX, which we used to\\nmake the web version of the book, and for offering support to improve the quality\\nof the resulting HTML.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='We would like to thank Ian’s wife Daniela Flori Goodfellow for patiently\\nsupporting Ian during the writing of the book as well as for help with proofreading.\\nWe would like to thank the Google Brain team for providing an intellectual\\nenvironment where Ian could devote a tremendous amount of time to writing this\\nbook and receive feedback and guidance from colleagues. We would especially like\\nto thank Ian’s former manager, Greg Corrado, and his current manager, Samy\\nBengio, for their support of this project. Finally, we would like to thank Geoffrey\\nHinton for encouragement when writing was difficult.\\nx\\nNotation\\nThis section provides a concise reference describing the notation used throughout\\nthis book. If you are unfamiliar with any of the corresponding mathematical\\nconcepts, we describe most of these ideas in chapters 2–4.\\nNumbers and Arrays\\na A scalar (integer or real)\\na A vector\\nA A matrix\\nA\\nA tensor\\nI Identity matrix with n rows and n columns\\nn'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='I Identity matrix with dimensionality implied by\\ncontext\\ne(i) Standard basis vector [0,...,0,1,0,...,0] with a\\n1 at position i\\ndiag(a) A square, diagonal matrix with diagonal entries\\ngiven by a\\na A scalar random variable\\na A vector-valued random variable\\nA A matrix-valued random variable\\nxi\\nCONTENTS\\nSets and Graphs\\nA\\nA set\\nR\\nThe set of real numbers\\n0,1 The set containing 0 and 1\\n{ }\\n0,1,...,n The set of all integers between 0 and n\\n{ }\\n[a,b] The real interval including a and b\\n(a,b] The real interval excluding a but including b\\nA B\\nSet subtraction, i.e., the set containing the ele-\\n\\\\\\nA B\\nments of that are not in\\nA graph\\nG\\nPa (x ) The parents of x in\\ni i\\nG G\\nIndexing\\na Element i of vector a, with indexing starting at 1\\ni\\na All elements of vector a except for element i\\ni\\n−\\nA Element i,j of matrix A\\ni,j\\nA Row i of matrix A\\ni,:\\nA Column i of matrix A\\n:,i\\nA Element (i,j,k) of a 3-D tensor A\\ni,j,k\\nA\\n2-D slice of a 3-D tensor\\n:,:,i\\na Element i of the random vector a\\ni'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Linear Algebra Operations\\nA Transpose of matrix A\\n\\ue03e\\nA+ Moore-Penrose pseudoinverse of A\\nA B Element-wise (Hadamard) product of A and B\\n\\ue00c\\ndet(A) Determinant of A\\nxii\\nCONTENTS\\nCalculus\\ndy\\nDerivative of y with respect to x\\ndx\\n∂y\\nPartial derivative of y with respect to x\\n∂x\\ny Gradient of y with respect to x\\nx\\n∇\\ny Matrix derivatives of y with respect to X\\nX\\n∇\\nXy Tensor containing derivatives of y with respect to\\n∇\\nX\\n∂f\\nJacobian matrix J Rm n of f : Rn Rm\\n×\\n∂x ∈ →\\n2f(x) or H(f)(x) The Hessian matrix of f at input point x\\nx\\n∇\\nf(x)dx Definite integral over the entire domain of x\\n\\ue05a\\nS\\nf(x)dx Definite integral with respect to x over the set\\nS\\n\\ue05a\\nProbability and Information Theory\\na b The random variables a and b are independent\\n⊥\\na b c They are conditionally independent given c\\n⊥ |\\nP(a) A probability distribution over a discrete variable\\np(a) A probability distribution over a continuous vari-\\nable, or over a variable whose type has not been\\nspecified\\na P Random variable a has distribution P\\n∼\\nE E'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='[f(x)] or f(x) Expectation of f(x) with respect to P(x)\\nx P\\n∼\\nVar(f(x)) Variance of f(x) under P(x)\\nCov(f(x),g(x)) Covariance of f(x) and g(x) under P(x)\\nH(x) Shannon entropy of the random variable x\\nD (P Q) Kullback-Leibler divergence of P and Q\\nKL\\n\\ue06b\\n(x;µ,Σ) Gaussian distribution over x with mean µ and\\nN\\ncovariance Σ\\nxiii\\nCONTENTS\\nFunctions\\nA B A B\\nf : The function f with domain and range\\n→\\nf g Composition of the functions f and g\\n◦\\nf(x;θ) A function of x parametrized by θ. (Sometimes\\nwe write f(x) and omit the argument θ to lighten\\nnotation)\\nlogx Natural logarithm of x\\n1\\nσ(x) Logistic sigmoid,\\n1+exp( x)\\n−\\nζ(x) Softplus, log(1+exp(x))\\nx Lp norm of x\\np\\n|| ||\\nx L2 norm of x\\n|| ||\\nx+ Positive part of x, i.e., max(0,x)\\n1 is 1 if the condition is true, 0 otherwise\\ncondition\\nSometimes we use a function f whose argument is a scalar but apply it to a\\nX\\nvector, matrix, or tensor: f(x), f(X), or f( ). This denotes the application of f\\nC X C X'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to the array element-wise. For example, if = σ( ), then = σ( ) for all\\ni,j,k i,j,k\\nvalid values of i, j and k.\\nDatasets and Distributions\\np The data generating distribution\\ndata\\npˆ The empirical distribution defined by the training\\ndata\\nset\\nX\\nA set of training examples\\nx(i) The i-th example (input) from a dataset\\ny(i) or y(i) Thetargetassociatedwith x(i)forsupervisedlearn-\\ning\\nX The m n matrix with input example x(i) in row\\n×\\nX\\ni,:\\nxiv\\nChapter 1\\nIntroduction\\nInventors have long dreamed of creating machines that think. This desire dates\\nback to at least the time of ancient Greece. The mythical figures Pygmalion,\\nDaedalus, and Hephaestus may all be interpreted as legendary inventors, and\\nGalatea, Talos, and Pandora may all be regarded as artificial life (Ovid and Martin,\\n2004; Sparkes, 1996; Tandy, 1997).\\nWhen programmable computers were first conceived, people wondered whether\\nsuch machines might become intelligent, over a hundred years before one was'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='built (Lovelace, 1842). Today, artificial intelligence (AI) is a thriving field with\\nmany practical applications and active research topics. We look to intelligent\\nsoftware to automate routine labor, understand speech or images, make diagnoses\\nin medicine and support basic scientific research.\\nIn the early days of artificial intelligence, the field rapidly tackled and solved\\nproblems that are intellectually difficult for human beings but relatively straight-\\nforward for computers—problems that can be described by a list of formal, math-\\nematical rules. The true challenge to artificial intelligence proved to be solving\\nthe tasks that are easy for people to perform but hard for people to describe\\nformally—problems that we solve intuitively, that feel automatic, like recognizing\\nspoken words or faces in images.\\nThis book is about a solution to these more intuitive problems. This solution is\\nto allow computers to learn from experience and understand the world in terms of a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='hierarchy of concepts, with each concept defined in terms of its relation to simpler\\nconcepts. By gathering knowledge from experience, this approach avoids the need\\nfor human operators to formally specify all of the knowledge that the computer\\nneeds. The hierarchy of concepts allows the computer to learn complicated concepts\\nby building them out of simpler ones. If we draw a graph showing how these\\n1\\nCHAPTER 1. INTRODUCTION\\nconcepts are built on top of each other, the graph is deep, with many layers. For\\nthis reason, we call this approach to AI deep learning.\\nMany of the early successes of AI took place in relatively sterile and formal\\nenvironments and did not require computers to have much knowledge about\\nthe world. For example, IBM’s Deep Blue chess-playing system defeated world\\nchampion Garry Kasparov in 1997 (Hsu, 2002). Chess is of course a very simple\\nworld, containing only sixty-four locations and thirty-two pieces that can move'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='in only rigidly circumscribed ways. Devising a successful chess strategy is a\\ntremendous accomplishment, but the challenge is not due to the difficulty of\\ndescribing the set of chess pieces and allowable moves to the computer. Chess\\ncan be completely described by a very brief list of completely formal rules, easily\\nprovided ahead of time by the programmer.\\nIronically, abstract and formal tasks that are among the most difficult mental\\nundertakings for a human being are among the easiest for a computer. Computers\\nhave long been able to defeat even the best human chess player, but are only\\nrecently matching some of the abilities of average human beings to recognize objects\\nor speech. A person’s everyday life requires an immense amount of knowledge\\nabout the world. Much of this knowledge is subjective and intuitive, and therefore\\ndifficult to articulate in a formal way. Computers need to capture this same\\nknowledge in order to behave in an intelligent way. One of the key challenges in'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='artificial intelligence is how to get this informal knowledge into a computer.\\nSeveral artificial intelligence projects have sought to hard-code knowledge about\\nthe world in formal languages. A computer can reason about statements in these\\nformal languages automatically using logical inference rules. This is known as the\\nknowledge base approach to artificial intelligence. None of these projects has led\\nto a major success. One of the most famous such projects is Cyc (Lenat and Guha,\\n1989). Cyc is an inference engine and a database of statements in a language\\ncalled CycL. These statements are entered by a staff of human supervisors. It is an\\nunwieldy process. People struggle to devise formal rules with enough complexity\\nto accurately describe the world. For example, Cyc failed to understand a story\\nabout a person named Fred shaving in the morning (Linde, 1992). Its inference\\nengine detected an inconsistency in the story: it knew that people do not have'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='electrical parts, but because Fred was holding an electric razor, it believed the\\nentity “FredWhileShaving” contained electrical parts. It therefore asked whether\\nFred was still a person while he was shaving.\\nThe difficulties faced by systems relying on hard-coded knowledge suggest\\nthat AI systems need the ability to acquire their own knowledge, by extracting\\npatterns from raw data. This capability is known as machine learning. The\\n2\\nCHAPTER 1. INTRODUCTION\\nintroduction of machine learning allowed computers to tackle problems involving\\nknowledge of the real world and make decisions that appear subjective. A simple\\nmachine learning algorithm called logistic regression can determine whether to\\nrecommend cesarean delivery (Mor-Yosef et al., 1990). A simple machine learning\\nalgorithm called naive Bayes can separate legitimate e-mail from spam e-mail.\\nThe performance of these simple machine learning algorithms depends heavily'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='on the representation of the data they are given. For example, when logistic\\nregression is used to recommend cesarean delivery, the AI system does not examine\\nthe patient directly. Instead, the doctor tells the system several pieces of relevant\\ninformation, such as the presence or absence of a uterine scar. Each piece of\\ninformation included in the representation of the patient is known as a feature.\\nLogistic regression learns how each of these features of the patient correlates with\\nvarious outcomes. However, it cannot influence the way that the features are\\ndefined in any way. If logistic regression was given an MRI scan of the patient,\\nrather than the doctor’s formalized report, it would not be able to make useful\\npredictions. Individual pixels in an MRI scan have negligible correlation with any\\ncomplications that might occur during delivery.\\nThis dependence on representations is a general phenomenon that appears'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='throughout computer science and even daily life. In computer science, opera-\\ntions such as searching a collection of data can proceed exponentially faster if\\nthe collection is structured and indexed intelligently. People can easily perform\\narithmetic on Arabic numerals, but find arithmetic on Roman numerals much\\nmore time-consuming. It is not surprising that the choice of representation has an\\nenormous effect on the performance of machine learning algorithms. For a simple\\nvisual example, see figure 1.1.\\nMany artificial intelligence tasks can be solved by designing the right set of\\nfeatures to extract for that task, then providing these features to a simple machine\\nlearning algorithm. For example, a useful feature for speaker identification from\\nsound is an estimate of the size of speaker’s vocal tract. It therefore gives a strong\\nclue as to whether the speaker is a man, woman, or child.\\nHowever, for many tasks, it is difficulttoknowwhat featuresshould be extracted.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='For example, suppose that we would like to write a program to detect cars in\\nphotographs. We know that cars have wheels, so we might like to use the presence\\nof a wheel as a feature. Unfortunately, it is difficult to describe exactly what a\\nwheel looks like in terms of pixel values. A wheel has a simple geometric shape but\\nits image may be complicated by shadows falling on the wheel, the sun glaring off\\nthe metal parts of the wheel, the fender of the car or an object in the foreground\\nobscuring part of the wheel, and so on.\\n3\\nCHAPTER 1. INTRODUCTION\\n\\ue078\\n\\ue079\\n\\ue043\\ue061\\ue072\\ue074\\ue065\\ue073\\ue069\\ue061\\ue06e\\ue020\\ue063\\ue06f\\ue06f\\ue072\\ue064\\ue069\\ue06e\\ue061\\ue074\\ue065\\ue073\\n\\ue072\\n\\ue0b5\\n\\ue050\\ue06f\\ue06c\\ue061\\ue072\\ue020\\ue063\\ue06f\\ue06f\\ue072\\ue064\\ue069\\ue06e\\ue061\\ue074\\ue065\\ue073\\nFigure 1.1: Example of different representations: suppose we want to separate two\\ncategories of data by drawing a line between them in a scatterplot. In the plot on the left,\\nwe represent some data using Cartesian coordinates, and the task is impossible. In the plot\\non the right, we represent the data with polar coordinates and the task becomes simple to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='solve with a vertical line. Figure produced in collaboration with David Warde-Farley.\\nOne solution to this problem is to use machine learning to discover not only\\nthe mapping from representation to output but also the representation itself.\\nThis approach is known as representation learning. Learned representations\\noften result in much better performance than can be obtained with hand-designed\\nrepresentations. They also allow AI systems to rapidly adapt to new tasks, with\\nminimal human intervention. A representation learning algorithm can discover a\\ngood set of features for a simple task in minutes, or a complex task in hours to\\nmonths. Manually designing features for a complex task requires a great deal of\\nhuman time and effort; it can take decades for an entire community of researchers.\\nThe quintessential example of a representation learning algorithm is the au-\\ntoencoder. An autoencoder is the combination of an encoder function that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='converts the input data into a different representation, and a decoder function\\nthat converts the new representation back into the original format. Autoencoders\\nare trained to preserve as much information as possible when an input is run\\nthrough the encoder and then the decoder, but are also trained to make the new\\nrepresentation have various nice properties. Different kinds of autoencoders aim to\\nachieve different kinds of properties.\\nWhen designing features or algorithms for learning features, our goal is usually\\nto separate the factors of variation that explain the observed data. In this\\ncontext, we use the word “factors” simply to refer to separate sources of influence;\\nthe factors are usually not combined by multiplication. Such factors are often not\\n4\\nCHAPTER 1. INTRODUCTION\\nquantities that are directly observed. Instead, they may exist either as unobserved\\nobjects or unobserved forces in the physical world that affect observable quantities.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='They may also exist as constructs in the human mind that provide useful simplifying\\nexplanations or inferred causes of the observed data. They can be thought of as\\nconcepts or abstractions that help us make sense of the rich variability in the data.\\nWhen analyzing a speech recording, the factors of variation include the speaker’s\\nage, their sex, their accent and the words that they are speaking. When analyzing\\nan image of a car, the factors of variation include the position of the car, its color,\\nand the angle and brightness of the sun.\\nA major source of difficulty in many real-world artificial intelligence applications\\nis that many of the factors of variation influence every single piece of data we are\\nable to observe. The individual pixels in an image of a red car might be very close\\nto black at night. The shape of the car’s silhouette depends on the viewing angle.\\nMost applications require us to disentangle the factors of variation and discard the\\nones that we do not care about.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Of course, it can be very difficult to extract such high-level, abstract features\\nfrom raw data. Many of these factors of variation, such as a speaker’s accent,\\ncan be identified only using sophisticated, nearly human-level understanding of\\nthe data. When it is nearly as difficult to obtain a representation as to solve the\\noriginal problem, representation learning does not, at first glance, seem to help us.\\nDeep learning solves this central problem in representation learning by intro-\\nducing representations that are expressed in terms of other, simpler representations.\\nDeep learning allows the computer to build complex concepts out of simpler con-\\ncepts. Figure 1.2 shows how a deep learning system can represent the concept of\\nan image of a person by combining simpler concepts, such as corners and contours,\\nwhich are in turn defined in terms of edges.\\nThe quintessential example of a deep learning model is the feedforward deep'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='network or multilayer perceptron (MLP). A multilayer perceptron is just a\\nmathematical function mapping some set of input values to output values. The\\nfunction is formed by composing many simpler functions. We can think of each\\napplication of a different mathematical function as providing a new representation\\nof the input.\\nThe idea of learning the right representation for the data provides one perspec-\\ntive on deep learning. Another perspective on deep learning is that depth allows the\\ncomputer to learn a multi-step computer program. Each layer of the representation\\ncan be thought of as the state of the computer’s memory after executing another\\nset of instructions in parallel. Networks with greater depth can execute more\\ninstructions in sequence. Sequential instructions offer great power because later\\n5\\nCHAPTER 1. INTRODUCTION\\nOutput\\nCAR PERSON ANIMAL\\n(object identity)\\n3rd hidden layer\\n(object parts)\\n2nd hidden layer\\n(corners and\\ncontours)\\n1st hidden layer\\n(edges)\\nVisible layer'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='(input pixels)\\nFigure1.2: Illustration ofadeep learningmodel. Itisdifficult fora computertounderstand\\nthe meaning of raw sensory input data, such as this image represented as a collection\\nof pixel values. The function mapping from a set of pixels to an object identity is very\\ncomplicated. Learning or evaluating this mapping seems insurmountable if tackled directly.\\nDeep learning resolves this difficulty by breaking the desired complicated mapping into a\\nseries of nested simple mappings, each described by a different layer of the model. The\\ninput is presented at the visible layer, so named because it contains the variables that\\nwe are able to observe. Then a series of hidden layers extracts increasingly abstract\\nfeatures from the image. These layers are called “hidden” because their values are not given\\nin the data; instead the model must determine which concepts are useful for explaining\\nthe relationships in the observed data. The images here are visualizations of the kind'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of feature represented by each hidden unit. Given the pixels, the first layer can easily\\nidentify edges, by comparing the brightness of neighboring pixels. Given the first hidden\\nlayer’s description of the edges, the second hidden layer can easily search for corners and\\nextended contours, which are recognizable as collections of edges. Given the second hidden\\nlayer’s description of the image in terms of corners and contours, the third hidden layer\\ncan detect entire parts of specific objects, by finding specific collections of contours and\\ncorners. Finally, this description of the image in terms of the object parts it contains can\\nbe used to recognize the objects present in the image. Images reproduced with permission\\nfrom Zeiler and Fergus (2014).\\n6\\nCHAPTER 1. INTRODUCTION\\nσ\\nElement Element\\nSet Set\\n+\\n+\\nLogistic Logistic\\n×\\nRegression Regression\\n× ×\\nσ\\nww 11 xx 11 ww 22 xx 22 ww xx\\nFigure 1.3: Illustration of computational graphs mapping an input to an output where'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='each node performs an operation. Depth is the length of the longest path from input to\\noutput but depends on the definition of what constitutes a possible computational step.\\nThe computation depicted in these graphs is the output of a logistic regression model,\\nσ(wTx), where σ is the logistic sigmoid function. If we use addition, multiplication and\\nlogistic sigmoids as the elements of our computer language, then this model has depth\\nthree. If we view logistic regression as an element itself, then this model has depth one.\\ninstructions can refer back to the results of earlier instructions. According to this\\nview of deep learning, not all of the information in a layer’s activations necessarily\\nencodes factors of variation that explain the input. The representation also stores\\nstate information that helps to execute a program that can make sense of the input.\\nThis state information could be analogous to a counter or pointer in a traditional'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='computer program. It has nothing to do with the content of the input specifically,\\nbut it helps the model to organize its processing.\\nThere are two main ways of measuring the depth of a model. The first view is\\nbased on the number of sequential instructions that must be executed to evaluate\\nthe architecture. We can think of this as the length of the longest path through\\na flow chart that describes how to compute each of the model’s outputs given\\nits inputs. Just as two equivalent computer programs will have different lengths\\ndepending on which language the program is written in, the same function may\\nbe drawn as a flowchart with different depths depending on which functions we\\nallow to be used as individual steps in the flowchart. Figure 1.3 illustrates how this\\nchoice of language can give two different measurements for the same architecture.\\nAnother approach, used by deep probabilistic models, regards the depth of a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='model as being not the depth of the computational graph but the depth of the\\ngraph describing how concepts are related to each other. In this case, the depth\\n7\\nCHAPTER 1. INTRODUCTION\\nof the flowchart of the computations needed to compute the representation of\\neach concept may be much deeper than the graph of the concepts themselves.\\nThis is because the system’s understanding of the simpler concepts can be refined\\ngiven information about the more complex concepts. For example, an AI system\\nobserving an image of a face with one eye in shadow may initially only see one eye.\\nAfter detecting that a face is present, it can then infer that a second eye is probably\\npresent as well. In this case, the graph of concepts only includes two layers—a\\nlayer for eyes and a layer for faces—but the graph of computations includes 2n\\nlayers if we refine our estimate of each concept given the other n times.\\nBecause it is not always clear which of these two views—the depth of the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='computational graph, or the depth of the probabilistic modeling graph—is most\\nrelevant, and because different people choose different sets of smallest elements\\nfrom which to construct their graphs, there is no single correct value for the\\ndepth of an architecture, just as there is no single correct value for the length of\\na computer program. Nor is there a consensus about how much depth a model\\nrequires to qualify as “deep.” However, deep learning can safely be regarded as the\\nstudy of models that either involve a greater amount of composition of learned\\nfunctions or learned concepts than traditional machine learning does.\\nTo summarize, deep learning, the subject of this book, is an approach to AI.\\nSpecifically, it is a type of machine learning, a technique that allows computer\\nsystems to improve with experience and data. According to the authors of this\\nbook, machine learning is the only viable approach to building AI systems that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='can operate in complicated, real-world environments. Deep learning is a particular\\nkind of machine learning that achieves great power and flexibility by learning to\\nrepresent the world as a nested hierarchy of concepts, with each concept defined in\\nrelation to simpler concepts, and more abstract representations computed in terms\\nof less abstract ones. Figure 1.4 illustrates the relationship between these different\\nAI disciplines. Figure 1.5 gives a high-level schematic of how each works.\\n1.1 Who Should Read This Book?\\nThis book can be useful for a variety of readers, but we wrote it with two main\\ntarget audiences in mind. One of these target audiences is university students\\n(undergraduate or graduate) learning about machine learning, including those who\\nare beginning a career in deep learning and artificial intelligence research. The\\nother target audience is software engineers who do not have a machine learning'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='or statistics background, but want to rapidly acquire one and begin using deep\\nlearning in their product or platform. Deep learning has already proven useful in\\n8\\nCHAPTER 1. INTRODUCTION\\nDeep learning Example:\\nShallow\\nExample: Example:\\nExample: autoencoders\\nLogistic Knowledge\\nMLPs\\nregression bases\\nRepresentation learning\\nMachine learning\\nAI\\nFigure 1.4: A Venn diagram showing how deep learning is a kind of representation learning,\\nwhich is in turn a kind of machine learning, which is used for many but not all approaches\\nto AI. Each section of the Venn diagram includes an example of an AI technology.\\n9\\nCHAPTER 1. INTRODUCTION\\nOutput\\nMapping from\\nOutput Output\\nfeatures\\nAdditional\\nMapping from Mapping from layers of more\\nOutput\\nfeatures features abstract\\nfeatures\\nHand- Hand-\\nSimple\\ndesigned designed Features\\nfeatures\\nprogram features\\nInput Input Input Input\\nDeep\\nClassic\\nRule-based learning\\nmachine\\nsystems\\nlearning Representation\\nlearning'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Figure 1.5: Flowcharts showing how the different parts of an AI system relate to each\\nother within different AI disciplines. Shaded boxes indicate components that are able to\\nlearn from data.\\n10\\nCHAPTER 1. INTRODUCTION\\nmany software disciplines including computer vision, speech and audio processing,\\nnatural language processing, robotics, bioinformatics and chemistry, video games,\\nsearch engines, online advertising and finance.\\nThis book has been organized into three parts in order to best accommodate a\\nvariety of readers. Part I introduces basic mathematical tools and machine learning\\nconcepts. Part II describes the most established deep learning algorithms that are\\nessentially solved technologies. Part III describes more speculative ideas that are\\nwidely believed to be important for future research in deep learning.\\nReaders should feel free to skip parts that are not relevant given their interests\\nor background. Readers familiar with linear algebra, probability, and fundamental'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='machine learning concepts can skip part I, for example, while readers who just want\\nto implement a working system need not read beyond part II. To help choose which\\nchapters to read, figure 1.6 provides a flowchart showing the high-level organization\\nof the book.\\nWe do assume that all readers come from a computer science background. We\\nassume familiarity with programming, a basic understanding of computational\\nperformance issues, complexity theory, introductory level calculus and some of the\\nterminology of graph theory.\\n1.2 Historical Trends in Deep Learning\\nIt is easiest to understand deep learning with some historical context. Rather than\\nproviding a detailed history of deep learning, we identify a few key trends:\\nDeep learning has had a long and rich history, but has gone by many names\\n•\\nreflecting different philosophical viewpoints, and has waxed and waned in\\npopularity.\\nDeep learning has become more useful as the amount of available training\\n•\\ndata has increased.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Deep learning models have grown in size over time as computer infrastructure\\n•\\n(both hardware and software) for deep learning has improved.\\nDeep learninghas solvedincreasingly complicated applicationswith increasing\\n•\\naccuracy over time.\\n11\\nCHAPTER 1. INTRODUCTION\\n1. Introduction\\nPart I: Applied Math and Machine Learning Basics\\n3. Probability and\\n2. Linear Algebra\\nInformation Theory\\n4. Numerical 5. Machine Learning\\nComputation Basics\\nPart II: Deep Networks: Modern Practices\\n6. Deep Feedforward\\nNetworks\\n7. Regularization 8. Optimization 9. CNNs 10. RNNs\\n11. Practical\\n12. Applications\\nMethodology\\nPart III: Deep Learning Research\\n13. Linear Factor 15. Representation\\n14. Autoencoders\\nModels Learning\\n16. Structured 17. Monte Carlo\\nProbabilistic Models Methods\\n18. Partition\\n19. Inference\\nFunction\\n20. Deep Generative\\nModels\\nFigure 1.6: The high-level organization of the book. An arrow from one chapter to another'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='indicates that the former chapter is prerequisite material for understanding the latter.\\n12\\nCHAPTER 1. INTRODUCTION\\n1.2.1 The Many Names and Changing Fortunes of Neural Net-\\nworks\\nWe expect that many readers of this book have heard of deep learning as an\\nexciting new technology, and are surprised to see a mention of “history” in a book\\nabout an emerging field. In fact, deep learning dates back to the 1940s. Deep\\nlearning only appears to be new, because it was relatively unpopular for several\\nyears preceding its current popularity, and because it has gone through many\\ndifferent names, and has only recently become called “deep learning.” The field\\nhas been rebranded many times, reflecting the influence of different researchers\\nand different perspectives.\\nA comprehensive history of deep learning is beyond the scope of this textbook.\\nHowever, some basic context is useful for understanding deep learning. Broadly\\nspeaking, there have been three waves of development of deep learning: deep'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='learning known as cybernetics in the 1940s–1960s, deep learning known as\\nconnectionism in the 1980s–1990s, and the current resurgence under the name\\ndeep learning beginning in 2006. This is quantitatively illustrated in figure 1.7.\\nSome of the earliest learning algorithms we recognize today were intended\\nto be computational models of biological learning, i.e. models of how learning\\nhappens or could happen in the brain. As a result, one of the names that deep\\nlearning has gone by is artificial neural networks (ANNs). The corresponding\\nperspective on deep learning models is that they are engineered systems inspired\\nby the biological brain (whether the human brain or the brain of another animal).\\nWhile the kinds of neural networks used for machine learning have sometimes\\nbeen used to understand brain function (Hinton and Shallice, 1991), they are\\ngenerally not designed to be realistic models of biological function. The neural'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='perspective on deep learning is motivated by two main ideas. One idea is that\\nthe brain provides a proof by example that intelligent behavior is possible, and a\\nconceptually straightforward path to building intelligence is to reverse engineer the\\ncomputational principles behind the brain and duplicate its functionality. Another\\nperspective is that it would be deeply interesting to understand the brain and the\\nprinciples that underlie human intelligence, so machine learning models that shed\\nlight on these basic scientific questions are useful apart from their ability to solve\\nengineering applications.\\nThe modern term “deep learning” goes beyond the neuroscientific perspective\\non the current breed of machine learning models. It appeals to a more general\\nprinciple of learning multiple levels of composition, which can be applied in machine\\nlearning frameworks that are not necessarily neurally inspired.\\n13\\nCHAPTER 1. INTRODUCTION\\n0.000250\\n0.000200\\n0.000150\\n0.000100\\n0.000050\\n0.000000'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='1940 1950 1960 1970 1980 1990 2000\\nYear\\nesarhP\\nro\\ndroW\\nfo\\nycneuqerF\\ncybernetics\\n(connectionism + neural networks)\\nFigure 1.7: The figure shows two of the three historical waves of artificial neural nets\\nresearch, as measured by the frequency of the phrases “cybernetics” and “connectionism” or\\n“neural networks” according to Google Books (the third wave is too recent to appear). The\\nfirst wave started with cybernetics in the 1940s–1960s, with the development of theories\\nof biological learning (McCulloch and Pitts, 1943; Hebb, 1949) and implementations of\\nthe first models such as the perceptron (Rosenblatt, 1958) allowing the training of a single\\nneuron. The second wave started with the connectionist approach of the 1980–1995 period,\\nwith back-propagation (Rumelhart et al., 1986a) to train a neural network with one or two\\nhidden layers. The current and third wave, deep learning, started around 2006 (Hinton'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='et al., 2006; Bengio et al., 2007; Ranzato et al., 2007a), and is just now appearing in book\\nform as of 2016. The other two waves similarly appeared in book form much later than\\nthe corresponding scientific activity occurred.\\n14\\nCHAPTER 1. INTRODUCTION\\nThe earliest predecessors of modern deep learning were simple linear models\\nmotivated from a neuroscientific perspective. These models were designed to\\ntake a set of n input values x ,...,x and associate them with an output y.\\n1 n\\nThese models would learn a set of weights w ,...,w and compute their output\\n1 n\\nf(x,w) = x w + +x w . This first wave of neural networks research was\\n1 1 n n\\n···\\nknown as cybernetics, as illustrated in figure 1.7.\\nThe McCulloch-Pitts Neuron (McCulloch and Pitts, 1943) was an early model\\nof brain function. This linear model could recognize two different categories of\\ninputs by testing whether f(x,w) is positive or negative. Of course, for the model'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to correspond to the desired definition of the categories, the weights needed to be\\nset correctly. These weights could be set by the human operator. In the 1950s,\\nthe perceptron (Rosenblatt, 1958, 1962) became the first model that could learn\\nthe weights defining the categories given examples of inputs from each category.\\nThe adaptive linear element (ADALINE), which dates from about the same\\ntime, simply returned the value of f(x) itself to predict a real number (Widrow\\nand Hoff, 1960), and could also learn to predict these numbers from data.\\nThese simple learning algorithms greatly affected the modern landscape of ma-\\nchine learning. The training algorithm used to adapt the weights of the ADALINE\\nwas a special case of an algorithm called stochastic gradient descent. Slightly\\nmodified versions of the stochastic gradient descent algorithm remain the dominant\\ntraining algorithms for deep learning models today.\\nModels based on the f(x,w) used by the perceptron and ADALINE are called'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='linear models. These models remain some of the most widely used machine\\nlearning models, though in many cases they are trained in different ways than the\\noriginal models were trained.\\nLinear models have many limitations. Most famously, they cannot learn the\\nXOR function, where f([0,1],w) = 1 and f([1,0],w) = 1 but f([1,1],w) = 0\\nand f([0,0],w) = 0. Critics who observed these flaws in linear models caused\\na backlash against biologically inspired learning in general (Minsky and Papert,\\n1969). This was the first major dip in the popularity of neural networks.\\nToday, neuroscience is regarded as an important source of inspiration for deep\\nlearning researchers, but it is no longer the predominant guide for the field.\\nThe main reason for the diminished role of neuroscience in deep learning\\nresearch today is that we simply do not have enough information about the brain\\nto use it as a guide. To obtain a deep understanding of the actual algorithms used'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='by the brain, we would need to be able to monitor the activity of (at the very\\nleast) thousands of interconnected neurons simultaneously. Because we are not\\nable to do this, we are far from understanding even some of the most simple and\\n15\\nCHAPTER 1. INTRODUCTION\\nwell-studied parts of the brain (Olshausen and Field, 2005).\\nNeuroscience has given us a reason to hope that a single deep learning algorithm\\ncan solve many different tasks. Neuroscientists have found that ferrets can learn to\\n“see” with the auditory processing region of their brain if their brains are rewired\\nto send visual signals to that area (Von Melchner et al., 2000). This suggests that\\nmuch of the mammalian brain might use a single algorithm to solve most of the\\ndifferent tasks that the brain solves. Before this hypothesis, machine learning\\nresearch was more fragmented, with different communities of researchers studying\\nnatural language processing, vision, motion planning and speechrecognition. Today,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='these application communities are still separate, but it is common for deep learning\\nresearch groups to study many or even all of these application areas simultaneously.\\nWe are able to draw some rough guidelines from neuroscience. The basic idea of\\nhaving many computational units that become intelligent only via their interactions\\nwith each other is inspired by the brain. The Neocognitron (Fukushima, 1980)\\nintroduced a powerful model architecture for processing images that was inspired\\nby the structure of the mammalian visual system and later became the basis\\nfor the modern convolutional network (LeCun et al., 1998b), as we will see in\\nsection 9.10. Most neural networks today are based on a model neuron called\\nthe rectified linear unit. The original Cognitron (Fukushima, 1975) introduced\\na more complicated version that was highly inspired by our knowledge of brain\\nfunction. The simplified modern version was developed incorporating ideas from'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='many viewpoints, with Nair and Hinton (2010) and Glorot et al. (2011a) citing\\nneuroscience as an influence, and Jarrett et al. (2009) citing more engineering-\\noriented influences. While neuroscience is an important source of inspiration, it\\nneed not be taken as a rigid guide. We know that actual neurons compute very\\ndifferent functions than modern rectified linear units, but greater neural realism\\nhas not yet led to an improvement in machine learning performance. Also, while\\nneuroscience has successfully inspired several neural network architectures, we\\ndo not yet know enough about biological learning for neuroscience to offer much\\nguidance for the learning algorithms we use to train these architectures.\\nMedia accounts often emphasize the similarity of deep learning to the brain.\\nWhile it is true that deep learning researchers are more likely to cite the brain as an\\ninfluence than researchers working in other machine learning fields such as kernel'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='machines or Bayesian statistics, one should not view deep learning as an attempt\\nto simulate the brain. Modern deep learning draws inspiration from many fields,\\nespecially applied math fundamentals like linear algebra, probability, information\\ntheory, and numerical optimization. While some deep learning researchers cite\\nneuroscience as an important source of inspiration, others are not concerned with\\n16\\nCHAPTER 1. INTRODUCTION\\nneuroscience at all.\\nIt is worth noting that the effort to understand how the brain works on\\nan algorithmic level is alive and well. This endeavor is primarily known as\\n“computational neuroscience” and is a separate field of study from deep learning.\\nIt is common for researchers to move back and forth between both fields. The\\nfield of deep learning is primarily concerned with how to build computer systems\\nthat are able to successfully solve tasks requiring intelligence, while the field of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='computational neuroscience is primarily concerned with building more accurate\\nmodels of how the brain actually works.\\nIn the 1980s, the second wave of neural network research emerged in great\\npart via a movement called connectionism or parallel distributed process-\\ning (Rumelhart et al., 1986c; McClelland et al., 1995). Connectionism arose in\\nthe context of cognitive science. Cognitive science is an interdisciplinary approach\\nto understanding the mind, combining multiple different levels of analysis. During\\nthe early 1980s, most cognitive scientists studied models of symbolic reasoning.\\nDespite their popularity, symbolic models were difficult to explain in terms of\\nhow the brain could actually implement them using neurons. The connectionists\\nbegan to study models of cognition that could actually be grounded in neural\\nimplementations (Touretzky and Minton, 1985), reviving many ideas dating back\\nto the work of psychologist Donald Hebb in the 1940s (Hebb, 1949).'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Thecentralidea inconnectionism is thata largenumber ofsimple computational\\nunits can achieve intelligent behavior when networked together. This insight\\napplies equally to neurons in biological nervous systems and to hidden units in\\ncomputational models.\\nSeveral key concepts arose during the connectionism movement of the 1980s\\nthat remain central to today’s deep learning.\\nOne of these concepts is that of distributed representation (Hinton et al.,\\n1986). This is the idea that each input to a system should be represented by\\nmany features, and each feature should be involved in the representation of many\\npossible inputs. For example, suppose we have a vision system that can recognize\\ncars, trucks, and birds and these objects can each be red, green, or blue. One way\\nof representing these inputs would be to have a separate neuron or hidden unit\\nthat activates for each of the nine possible combinations: red truck, red car, red'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='bird, green truck, and so on. This requires nine different neurons, and each neuron\\nmust independently learn the concept of color and object identity. One way to\\nimprove on this situation is to use a distributed representation, with three neurons\\ndescribing the color and three neurons describing the object identity. This requires\\nonly six neurons total instead of nine, and the neuron describing redness is able to\\n17\\nCHAPTER 1. INTRODUCTION\\nlearn about redness from images of cars, trucks and birds, not only from images\\nof one specific category of objects. The concept of distributed representation is\\ncentral to this book, and will be described in greater detail in chapter 15.\\nAnother major accomplishment of the connectionist movement was the suc-\\ncessful use of back-propagation to train deep neural networks with internal repre-\\nsentations and the popularization of the back-propagation algorithm (Rumelhart\\net al., 1986a; LeCun, 1987). This algorithm has waxed and waned in popularity'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='but as of this writing is currently the dominant approach to training deep models.\\nDuring the 1990s, researchers made important advances in modeling sequences\\nwith neural networks. Hochreiter (1991) and Bengio et al. (1994) identified some of\\nthe fundamental mathematical difficulties in modeling long sequences, described in\\nsection 10.7. Hochreiter and Schmidhuber (1997) introduced the long short-term\\nmemory or LSTM network to resolve some of these difficulties. Today, the LSTM\\nis widely used for many sequence modeling tasks, including many natural language\\nprocessing tasks at Google.\\nThe second wave of neural networks research lasted until the mid-1990s. Ven-\\ntures based on neural networks and other AI technologies began to make unrealisti-\\ncally ambitious claims while seeking investments. When AI research did not fulfill\\nthese unreasonable expectations, investors were disappointed. Simultaneously,\\nother fields of machine learning made advances. Kernel machines (Boser et al.,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='1992; Cortes and Vapnik, 1995; Schölkopf et al., 1999) and graphical models (Jor-\\ndan, 1998) both achieved good results on many important tasks. These two factors\\nled to a decline in the popularity of neural networks that lasted until 2007.\\nDuring this time, neural networks continued to obtain impressive performance\\non some tasks (LeCun et al., 1998b; Bengio et al., 2001). The Canadian Institute\\nfor Advanced Research (CIFAR) helped to keep neural networks research alive\\nvia its Neural Computation and Adaptive Perception (NCAP) research initiative.\\nThis program united machine learning research groups led by Geoffrey Hinton\\nat University of Toronto, Yoshua Bengio at University of Montreal, and Yann\\nLeCun at New York University. The CIFAR NCAP research initiative had a\\nmulti-disciplinary nature that also included neuroscientists and experts in human\\nand computer vision.\\nAt this point in time, deep networks were generally believed to be very difficult'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to train. We now know that algorithms that have existed since the 1980s work\\nquite well, but this was not apparent circa 2006. The issue is perhaps simply that\\nthese algorithms were too computationally costly to allow much experimentation\\nwith the hardware available at the time.\\nThe third wave of neural networks research began with a breakthrough in\\n18\\nCHAPTER 1. INTRODUCTION\\n2006. Geoffrey Hinton showed that a kind of neural network called a deep belief\\nnetwork could be efficiently trained using a strategy called greedy layer-wise pre-\\ntraining (Hinton et al., 2006), which will be described in more detail in section 15.1.\\nThe other CIFAR-affiliated research groups quickly showed that the same strategy\\ncould be used to train many other kinds of deep networks (Bengio et al., 2007;\\nRanzato et al., 2007a) and systematically helped to improve generalization on\\ntest examples. This wave of neural networks research popularized the use of the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='term “deep learning” to emphasize that researchers were now able to train deeper\\nneural networks than had been possible before, and to focus attention on the\\ntheoretical importance of depth (Bengio and LeCun, 2007; Delalleau and Bengio,\\n2011; Pascanu et al., 2014a; Montufar et al., 2014). At this time, deep neural\\nnetworks outperformed competing AI systems based on other machine learning\\ntechnologies as well as hand-designed functionality. This third wave of popularity\\nof neural networks continues to the time of this writing, though the focus of deep\\nlearning research has changed dramatically within the time of this wave. The\\nthird wave began with a focus on new unsupervised learning techniques and the\\nability of deep models to generalize well from small datasets, but today there is\\nmore interest in much older supervised learning algorithms and the ability of deep\\nmodels to leverage large labeled datasets.\\n1.2.2 Increasing Dataset Sizes'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='One may wonder why deep learning has only recently become recognized as a\\ncrucial technology though the first experiments with artificial neural networks were\\nconducted in the 1950s. Deep learning has been successfully used in commercial\\napplications since the 1990s, but was often regarded as being more of an art than\\na technology and something that only an expert could use, until recently. It is true\\nthat some skill is required to get good performance from a deep learning algorithm.\\nFortunately, the amount of skill required reduces as the amount of training data\\nincreases. The learning algorithms reaching human performance on complex tasks\\ntoday are nearly identical to the learning algorithms that struggled to solve toy\\nproblems in the 1980s, though the models we train with these algorithms have\\nundergone changes that simplify the training of very deep architectures. The most\\nimportant new development is that today we can provide these algorithms with'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the resources they need to succeed. Figure 1.8 shows how the size of benchmark\\ndatasets has increased remarkably over time. This trend is driven by the increasing\\ndigitization of society. As more and more of our activities take place on computers,\\nmore and more of what we do is recorded. As our computers are increasingly\\nnetworked together, it becomes easier to centralize these records and curate them\\n19\\nCHAPTER 1. INTRODUCTION\\ninto a dataset appropriate for machine learning applications. The age of “Big\\nData” has made machine learning much easier because the key burden of statistical\\nestimation—generalizing well to new data after observing only a small amount\\nof data—has been considerably lightened. As of 2016, a rough rule of thumb\\nis that a supervised deep learning algorithm will generally achieve acceptable\\nperformance with around 5,000 labeled examples per category, and will match or\\nexceed human performance when trained with a dataset containing at least 10'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='million labeled examples. Working successfully with datasets smaller than this is\\nan important research area, focusing in particular on how we can take advantage\\nof large quantities of unlabeled examples, with unsupervised or semi-supervised\\nlearning.\\n1.2.3 Increasing Model Sizes\\nAnother key reason that neural networks are wildly successful today after enjoying\\ncomparatively little success since the 1980s is that we have the computational\\nresources to run much larger models today. One of the main insights of connection-\\nism is that animals become intelligent when many of their neurons work together.\\nAn individual neuron or small collection of neurons is not particularly useful.\\nBiological neurons are not especially densely connected. As seen in figure 1.10,\\nour machine learning models have had a number of connections per neuron that\\nwas within an order of magnitude of even mammalian brains for decades.\\nIn termsof the total number of neurons, neural networks have been astonishingly'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='small until quite recently, as shown in figure 1.11. Since the introduction of hidden\\nunits, artificial neural networks have doubled in size roughly every 2.4 years. This\\ngrowth is driven by faster computers with larger memory and by the availability\\nof larger datasets. Larger networks are able to achieve higher accuracy on more\\ncomplex tasks. Thistrend looksset to continuefor decades. Unless newtechnologies\\nallow faster scaling, artificial neural networks will not have the same number of\\nneurons as the human brain until at least the 2050s. Biological neurons may\\nrepresent more complicated functions than current artificial neurons, so biological\\nneural networks may be even larger than this plot portrays.\\nIn retrospect, it is not particularly surprising that neural networks with fewer\\nneurons than a leech were unable to solve sophisticated artificial intelligence prob-\\nlems. Even today’s networks, which we consider quite large from a computational'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='systems point of view, are smaller than the nervous system of even relatively\\nprimitive vertebrate animals like frogs.\\nThe increase in model size over time, due to the availability of faster CPUs,\\n20\\nCHAPTER 1. INTRODUCTION\\n109\\n108\\n107\\n106\\n105\\n104\\n103\\n102\\n101\\n100\\n1900 1950 1985 2000 2015\\nYear\\n)selpmaxe\\nrebmun(\\nezis\\ntesataD\\nCanadian Hansard\\nWMT Sports-1M\\nImageNet10k\\nPublic SVHN\\nCriminals ImageNet ILSVRC 2014\\nMNIST CIFAR-10\\nT vs. G vs. F Rotated T vs. C\\nIris\\nFigure 1.8: Dataset sizes have increased greatly over time. In the early 1900s, statisticians\\nstudied datasetsusing hundredsor thousandsof manuallycompiled measurements (Garson,\\n1900; Gosset,1908; Anderson, 1935;Fisher, 1936). In the 1950s through1980s, the pioneers\\nof biologically inspired machine learning often worked with small, synthetic datasets, such\\nas low-resolutionbitmaps ofletters, that weredesigned toincur lowcomputational cost and\\ndemonstrate that neural networks were able to learn specific kinds of functions (Widrow'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='and Hoff, 1960; Rumelhart et al., 1986b). In the 1980s and 1990s, machine learning\\nbecame more statistical in nature and began to leverage larger datasets containing tens\\nof thousands of examples such as the MNIST dataset (shown in figure 1.9) of scans\\nof handwritten numbers (LeCun et al., 1998b). In the first decade of the 2000s, more\\nsophisticated datasets of this same size, such as the CIFAR-10 dataset (Krizhevsky and\\nHinton, 2009) continued to be produced. Toward the end of that decade and throughout\\nthe first half of the 2010s, significantly larger datasets, containing hundreds of thousands\\nto tens of millions of examples, completely changed what was possible with deep learning.\\nThese datasets included the public Street View House Numbers dataset (Netzer et al.,\\n2011), various versions of the ImageNet dataset (Deng et al., 2009, 2010a; Russakovsky\\net al., 2014a), and the Sports-1M dataset (Karpathy et al., 2014). At the top of the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='graph, we see that datasets of translated sentences, such as IBM’s dataset constructed\\nfrom the Canadian Hansard (Brown et al., 1990) and the WMT 2014 English to French\\ndataset (Schwenk, 2014) are typically far ahead of other dataset sizes.\\n21\\nCHAPTER 1. INTRODUCTION\\nFigure 1.9: Example inputs from the MNIST dataset. The “NIST” stands for National\\nInstitute of Standards and Technology, the agency that originally collected this data.\\nThe “M” stands for “modified,” since the data has been preprocessed for easier use with\\nmachine learning algorithms. The MNIST dataset consists of scans of handwritten digits\\nand associated labels describing which digit 0–9 is contained in each image. This simple\\nclassification problem is one of the simplest and most widely used tests in deep learning\\nresearch. It remains popular despite being quite easy for modern techniques to solve.\\nGeoffrey Hinton has described it as “the drosophila of machine learning,” meaning that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='it allows machine learning researchers to study their algorithms in controlled laboratory\\nconditions, much as biologists often study fruit flies.\\n22\\nCHAPTER 1. INTRODUCTION\\nthe advent of general purpose GPUs (described in section 12.1.2), faster network\\nconnectivity and better software infrastructure for distributed computing, is one of\\nthe most important trends in the history of deep learning. This trend is generally\\nexpected to continue well into the future.\\n1.2.4 Increasing Accuracy, Complexity and Real-World Impact\\nSince the 1980s, deep learning has consistently improved in its ability to provide\\naccurate recognition or prediction. Moreover, deep learning has consistently been\\napplied with success to broader and broader sets of applications.\\nThe earliest deep models were used to recognize individual objects in tightly\\ncropped, extremely small images (Rumelhart et al., 1986a). Since then there has\\nbeen a gradual increase in the size of images neural networks could process. Modern'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='object recognition networks process rich high-resolution photographs and do not\\nhave a requirement that the photo be cropped near the object to be recognized\\n(Krizhevsky et al., 2012). Similarly, the earliest networks could only recognize\\ntwo kinds of objects (or in some cases, the absence or presence of a single kind of\\nobject), while these modern networks typically recognize at least 1,000 different\\ncategories of objects. The largest contest in object recognition is the ImageNet\\nLarge Scale Visual Recognition Challenge (ILSVRC) held each year. A dramatic\\nmoment in the meteoric rise of deep learning came when a convolutional network\\nwon this challenge for the first time and by a wide margin, bringing down the\\nstate-of-the-art top-5 error rate from 26.1% to 15.3% (Krizhevsky et al., 2012),\\nmeaning that the convolutional network produces a ranked list of possible categories\\nfor each image and the correct category appeared in the first five entries of this'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='list for all but 15.3% of the test examples. Since then, these competitions are\\nconsistently won by deep convolutional nets, and as of this writing, advances in\\ndeep learning have brought the latest top-5 error rate in this contest down to 3.6%,\\nas shown in figure 1.12.\\nDeep learning has also had a dramatic impact on speech recognition. After\\nimproving throughout the 1990s, the error rates for speech recognition stagnated\\nstarting in about 2000. The introduction of deep learning (Dahl et al., 2010; Deng\\net al., 2010b; Seide et al., 2011; Hinton et al., 2012a) to speech recognition resulted\\nin a sudden drop of error rates, with some error rates cut in half. We will explore\\nthis history in more detail in section 12.3.\\nDeep networks have also had spectacular successes for pedestrian detection and\\nimage segmentation (Sermanet et al., 2013; Farabet et al., 2013; Couprie et al.,\\n2013) and yielded superhuman performance in traffic sign classification (Ciresan\\n23\\nCHAPTER 1. INTRODUCTION\\n104'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='103\\n102\\n101\\n1950 1985 2000 2015\\nYear\\nnoruen\\nrep\\nsnoitcennoC\\nHuman\\n6 Cat\\n9 7\\n4\\nMouse\\n2\\n10\\n5\\n8\\nFruit fly\\n3\\n1\\nFigure 1.10: Initially, the number of connections between neurons in artificial neural\\nnetworks was limited by hardware capabilities. Today, the number of connections between\\nneurons is mostly a design consideration. Some artificial neural networks have nearly as\\nmany connections per neuron as a cat, and it is quite common for other neural networks\\nto have as many connections per neuron as smaller mammals like mice. Even the human\\nbrain does not have an exorbitant amount of connections per neuron. Biological neural\\nnetwork sizes from Wikipedia (2015).\\n1. Adaptivelinear element(Widrowand Hoff,1960)\\n2. Neocognitron (Fukushima, 1980)\\n3. GPU-acceleratedconvolutionalnetwork(Chellapilla et al., 2006)\\n4. DeepBoltzmann machine(Salakhutdinovand Hinton,2009a)\\n5. Unsupervised convolutionalnetwork(Jarrett et al., 2009)\\n6. GPU-acceleratedmultilayerperceptron(Ciresan et al., 2010)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='7. Distributedautoencoder(Le et al., 2012)\\n8. Multi-GPUconvolutionalnetwork(Krizhevsky et al., 2012)\\n9. COTSHPC unsupervised convolutionalnetwork(Coates et al., 2013)\\n10. GoogLeNet (Szegedyet al., 2014a)\\n24\\nCHAPTER 1. INTRODUCTION\\net al., 2012).\\nAt the same time that the scale and accuracy of deep networks has increased,\\nso has the complexity of the tasks that they can solve. Goodfellow et al. (2014d)\\nshowed that neural networks could learn to output an entire sequence of characters\\ntranscribed from an image, rather than just identifying a single object. Previously,\\nit was widely believed that this kind of learning required labeling of the individual\\nelements of the sequence (Gülçehre and Bengio, 2013). Recurrent neural networks,\\nsuch as the LSTM sequence model mentioned above, are now used to model\\nrelationships between sequences and other sequences rather than just fixed inputs.\\nThis sequence-to-sequence learning seems to be on the cusp of revolutionizing'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='another application: machine translation (Sutskever et al., 2014; Bahdanau et al.,\\n2015).\\nThis trend of increasing complexity has been pushed to its logical conclusion\\nwith the introduction of neural Turing machines (Graves et al., 2014a) that learn\\nto read from memory cells and write arbitrary content to memory cells. Such\\nneural networks can learn simple programs from examples of desired behavior. For\\nexample, they can learn to sort lists of numbers given examples of scrambled and\\nsorted sequences. This self-programming technology is in its infancy, but in the\\nfuture could in principle be applied to nearly any task.\\nAnother crowning achievement of deep learning is its extension to the domain of\\nreinforcement learning. Inthe contextof reinforcementlearning, an autonomous\\nagent must learn to perform a task by trial and error, without any guidance from\\nthe human operator. DeepMind demonstrated that a reinforcement learning system'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='based on deep learning is capable of learning to play Atari video games, reaching\\nhuman-level performance on many tasks (Mnih et al., 2015). Deep learning has\\nalso significantly improved the performance of reinforcement learning for robotics\\n(Finn et al., 2015).\\nMany of these applications of deep learning are highly profitable. Deep learning\\nis now used by many top technology companies including Google, Microsoft,\\nFacebook, IBM, Baidu, Apple, Adobe, Netflix, NVIDIA and NEC.\\nAdvances in deep learning have also depended heavily on advances in software\\ninfrastructure. Software libraries such as Theano (Bergstra et al., 2010; Bastien\\net al., 2012), PyLearn2 (Goodfellow et al., 2013c), Torch (Collobert et al., 2011b),\\nDistBelief (Dean et al., 2012), Caffe (Jia, 2013), MXNet (Chen et al., 2015), and\\nTensorFlow (Abadi et al., 2015) have all supported important research projects or\\ncommercial products.\\nDeep learning has also made contributions back to other sciences. Modern'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='convolutional networks for object recognition provide a model of visual processing\\n25\\nCHAPTER 1. INTRODUCTION\\nthat neuroscientists can study (DiCarlo, 2013). Deep learning also provides useful\\ntools for processing massive amounts of data and making useful predictions in\\nscientific fields. It has been successfully used to predict how molecules will interact\\nin order to help pharmaceutical companies design new drugs (Dahl et al., 2014),\\nto search for subatomic particles (Baldi et al., 2014), and to automatically parse\\nmicroscope images used to construct a 3-D map of the human brain (Knowles-\\nBarley et al., 2014). We expect deep learning to appear in more and more scientific\\nfields in the future.\\nIn summary, deep learning is an approach to machine learning that has drawn\\nheavily on our knowledge of the human brain, statistics and applied math as it\\ndeveloped over the past several decades. In recent years, it has seen tremendous'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='growth in its popularity and usefulness, due in large part to more powerful com-\\nputers, larger datasets and techniques to train deeper networks. The years ahead\\nare full of challenges and opportunities to improve deep learning even further and\\nbring it to new frontiers.\\n26\\nCHAPTER 1. INTRODUCTION\\n1011\\n1010\\n109\\n108\\n107\\n106\\n105\\n104\\n103\\n102\\n101\\n100\\n10 1 −\\n10 2 −\\n1950 1985 2000 2015 2056\\nYear\\n)elacs\\ncimhtiragol(\\nsnoruen\\nfo\\nrebmuN\\nHuman\\n17 20\\n16 19\\nOctopus\\n18\\n14\\n11 Frog\\n8\\n3 Bee\\nAnt\\nLeech\\n13\\n1 2 12 15 Roundworm\\n6\\n9\\n5 10\\n4 7\\nSponge\\nFigure 1.11: Since the introduction of hidden units, artificial neural networks have doubled\\nin size roughly every 2.4 years. Biological neural network sizes from Wikipedia (2015).\\n1. Perceptron(Rosenblatt,1958,1962)\\n2. Adaptivelinear element(Widrowand Hoff,1960)\\n3. Neocognitron (Fukushima, 1980)\\n4. Earlyback-propagationnetwork(Rumelhartet al., 1986b)\\n5. Recurrentneuralnetworkforspeechrecognition (Robinsonand Fallside,1991)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='6. Multilayerperceptronfor speechrecognition (Bengioet al.,1991)\\n7. Meanfield sigmoidbelief network(Saulet al.,1996)\\n8. LeNet-5(LeCun et al., 1998b)\\n9. Echostatenetwork(Jaegerand Haas,2004)\\n10. Deepbelief network(Hintonet al., 2006)\\n11. GPU-acceleratedconvolutionalnetwork(Chellapilla et al., 2006)\\n12. DeepBoltzmann machine(Salakhutdinovand Hinton,2009a)\\n13. GPU-accelerateddeep belief network(Raina et al.,2009)\\n14. Unsupervised convolutionalnetwork(Jarrett et al., 2009)\\n15. GPU-acceleratedmultilayerperceptron(Ciresan et al., 2010)\\n16. OMP-1network(Coatesand Ng,2011)\\n17. Distributedautoencoder(Le et al., 2012)\\n18. Multi-GPUconvolutionalnetwork(Krizhevsky et al., 2012)\\n19. COTSHPC unsupervised convolutionalnetwork(Coates et al., 2013)\\n20. GoogLeNet (Szegedyet al., 2014a)\\n27\\nCHAPTER 1. INTRODUCTION\\n0.30\\n0.25\\n0.20\\n0.15\\n0.10\\n0.05\\n0.00\\n2010 2011 2012 2013 2014 2015\\nYear\\netar\\nrorre\\nnoitacfiissalc\\nCRVSLI\\nFigure 1.12: Since deep networks reached the scale necessary to compete in the ImageNet'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Large Scale Visual Recognition Challenge, they have consistently won the competition\\nevery year, and yielded lower and lower error rates each time. Data from Russakovsky\\net al. (2014b) and He et al. (2015).\\n28\\nPart I\\nApplied Math and Machine\\nLearning Basics\\n29\\nThis part of the book introduces the basic mathematical concepts needed to\\nunderstand deep learning. We begin with general ideas from applied math that\\nallow us to define functions of many variables, find the highest and lowest points\\non these functions and quantify degrees of belief.\\nNext, we describe the fundamental goals of machine learning. We describe how\\nto accomplish these goals by specifying a model that represents certain beliefs,\\ndesigning a cost function that measures how well those beliefs correspond with\\nreality and using a training algorithm to minimize that cost function.\\nThis elementary framework is the basis for a broad variety of machine learning'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='algorithms, including approaches to machine learning that are not deep. In the\\nsubsequent parts of the book, we develop deep learning algorithms within this\\nframework.\\n30\\nChapter 2\\nLinear Algebra\\nLinear algebra is a branch of mathematics that is widely used throughout science\\nand engineering. However, because linear algebra is a form of continuous rather\\nthan discrete mathematics, many computer scientists have little experience with it.\\nA good understanding of linear algebra is essential for understanding and working\\nwith many machine learning algorithms, especially deep learning algorithms. We\\ntherefore precede our introduction to deep learning with a focused presentation of\\nthe key linear algebra prerequisites.\\nIf you are already familiar with linear algebra, feel free to skip this chapter. If\\nyou have previous experience with these concepts but need a detailed reference\\nsheet to review key formulas, we recommend The Matrix Cookbook (Petersen and'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Pedersen, 2006). If you have no exposure at all to linear algebra, this chapter\\nwill teach you enough to read this book, but we highly recommend that you also\\nconsult another resource focused exclusively on teaching linear algebra, such as\\nShilov (1977). This chapter will completely omit many important linear algebra\\ntopics that are not essential for understanding deep learning.\\n2.1 Scalars, Vectors, Matrices and Tensors\\nThe study of linear algebra involves several types of mathematical objects:\\nScalars: A scalar is just a single number, in contrast to most of the other\\n•\\nobjects studied in linear algebra, which are usually arrays of multiple numbers.\\nWe write scalars in italics. We usually give scalars lower-case variable names.\\nWhen we introduce them, we specify what kind of number they are. For\\n31\\nCHAPTER 2. LINEAR ALGEBRA\\nR\\nexample, we might say “Let s be the slope of the line,” while defining a\\n∈\\nN\\nreal-valued scalar, or “Let n be the number of units,” while defining a\\n∈'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='natural number scalar.\\nVectors: A vector is an array of numbers. The numbers are arranged in\\n•\\norder. We can identify each individual number by its index in that ordering.\\nTypically we give vectors lower case names written in bold typeface, such\\nas x. The elements of the vector are identified by writing its name in italic\\ntypeface, with a subscript. The first element of x is x , the second element\\n1\\nis x and so on. We also need to say what kind of numbers are stored in\\n2\\nR\\nthe vector. If each element is in , and the vector has n elements, then the\\nR\\nvector lies in the set formed by taking the Cartesian product of n times,\\ndenoted as\\nRn.\\nWhen we need to explicitly identify the elements of a vector,\\nwe write them as a column enclosed in square brackets:\\nx\\n1\\nx\\n2\\n\\uf8ee \\uf8f9\\nx = . . (2.1)\\n.\\n.\\n\\uf8ef \\uf8fa\\n\\uf8ef x \\uf8fa\\nn\\n\\uf8ef \\uf8fa\\n\\uf8f0 \\uf8fb\\nWe can think of vectors as identifying points in space, with each element\\ngiving the coordinate along a different axis.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Sometimes we need to index a set of elements of a vector. In this case, we\\ndefine a set containing the indices and write the set as a subscript. For\\nexample, to access x , x and x , we define the set S = 1,3,6 and write\\n1 3 6\\n{ }\\nx . We use the sign to index the complement of a set. For example x is\\nS 1\\n− −\\nthe vector containing all elements of x except for x , and x is the vector\\n1 S\\n−\\ncontaining all of the elements of x except for x , x and x .\\n1 3 6\\nMatrices: A matrix is a 2-D array of numbers, so each element is identified\\n•\\nby two indices instead of just one. We usually give matrices upper-case\\nvariable names with bold typeface, such as A. If a real-valued matrix A has\\na height of m and a width of n, then we say that A Rm n. We usually\\n×\\n∈\\nidentify the elements of a matrix using its name in italic but not bold font,\\nand the indices are listed with separating commas. For example, A is the\\n1,1\\nupper left entry of A and A is the bottom right entry. We can identify all\\nm,n'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of the numbers with vertical coordinate i by writing a “:” for the horizontal\\ncoordinate. For example, A denotes the horizontal cross section of A with\\ni,:\\nvertical coordinate i. This is known as the i-th row of A. Likewise, A is\\n:,i\\n32\\nCHAPTER 2. LINEAR ALGEBRA\\nA A\\n1,1 1,2 A A A\\nA=\\n\\uf8ee\\nA\\nA2 3, ,1\\n1\\nA\\nA2 3, ,2\\n2\\n\\uf8f9⇒A\\ue021 =\\n\\ue025\\nA1 1, ,1\\n2\\nA2 2, ,1\\n2\\nA3 3, ,1\\n2 \\ue026\\n\\uf8f0 \\uf8fb\\nFigure 2.1: The transpose of the matrix can be thought of as a mirror image across the\\nmain diagonal.\\nthe i-th column of A. When we need to explicitly identify the elements of\\na matrix, we write them as an array enclosed in square brackets:\\nA A\\n1,1 1,2\\n. (2.2)\\nA A\\n2,1 2,2\\n\\ue014 \\ue015\\nSometimes we may need to index matrix-valued expressions that are not just\\na single letter. In this case, we use subscripts after the expression, but do\\nnot convert anything to lower case. For example, f(A) gives element (i,j)\\ni,j\\nof the matrix computed by applying the function f to A.\\nTensors: In some cases we will need an array with more than two axes.\\n•'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In the general case, an array of numbers arranged on a regular grid with a\\nvariable number of axes is known as a tensor. We denote a tensor named “A”\\nwith this typeface: A. We identify the element of A at coordinates (i,j,k)\\nA\\nby writing .\\ni,j,k\\nOne important operation on matrices is the transpose. The transpose of a\\nmatrix is the mirror image of the matrix across a diagonal line, called the main\\ndiagonal, running down and to the right, starting from its upper left corner. See\\nfigure 2.1 for a graphical depiction of this operation. We denote the transpose of a\\nmatrix A as A , and it is defined such that\\n\\ue03e\\n(A ) = A . (2.3)\\n\\ue03e i,j j,i\\nVectors can be thought of as matrices that contain only one column. The\\ntranspose of a vector is therefore a matrix with only one row. Sometimes we\\n33\\nCHAPTER 2. LINEAR ALGEBRA\\ndefine a vector by writing out its elements in the text inline as a row matrix,\\nthen using the transpose operator to turn it into a standard column vector, e.g.,\\nx = [x ,x ,x ] .'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='1 2 3 \\ue03e\\nA scalar can be thought of as a matrix with only a single entry. From this, we\\ncan see that a scalar is its own transpose: a = a .\\n\\ue03e\\nWe can add matrices to each other, as long as they have the same shape, just\\nby adding their corresponding elements: C = A +B where C = A +B .\\ni,j i,j i,j\\nWe can also add a scalar to a matrix or multiply a matrix by a scalar, just\\nby performing that operation on each element of a matrix: D = a B +c where\\n·\\nD = a B +c.\\ni,j i,j\\n·\\nIn the context of deep learning, we also use some less conventional notation.\\nWe allow the addition of matrix and a vector, yielding another matrix: C = A +b,\\nwhere C = A +b . In other words, the vector b is added to each row of the\\ni,j i,j j\\nmatrix. This shorthand eliminates the need to define a matrix with b copied into\\neach row before doing the addition. This implicit copying of b to many locations\\nis called broadcasting.\\n2.2 Multiplying Matrices and Vectors'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='One of the most important operations involving matrices is multiplication of two\\nmatrices. The matrix product of matrices A and B is a third matrix C. In\\norder for this product to be defined, A must have the same number of columns as\\nB has rows. If A is of shape m n and B is of shape n p, then C is of shape\\n× ×\\nm p. We can write the matrix product just by placing two or more matrices\\n×\\ntogether, e.g.\\nC = AB. (2.4)\\nThe product operation is defined by\\nC = A B . (2.5)\\ni,j i,k k,j\\nk\\n\\ue058\\nNote that the standard product of two matrices is not just a matrix containing\\nthe product of the individual elements. Such an operation exists and is called the\\nelement-wise product or Hadamard product, and is denoted as A B.\\n\\ue00c\\nThe dot product between two vectors x and y of the same dimensionality\\nis the matrix product x y. We can think of the matrix product C = AB as\\n\\ue03e\\ncomputing C as the dot product between row i of A and column j of B.\\ni,j\\n34\\nCHAPTER 2. LINEAR ALGEBRA'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Matrix product operations have many useful properties that make mathematical\\nanalysis of matrices more convenient. For example, matrix multiplication is\\ndistributive:\\nA(B + C) = AB +AC. (2.6)\\nIt is also associative:\\nA(BC) = (AB)C. (2.7)\\nMatrix multiplication is not commutative (the condition AB = BA does not\\nalways hold), unlike scalar multiplication. However, the dot product between two\\nvectors is commutative:\\nx y = y x. (2.8)\\n\\ue03e \\ue03e\\nThe transpose of a matrix product has a simple form:\\n(AB) = B A . (2.9)\\n\\ue03e \\ue03e \\ue03e\\nThis allows us to demonstrate equation 2.8, by exploiting the fact that the value\\nof such a product is a scalar and therefore equal to its own transpose:\\nx y = x y \\ue03e = y x. (2.10)\\n\\ue03e \\ue03e \\ue03e\\n\\ue010 \\ue011\\nSince the focus of this textbook is not linear algebra, we do not attempt to\\ndevelop a comprehensive list of useful properties of the matrix product here, but\\nthe reader should be aware that many more exist.\\nWe now know enough linear algebra notation to write down a system of linear\\nequations:'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Ax = b (2.11)\\nwhere A Rm n is a known matrix, b Rm is a known vector, and x Rn is a\\n×\\n∈ ∈ ∈\\nvector of unknown variables we would like to solve for. Each element x of xis one\\ni\\nof these unknown variables. Each row of A and each element of b provide another\\nconstraint. We can rewrite equation 2.11 as:\\nA x = b (2.12)\\n1,: 1\\nA x = b (2.13)\\n2,: 2\\n... (2.14)\\nA x = b (2.15)\\nm,: m\\nor, even more explicitly, as:\\nA x +A x + + A x = b (2.16)\\n1,1 1 1,2 2 1,n n 1\\n···\\n35\\nCHAPTER 2. LINEAR ALGEBRA\\n1 0 0\\n0 1 0\\n\\uf8ee \\uf8f9\\n0 0 1\\n\\uf8f0 \\uf8fb\\nFigure 2.2: Example identity matrix: This is I .\\n3\\nA x +A x + + A x = b (2.17)\\n2,1 1 2,2 2 2,n n 2\\n···\\n... (2.18)\\nA x +A x + + A x = b . (2.19)\\nm,1 1 m,2 2 m,n n m\\n···\\nMatrix-vector product notation provides a more compact representation for\\nequations of this form.\\n2.3 Identity and Inverse Matrices\\nLinear algebra offers a powerful tool called matrix inversion that allows us to\\nanalytically solve equation 2.11 for many values of A.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='To describe matrix inversion, we first need to define the concept of an identity\\nmatrix. An identity matrix is a matrix that does not change any vector when we\\nmultiply that vector by that matrix. We denote the identity matrix that preserves\\nn-dimensional vectors as I . Formally, I Rn n, and\\nn n ×\\n∈\\nx\\nRn,I\\nx = x. (2.20)\\nn\\n∀ ∈\\nThe structure of the identity matrix is simple: all of the entries along the main\\ndiagonal are 1, while all of the other entries are zero. See figure 2.2 for an example.\\nThe matrix inverse of A is denoted as A 1, and it is defined as the matrix\\n−\\nsuch that\\nA 1A = I . (2.21)\\n− n\\nWe can now solve equation 2.11 by the following steps:\\nAx = b (2.22)\\nA 1Ax = A 1b (2.23)\\n− −\\nI x = A 1b (2.24)\\nn −\\n36\\nCHAPTER 2. LINEAR ALGEBRA\\nx = A 1b. (2.25)\\n−\\nOf course, this process depends on it being possible to find A 1. We discuss\\n−\\nthe conditions for the existence of A 1 in the following section.\\n−\\nWhen A 1 exists, several different algorithms exist for finding it in closed form.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='−\\nIn theory, the same inverse matrix can then be used to solve the equation many\\ntimes for different values of b. However,A 1 is primarily useful as a theoretical\\n−\\ntool, and should not actually be used in practice for most software applications.\\nBecause A 1 can be represented with only limited precision on a digital computer,\\n−\\nalgorithms that make use of the value of b can usually obtain more accurate\\nestimates of x.\\n2.4 Linear Dependence and Span\\nIn order for A 1 to exist, equation 2.11 must have exactly one solution for every\\n−\\nvalue of b. However, it is also possible for the system of equations to have no\\nsolutions or infinitely many solutions for some values of b. It is not possible to\\nhave more than one but less than infinitely many solutions for a particular b; if\\nboth x and y are solutions then\\nz = αx + (1 α)y (2.26)\\n−\\nis also a solution for any real α.\\nTo analyze how many solutions the equation has, we can think of the columns'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of A as specifying different directions we can travel from the origin (the point\\nspecified by the vector of all zeros), and determine how many ways there are of\\nreaching b. In this view, each element ofx specifies how far we should travel in\\neach of these directions, with x specifying how far to move in the direction of\\ni\\ncolumn i:\\nAx = x A . (2.27)\\ni :,i\\ni\\n\\ue058\\nIn general, this kind of operation is called a linear combination. Formally, a\\nlinear combination of some set of vectors v(1),...,v(n) is given by multiplying\\n{ }\\neach vector v(i) by a corresponding scalar coefficient and adding the results:\\nc v(i). (2.28)\\ni\\ni\\n\\ue058\\nThe spanof a set of vectors is the set of all points obtainable by linear combination\\nof the original vectors.\\n37\\nCHAPTER 2. LINEAR ALGEBRA\\nDetermining whether Ax= b has a solution thus amounts to testing whether b\\nis in the span of the columns of A. This particular span is known as the column\\nspace or the range of A.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In order for the system Ax = b to have a solution for all values of b\\nRm,\\n∈\\nwe therefore require that the column space of Abe all of\\nRm.\\nIf any point in\\nRm\\nis excluded from the column space, that point is a potential value of bthat has\\nno solution. The requirement that the column space of A be all of\\nRm\\nimplies\\nimmediately that A must have at least m columns, i.e., n m. Otherwise, the\\n≥\\ndimensionality of the column space would be less than m. For example, consider a\\n3 2 matrix. The target bis 3-D, but x is only 2-D, so modifying the value of x\\n×\\nat best allows us to trace out a 2-D plane within\\nR3.\\nThe equation has a solution\\nif and only if b lies on that plane.\\nHaving n m is only a necessary condition for every point to have a solution.\\n≥\\nIt is not a sufficient condition, because it is possible for some of the columns to\\nbe redundant. Consider a 2 2 matrix where both of the columns are identical.\\n×\\nThis has the same column space as a 2 1 matrix containing only one copy of the\\n×'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='replicated column. In other words, the column space is still just a line, and fails to\\nencompass all of\\nR2,\\neven though there are two columns.\\nFormally, this kind of redundancy is known as linear dependence. A set of\\nvectors is linearly independent if no vector in the set is a linear combination\\nof the other vectors. If we add a vector to a set that is a linear combination of\\nthe other vectors in the set, the new vector does not add any points to the set’s\\nspan. This means that for the column space of the matrix to encompass all of\\nRm,\\nthe matrix must contain at least one set of m linearly independent columns. This\\ncondition is both necessary and sufficient for equation 2.11 to have a solution for\\nevery value of b. Note that the requirement is for a set to have exactly m linear\\nindependent columns, not at least m. No set of m-dimensional vectors can have\\nmore than m mutually linearly independent columns, but a matrix with more than\\nm columns may have more than one such set.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In order for the matrix to have an inverse, we additionally need to ensure that\\nequation 2.11 has at most one solution for each value of b. To do so, we need to\\nensure that the matrix has at most m columns. Otherwise there is more than one\\nway of parametrizing each solution.\\nTogether, this means that the matrix must be square, that is, we require that\\nm = n and that all of the columns must be linearly independent. A square matrix\\nwith linearly dependent columns is known as singular.\\nIf A is not square or is square but singular, it can still be possible to solve the\\nequation. However, we can not use the method of matrix inversion to find the\\n38\\nCHAPTER 2. LINEAR ALGEBRA\\nsolution.\\nSo far we have discussed matrix inverses as being multiplied on the left. It is\\nalso possible to define an inverse that is multiplied on the right:\\nAA 1 = I. (2.29)\\n−\\nFor square matrices, the left inverse and right inverse are equal.\\n2.5 Norms'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Sometimes we need to measure the size of a vector. In machine learning, we usually\\nmeasure the size of vectors using a function called a norm. Formally, the Lp norm\\nis given by\\n1\\np\\nx = x p (2.30)\\np i\\n|| || | |\\n\\ue020 \\ue021\\ni\\n\\ue058\\nR\\nfor p ,p 1.\\n∈ ≥\\nNorms, including the Lp norm, are functions mapping vectors to non-negative\\nvalues. On an intuitive level, the norm of a vector x measures the distance from\\nthe origin to the point x. More rigorously, a norm is any function f that satisfies\\nthe following properties:\\nf(x) = 0 x = 0\\n• ⇒\\nf(x+ y) f(x) + f(y) (the triangle inequality)\\n• ≤\\nR\\nα ,f(αx) = α f(x)\\n• ∀ ∈ | |\\nThe L2 norm, with p = 2, is known as the Euclidean norm. It is simply the\\nEuclidean distance from the origin to the point identified by x. The L2 norm is\\nused so frequently in machine learning that it is often denoted simply as x , with\\n|| ||\\nthe subscript 2 omitted. It is also common to measure the size of a vector using\\nthe squared L2 norm, which can be calculated simply as x x.\\n\\ue03e'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The squared L2 norm is more convenient to work with mathematically and\\ncomputationally than the L2 norm itself. For example, the derivatives of the\\nsquared L2 norm with respect to each element of x each depend only on the\\ncorresponding element of x, while all of the derivatives of the L2 norm depend\\non the entire vector. In many contexts, the squared L2 norm may be undesirable\\nbecause it increases very slowly near the origin. In several machine learning\\n39\\nCHAPTER 2. LINEAR ALGEBRA\\napplications, it is important to discriminate between elements that are exactly\\nzero and elements that are small but nonzero. In these cases, we turn to a function\\nthat grows at the same rate in all locations, but retains mathematical simplicity:\\nthe L1 norm. The L1 norm may be simplified to\\nx = x . (2.31)\\n1 i\\n|| || | |\\ni\\n\\ue058\\nThe L1 norm is commonly used in machine learning when the difference between\\nzero and nonzero elements is very important. Every time an element of x moves'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='away from 0 by \\ue00f, the L1 norm increases by \\ue00f.\\nWe sometimes measure the size of the vector by counting its number of nonzero\\nelements. Some authors refer to this function as the “L0 norm,” but this is incorrect\\nterminology. The number of non-zero entries in a vector is not a norm, because\\nscaling the vector by α does not change the number of nonzero entries. The L1\\nnorm is often used as a substitute for the number of nonzero entries.\\nOne other norm that commonly arises in machine learning is the L norm,\\n∞\\nalso known as the max norm. This norm simplifies to the absolute value of the\\nelement with the largest magnitude in the vector,\\nx = max x . (2.32)\\ni\\n|| ||∞ i | |\\nSometimes we may also wish to measure the size of a matrix. In the context\\nof deep learning, the most common way to do this is with the otherwise obscure\\nFrobenius norm:\\nA = A2 , (2.33)\\n|| ||F i,j\\n\\ue073 i,j\\n\\ue058\\nwhich is analogous to the L2 norm of a vector.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The dot product of two vectors can be rewritten in terms of norms. Specifically,\\nx y = x y cosθ (2.34)\\n\\ue03e 2 2\\n|| || || ||\\nwhere θ is the angle between x and y.\\n2.6 Special Kinds of Matrices and Vectors\\nSome special kinds of matrices and vectors are particularly useful.\\nDiagonal matrices consist mostly of zeros and have non-zero entries only along\\nthe main diagonal. Formally, a matrix D is diagonal if and only if D = 0 for\\ni,j\\n40\\nCHAPTER 2. LINEAR ALGEBRA\\nall i = j. We have already seen one example of a diagonal matrix: the identity\\n\\ue036\\nmatrix, where all of the diagonal entries are 1. We write diag(v) to denote a square\\ndiagonal matrix whose diagonal entries are given by the entries of the vector v.\\nDiagonal matrices are of interest in part because multiplying by a diagonal matrix\\nis very computationally efficient. To compute diag(v)x, we only need to scale each\\nelement x by v . In other words, diag(v)x = v x. Inverting a square diagonal\\ni i\\n\\ue00c'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='matrix is also efficient. The inverse exists only if every diagonal entry is nonzero,\\nand in that case, diag(v) 1 = diag([1/v ,...,1/v ] ). In many cases, we may\\n− 1 n \\ue03e\\nderive some very general machine learning algorithm in terms of arbitrary matrices,\\nbut obtain a less expensive (and less descriptive) algorithm by restricting some\\nmatrices to be diagonal.\\nNot alldiagonal matrices need be square. It ispossible toconstruct a rectangular\\ndiagonal matrix. Non-square diagonal matrices do not have inverses but it is still\\npossible to multiply by them cheaply. For a non-square diagonal matrix D, the\\nproduct Dx will involve scaling each element of x, and either concatenating some\\nzeros to the result if D is taller than it is wide, or discarding some of the last\\nelements of the vector if D is wider than it is tall.\\nA symmetric matrix is any matrix that is equal to its own transpose:\\nA = A . (2.35)\\n\\ue03e\\nSymmetric matrices often arise when the entries are generated by some function of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='two arguments that does not depend on the order of the arguments. For example,\\nif A is a matrix of distance measurements, with A giving the distance from point\\ni,j\\ni to point j, then A = A because distance functions are symmetric.\\ni,j j,i\\nA unit vector is a vector with unit norm:\\nx = 1. (2.36)\\n2\\n|| ||\\nA vector x and a vector y are orthogonal to each other if x y = 0. If both\\n\\ue03e\\nvectors have nonzero norm, this means that they are at a 90 degree angle to each\\nother. In\\nRn,\\nat most n vectors may be mutually orthogonal with nonzero norm.\\nIf the vectors are not only orthogonal but also have unit norm, we call them\\northonormal.\\nAn orthogonal matrix is a square matrix whose rows are mutually orthonor-\\nmal and whose columns are mutually orthonormal:\\nA A = AA = I. (2.37)\\n\\ue03e \\ue03e\\n41\\nCHAPTER 2. LINEAR ALGEBRA\\nThis implies that\\nA 1 = A , (2.38)\\n− \\ue03e\\nso orthogonal matrices are of interest because their inverse is very cheap to compute.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Pay careful attention to the definition of orthogonal matrices. Counterintuitively,\\ntheir rows are not merely orthogonal but fully orthonormal. There is no special\\nterm for a matrix whose rows or columns are orthogonal but not orthonormal.\\n2.7 Eigendecomposition\\nMany mathematical objects can be understood better by breaking them into\\nconstituent parts, or finding some properties of them that are universal, not caused\\nby the way we choose to represent them.\\nFor example, integers can be decomposed into prime factors. The way we\\nrepresent the number 12 will change depending on whether we write it in base ten\\nor in binary, but it will always be true that 12 = 2 2 3. From this representation\\n× ×\\nwe can conclude useful properties, such as that 12 is not divisible by 5, or that any\\ninteger multiple of 12 will be divisible by 3.\\nMuch as we can discover something about the true nature of an integer by\\ndecomposing it into prime factors, we can also decompose matrices in ways that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='show us information about their functional properties that is not obvious from the\\nrepresentation of the matrix as an array of elements.\\nOne of the most widely used kinds of matrix decomposition is called eigen-\\ndecomposition, in which we decompose a matrix into a set of eigenvectors and\\neigenvalues.\\nAn eigenvector of a square matrix A is a non-zero vector v such that multi-\\nplication by A alters only the scale of v:\\nAv = λv. (2.39)\\nThe scalar λ is known as the eigenvalue corresponding to this eigenvector. (One\\ncan also find a left eigenvector such that v A = λv , but we are usually\\n\\ue03e \\ue03e\\nconcerned with right eigenvectors).\\nR\\nIf v is an eigenvector of A, then so is any rescaled vector sv for s ,s = 0.\\n∈ \\ue036\\nMoreover, sv still has the same eigenvalue. For this reason, we usually only look\\nfor unit eigenvectors.\\nSuppose that a matrix A has nlinearly independent eigenvectors, v(1),...,\\n{\\nv(n) , with corresponding eigenvalues λ ,...,λ . We may concatenate all of the\\n1 n\\n} { }\\n42'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 2. LINEAR ALGEBRA\\n\\ue033\\n\\ue032\\n\\ue031\\n\\ue030\\n\\uf091\\ue031\\n\\uf091\\ue032\\n\\uf091\\ue033\\n\\uf091\\ue033 \\uf091\\ue032 \\uf091\\ue031 \\ue030 \\ue031 \\ue032 \\ue033\\n\\ue078\\n\\ue030\\n\\ue078\\n\\ue031\\n\\ue042\\ue065\\ue066\\ue06f\\ue072\\ue065\\ue020\\ue06d\\ue075\\ue06c\\ue074\\ue069\\ue070\\ue06c\\ue069\\ue063\\ue061\\ue074\\ue069\\ue06f\\ue06e\\n\\ue033\\n\\ue032\\n\\ue076\\ue028\\ue031\\ue029 \\ue031\\n\\ue030\\n\\ue076\\ue028\\ue032\\ue029\\n\\uf091\\ue031\\n\\uf091\\ue032\\n\\uf091\\ue033\\n\\uf091\\ue033 \\uf091\\ue032 \\uf091\\ue031 \\ue030 \\ue031 \\ue032 \\ue033\\n\\ue078\\n\\ue030\\ue030\\n\\ue078\\n\\ue031\\ue030\\n\\ue045\\ue066\\ue066\\ue065\\ue063\\ue074\\ue020\\ue06f\\ue066\\ue020\\ue065\\ue069\\ue067\\ue065\\ue06e\\ue076\\ue065\\ue063\\ue074\\ue06f\\ue072\\ue073\\ue020\\ue061\\ue06e\\ue064\\ue020\\ue065\\ue069\\ue067\\ue065\\ue06e\\ue076\\ue061\\ue06c\\ue075\\ue065\\ue073\\n\\ue041\\ue066\\ue074\\ue065\\ue072\\ue020\\ue06d\\ue075\\ue06c\\ue074\\ue069\\ue070\\ue06c\\ue069\\ue063\\ue061\\ue074\\ue069\\ue06f\\ue06e\\n\\ue0b8 \\ue076\\ue028\\ue031\\ue029\\n\\ue031\\n\\ue076\\ue028\\ue031\\ue029\\n\\ue0b8 \\ue076\\ue028\\ue032\\ue029\\n\\ue032 \\ue076\\ue028\\ue032\\ue029\\nFigure 2.3: An example of the effect of eigenvectors and eigenvalues. Here, we have\\na matrix A with two orthonormal eigenvectors, v(1) with eigenvalue λ and v(2) with\\n1\\neigenvalue λ . (Left)We plot the set of all unit vectors u R2 as a unit circle. (Right)We\\n2\\n∈\\nplot the set of all points Au. By observing the way that A distorts the unit circle, we\\ncan see that it scales space in direction v(i) by λ .\\ni\\neigenvectors to form a matrix V with one eigenvector per column: V = [v(1),...,\\nv(n)]. Likewise, we can concatenate the eigenvalues to form a vector λ = [λ ,...,\\n1\\nλ ] . The eigendecomposition of A is then given by\\nn \\ue03e\\nA = V diag(λ)V 1. (2.40)\\n−\\nWe have seen that constructing matrices with specific eigenvalues and eigenvec-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='tors allows us to stretch space in desired directions. However, we often want to\\ndecompose matrices into their eigenvalues and eigenvectors. Doing so can help\\nus to analyze certain properties of the matrix, much as decomposing an integer\\ninto its prime factors can help us understand the behavior of that integer.\\nNot every matrix can be decomposed into eigenvalues and eigenvectors. In some\\n43\\nCHAPTER 2. LINEAR ALGEBRA\\ncases, the decomposition exists, but may involve complex rather than real numbers.\\nFortunately, in this book, we usually need to decompose only a specific class of\\nmatrices that have a simple decomposition. Specifically, every real symmetric\\nmatrix can be decomposed into an expression using only real-valued eigenvectors\\nand eigenvalues:\\nA = QΛQ , (2.41)\\n\\ue03e\\nwhere Q is an orthogonal matrix composed of eigenvectors of A, and Λ is a\\ndiagonal matrix. The eigenvalue Λ is associated with the eigenvector in column i\\ni,i'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of Q, denoted as Q . Because Q is an orthogonal matrix, we can think of A as\\n:,i\\nscaling space by λ in direction v(i). See figure 2.3 for an example.\\ni\\nWhile any real symmetric matrix A is guaranteed to have an eigendecomposi-\\ntion, the eigendecomposition may not be unique. If any two or more eigenvectors\\nshare the same eigenvalue, then any set of orthogonal vectors lying in their span\\nare also eigenvectors with that eigenvalue, and we could equivalently choose a Q\\nusing those eigenvectors instead. By convention, we usually sort the entries of Λ\\nin descending order. Under this convention, the eigendecomposition is unique only\\nif all of the eigenvalues are unique.\\nThe eigendecomposition of a matrix tells us many useful facts about the\\nmatrix. The matrix is singular if and only if any of the eigenvalues are zero.\\nThe eigendecomposition of a real symmetric matrix can also be used to optimize\\nquadratic expressions of the form f(x) = x Ax subject to x = 1. Whenever x\\n\\ue03e 2\\n|| ||'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='is equal to an eigenvector of A, f takes on the value of the corresponding eigenvalue.\\nThe maximum value of f within the constraint region is the maximum eigenvalue\\nand its minimum value within the constraint region is the minimum eigenvalue.\\nA matrix whose eigenvalues are all positive is called positive definite. A\\nmatrix whose eigenvalues are all positive or zero-valued is calledpositive semidefi-\\nnite. Likewise, if all eigenvalues are negative, the matrix is negative definite, and\\nif all eigenvalues are negative or zero-valued, it is negative semidefinite. Positive\\nsemidefinite matrices are interesting because they guarantee that x, x Ax 0.\\n\\ue03e\\n∀ ≥\\nPositive definite matrices additionally guarantee that x Ax = 0 x = 0.\\n\\ue03e\\n⇒\\n2.8 Singular Value Decomposition\\nIn section 2.7, we saw how to decompose a matrix into eigenvectors and eigenvalues.\\nThe singular value decomposition (SVD) provides another way to factorize\\na matrix, into singular vectors and singular values. The SVD allows us to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='discover some of the same kind of information as the eigendecomposition. However,\\n44\\nCHAPTER 2. LINEAR ALGEBRA\\nthe SVD is more generally applicable. Every real matrix has a singular value\\ndecomposition, but the same is not true of the eigenvalue decomposition. For\\nexample, if a matrix is not square, the eigendecomposition is not defined, and we\\nmust use a singular value decomposition instead.\\nRecall that the eigendecomposition involves analyzing a matrix A to discover\\na matrix V of eigenvectors and a vector of eigenvalues λ such that we can rewrite\\nA as\\nA = V diag(λ)V 1. (2.42)\\n−\\nThe singular value decomposition is similar, except this time we will write A\\nas a product of three matrices:\\nA = UDV . (2.43)\\n\\ue03e\\nSuppose that A is an m n matrix. Then U is defined to be an m m matrix,\\n× ×\\nD to be an m n matrix, and V to be an n n matrix.\\n× ×\\nEach of these matrices is defined to have a special structure. The matrices U'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='and V are both defined to be orthogonal matrices. The matrix D is defined to be\\na diagonal matrix. Note that D is not necessarily square.\\nThe elements along the diagonal of D are known as the singular values of\\nthe matrix A. The columns of U are known as the left-singular vectors. The\\ncolumns of V are known as as the right-singular vectors.\\nWe can actually interpret the singular value decomposition of A in terms of\\nthe eigendecomposition of functions of A. The left-singular vectors ofA are the\\neigenvectors of AA . The right-singular vectors of Aare the eigenvectors of A A.\\n\\ue03e \\ue03e\\nThe non-zero singular values of A are the square roots of the eigenvalues of A A.\\n\\ue03e\\nThe same is true for AA .\\n\\ue03e\\nPerhaps the most useful feature of the SVD is that we can use it to partially\\ngeneralize matrix inversion to non-square matrices, as we will see in the next\\nsection.\\n2.9 The Moore-Penrose Pseudoinverse\\nMatrix inversion is not defined for matrices that are not square. Suppose we want'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to make a left-inverse B of a matrix A, so that we can solve a linear equation\\nAx = y (2.44)\\n45\\nCHAPTER 2. LINEAR ALGEBRA\\nby left-multiplying each side to obtain\\nx = By. (2.45)\\nDepending on the structure of the problem, it may not be possible to design a\\nunique mapping from A to B.\\nIf A is taller than it is wide, then it is possible for this equation to have\\nno solution. If A is wider than it is tall, then there could be multiple possible\\nsolutions.\\nThe Moore-Penrose pseudoinverse allows us to make some headway in\\nthese cases. The pseudoinverse of A is defined as a matrix\\nA+ = lim(A A+ αI) 1A . (2.46)\\n\\ue03e − \\ue03e\\nα 0\\n\\ue026\\nPractical algorithms for computing the pseudoinverse are not based on this defini-\\ntion, but rather the formula\\nA+ = V D+U , (2.47)\\n\\ue03e\\nwhere U, Dand V are thesingular value decompositionof A, and the pseudoinverse\\nD+ of a diagonal matrix D is obtained by taking the reciprocal of its non-zero\\nelements then taking the transpose of the resulting matrix.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='When A has more columns than rows, then solving a linear equation using the\\npseudoinverse provides one of the many possible solutions. Specifically, it provides\\nthe solution x = A+y with minimal Euclidean norm x among all possible\\n2\\n|| ||\\nsolutions.\\nWhen A has more rows than columns, it is possible for there to be no solution.\\nIn this case, using the pseudoinverse gives us the x for which Ax is as close as\\npossible to y in terms of Euclidean norm Ax y .\\n2\\n|| − ||\\n2.10 The Trace Operator\\nThe trace operator gives the sum of all of the diagonal entries of a matrix:\\nTr(A) = A . (2.48)\\ni,i\\ni\\n\\ue058\\nThe trace operator is useful for a variety of reasons. Some operations that are\\ndifficult to specify without resorting to summation notation can be specified using\\n46\\nCHAPTER 2. LINEAR ALGEBRA\\nmatrix products and the trace operator. For example, the trace operator provides\\nan alternative way of writing the Frobenius norm of a matrix:\\nA = Tr(AA ). (2.49)\\nF \\ue03e\\n|| ||\\n\\ue071'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Writing an expression in terms of the trace operator opens up opportunities to\\nmanipulate the expression using many useful identities. For example, the trace\\noperator is invariant to the transpose operator:\\nTr(A) = Tr(A ). (2.50)\\n\\ue03e\\nThe trace of a square matrix composed of many factors is also invariant to\\nmoving the last factor into the first position, if the shapes of the corresponding\\nmatrices allow the resulting product to be defined:\\nTr(ABC) = Tr(CAB) = Tr(BCA) (2.51)\\nor more generally,\\nn n 1\\n−\\nTr( F(i)) = Tr(F(n) F(i)). (2.52)\\ni=1 i=1\\n\\ue059 \\ue059\\nThis invariance to cyclic permutation holds even if the resulting product has a\\ndifferent shape. For example, for A Rm n and B Rn m, we have\\n× ×\\n∈ ∈\\nTr(AB) = Tr(BA) (2.53)\\neven though AB Rm m and BA Rn n.\\n× ×\\n∈ ∈\\nAnother useful fact to keep in mind is that a scalar is its own trace: a = Tr(a).\\n2.11 The Determinant\\nThe determinant of a square matrix, denoted det(A), is a function mapping'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='matrices to real scalars. The determinant is equal to the product of all the\\neigenvalues of the matrix. The absolute value of the determinant can be thought\\nof as a measure of how much multiplication by the matrix expands or contracts\\nspace. If the determinant is 0, then space is contracted completely along at least\\none dimension, causing it to lose all of its volume. If the determinant is 1, then\\nthe transformation preserves volume.\\n47\\nCHAPTER 2. LINEAR ALGEBRA\\n2.12 Example: Principal Components Analysis\\nOne simple machine learning algorithm, principal components analysis or PCA\\ncan be derived using only knowledge of basic linear algebra.\\nSuppose we have a collection of m points x(1),...,x(m) in Rn. Suppose we\\n{ }\\nwould like to apply lossy compression to these points. Lossy compression means\\nstoring the points in a way that requires less memory but may lose some precision.\\nWe would like to lose as little precision as possible.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='One way we can encode these points is to represent a lower-dimensional version\\nof them. For each point x(i) Rn we will find a corresponding code vector c(i) Rl.\\n∈ ∈\\nIf l is smaller than n, it will take less memory to store the code points than the\\noriginal data. We will want to find some encoding function that produces the code\\nfor an input, f(x) = c, and a decoding function that produces the reconstructed\\ninput given its code, x g(f(x)).\\n≈\\nPCA is defined by our choice of the decoding function. Specifically, to make the\\ndecoder very simple, we choose to use matrix multiplication to map the code back\\ninto Rn. Let g(c) = Dc, where D Rn l is the matrix defining the decoding.\\n×\\n∈\\nComputing the optimal code for this decoder could be a difficult problem. To\\nkeep the encoding problem easy, PCA constrains the columns of D to be orthogonal\\nto each other. (Note that D is still not technically “an orthogonal matrix” unless\\nl = n)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='With the problem as described so far, many solutions are possible, because we\\ncan increase the scale of D if we decrease c proportionally for all points. To give\\n:,i i\\nthe problem a unique solution, we constrain all of the columns of D to have unit\\nnorm.\\nIn order to turn this basic idea into an algorithm we can implement, the first\\nthing we need to do is figure out how to generate the optimal code point c for\\n∗\\neach input point x. One way to do this is to minimize the distance between the\\ninput point x and its reconstruction, g(c ). We can measure this distance using a\\n∗\\nnorm. In the principal components algorithm, we use the L2 norm:\\nc = argmin x g(c) . (2.54)\\n∗ 2\\n|| − ||\\nc\\nWe can switch to the squared L2 norm instead of the L2 norm itself, because\\nboth are minimized by the same value of c. Both are minimized by the same\\nvalue of c because the L2 norm is non-negative and the squaring operation is\\n48\\nCHAPTER 2. LINEAR ALGEBRA\\nmonotonically increasing for non-negative arguments.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='c = argmin x g(c) 2. (2.55)\\n∗ 2\\n|| − ||\\nc\\nThe function being minimized simplifies to\\n(x g(c)) (x g(c)) (2.56)\\n\\ue03e\\n− −\\n(by the definition of the L2 norm, equation 2.30)\\n= x x x g(c) g(c) x+ g(c) g(c) (2.57)\\n\\ue03e \\ue03e \\ue03e \\ue03e\\n− −\\n(by the distributive property)\\n= x x 2x g(c) +g(c) g(c) (2.58)\\n\\ue03e \\ue03e \\ue03e\\n−\\n(because the scalar g(c) x is equal to the transpose of itself).\\n\\ue03e\\nWe can now change the function being minimized again, to omit the first term,\\nsince this term does not depend on c:\\nc = argmin 2x g(c) +g(c) g(c). (2.59)\\n∗ \\ue03e \\ue03e\\n−\\nc\\nTo make further progress, we must substitute in the definition of g(c):\\nc = argmin 2x Dc +c D Dc (2.60)\\n∗ \\ue03e \\ue03e \\ue03e\\n−\\nc\\n= argmin 2x Dc +c I c (2.61)\\n\\ue03e \\ue03e l\\n−\\nc\\n(by the orthogonality and unit norm constraints on D)\\n= argmin 2x Dc +c c (2.62)\\n\\ue03e \\ue03e\\n−\\nc\\nWe can solve this optimization problem using vector calculus (see section 4.3 if\\nyou do not know how to do this):\\n( 2x Dc +c c) = 0 (2.63)\\nc \\ue03e \\ue03e\\n∇ −\\n2D x+ 2c = 0 (2.64)\\n\\ue03e\\n−\\nc = D x. (2.65)\\n\\ue03e\\n49\\nCHAPTER 2. LINEAR ALGEBRA'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='This makes the algorithm efficient: we can optimally encode x just using a\\nmatrix-vector operation. To encode a vector, we apply the encoder function\\nf(x) = D x. (2.66)\\n\\ue03e\\nUsing a further matrix multiplication, we can also define the PCA reconstruction\\noperation:\\nr(x) = g(f (x)) = DD x. (2.67)\\n\\ue03e\\nNext, we need to choose the encoding matrix D. To do so, we revisit the idea\\nof minimizing the L2 distance between inputs and reconstructions. Since we will\\nuse the same matrix D to decode all of the points, we can no longer consider the\\npoints in isolation. Instead, we must minimize the Frobenius norm of the matrix\\nof errors computed over all dimensions and all points:\\n2\\nD = argmin x(i) r(x(i)) subject to D D = I (2.68)\\n∗ j − j \\ue03e l\\nD \\ue073 \\ue058i,j \\ue010 \\ue011\\nTo derive the algorithm for finding D , we will start by considering the case\\n∗\\nwhere l = 1. In this case, D is just a single vector, d. Substituting equation 2.67\\ninto equation 2.68 and simplifying D into d, the problem reduces to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='d = argmin x(i) dd x(i) 2 subject to d = 1. (2.69)\\n∗ \\ue03e 2 2\\n|| − || || ||\\nd\\ni\\n\\ue058\\nThe above formulation is the most direct way of performing the substitution,\\nbut is not the most stylistically pleasing way to write the equation. It places the\\nscalar value d x(i) on the right of the vector d. It is more conventional to write\\n\\ue03e\\nscalar coefficients on the left of vector they operate on. We therefore usually write\\nsuch a formula as\\nd = argmin x(i) d x(i)d 2 subject to d = 1, (2.70)\\n∗ \\ue03e 2 2\\n|| − || || ||\\nd\\ni\\n\\ue058\\nor, exploiting the fact that a scalar is its own transpose, as\\nd = argmin x(i) x(i) dd 2 subject to d = 1. (2.71)\\n∗ \\ue03e 2 2\\n|| − || || ||\\nd\\ni\\n\\ue058\\nThe reader should aim to become familiar with such cosmetic rearrangements.\\n50\\nCHAPTER 2. LINEAR ALGEBRA\\nAt this point, it can be helpful to rewrite the problem in terms of a single\\ndesign matrix of examples, rather than as a sum over separate example vectors.\\nThis will allow us to use more compact notation. Let X Rm n be the matrix\\n×\\n∈'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='defined by stacking all of the vectors describing the points, such that X =x(i) \\ue03e.\\ni,:\\nWe can now rewrite the problem as\\nd = argmin X Xdd 2 subject to d d = 1. (2.72)\\n∗ \\ue03e F \\ue03e\\n|| − ||\\nd\\nDisregarding the constraint for the moment, we can simplify the Frobenius norm\\nportion as follows:\\nargmin X Xdd 2 (2.73)\\n\\ue03e F\\n|| − ||\\nd\\n= argminTr X Xdd \\ue03e X Xdd (2.74)\\n\\ue03e \\ue03e\\n− −\\nd\\n\\ue012 \\ue013\\n\\ue010 \\ue011 \\ue010 \\ue011\\n(by equation 2.49)\\n= argminTr(X X X Xdd dd X X +dd X Xdd ) (2.75)\\n\\ue03e \\ue03e \\ue03e \\ue03e \\ue03e \\ue03e \\ue03e \\ue03e\\n− −\\nd\\n= argminTr(X X) Tr(X Xdd ) Tr(dd X X)+ Tr(dd X Xdd )\\n\\ue03e \\ue03e \\ue03e \\ue03e \\ue03e \\ue03e \\ue03e \\ue03e\\n− −\\nd\\n(2.76)\\n= argmin Tr(X Xdd ) Tr(dd X X)+ Tr(dd X Xdd ) (2.77)\\n\\ue03e \\ue03e \\ue03e \\ue03e \\ue03e \\ue03e \\ue03e\\n− −\\nd\\n(because terms not involving d do not affect the argmin)\\n= argmin 2Tr(X Xdd )+ Tr(dd X Xdd ) (2.78)\\n\\ue03e \\ue03e \\ue03e \\ue03e \\ue03e\\n−\\nd\\n(because we can cycle the order of the matrices inside a trace, equation 2.52)\\n= argmin 2Tr(X Xdd )+ Tr(X Xdd dd ) (2.79)\\n\\ue03e \\ue03e \\ue03e \\ue03e \\ue03e\\n−\\nd\\n(using the same property again)\\nAt this point, we re-introduce the constraint:'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='argmin 2Tr(X Xdd )+ Tr(X Xdd dd ) subject to d d = 1 (2.80)\\n\\ue03e \\ue03e \\ue03e \\ue03e \\ue03e \\ue03e\\n−\\nd\\n= argmin 2Tr(X Xdd )+ Tr(X Xdd ) subject to d d = 1 (2.81)\\n\\ue03e \\ue03e \\ue03e \\ue03e \\ue03e\\n−\\nd\\n(due to the constraint)\\n= argmin Tr(X Xdd ) subject to d d = 1 (2.82)\\n\\ue03e \\ue03e \\ue03e\\n−\\nd\\n51\\nCHAPTER 2. LINEAR ALGEBRA\\n= argmax Tr(X Xdd ) subject to d d = 1 (2.83)\\n\\ue03e \\ue03e \\ue03e\\nd\\n= argmax Tr(d X Xd) subject to d d = 1 (2.84)\\n\\ue03e \\ue03e \\ue03e\\nd\\nThis optimization problem may be solvedusing eigendecomposition. Specifically,\\nthe optimal d is given by the eigenvector of X X corresponding to the largest\\n\\ue03e\\neigenvalue.\\nThis derivation is specific to the case of l = 1 and recovers only the first\\nprincipal component. More generally, when we wish to recover a basis of principal\\ncomponents, the matrix D is given by the l eigenvectors corresponding to the\\nlargest eigenvalues. This may be shown using proof by induction. We recommend\\nwriting this proof as an exercise.\\nLinear algebra is one of the fundamental mathematical disciplines that is'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='necessary to understand deep learning. Another key area of mathematics that is\\nubiquitous in machine learning is probability theory, presented next.\\n52\\nChapter 3\\nProbability and Information\\nTheory\\nIn this chapter, we describe probability theory and information theory.\\nProbability theory is a mathematical framework for representing uncertain\\nstatements. It provides a means of quantifying uncertainty and axioms for deriving\\nnew uncertain statements. In artificial intelligence applications, we use probability\\ntheory in two major ways. First, the laws of probability tell us how AI systems\\nshould reason, so we design our algorithms to compute or approximate various\\nexpressions derived using probability theory. Second, we can use probability and\\nstatistics to theoretically analyze the behavior of proposed AI systems.\\nProbability theory is a fundamental tool of many disciplines of science and\\nengineering. We provide this chapter to ensure that readers whose background is'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='primarily in software engineering with limited exposure to probability theory can\\nunderstand the material in this book.\\nWhile probability theory allows us to make uncertain statements and reason in\\nthe presence of uncertainty, information theory allows us to quantify the amount\\nof uncertainty in a probability distribution.\\nIf you are already familiar with probability theory and information theory, you\\nmay wish to skip all of this chapter except for section 3.14, which describes the\\ngraphs we use to describe structured probabilistic models for machine learning. If\\nyou have absolutely no prior experience with these subjects, this chapter should\\nbe sufficient to successfully carry out deep learning research projects, but we do\\nsuggest that you consult an additional resource, such as Jaynes (2003).\\n53\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n3.1 Why Probability?\\nMany branches of computer science deal mostly with entities that are entirely'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='deterministic and certain. A programmer can usually safely assume that a CPU will\\nexecute each machine instruction flawlessly. Errors in hardware do occur, but are\\nrare enough that most software applications do not need to be designed to account\\nfor them. Given that many computer scientists and software engineers work in a\\nrelatively clean and certain environment, it can be surprising that machine learning\\nmakes heavy use of probability theory.\\nThis is because machine learning must always deal with uncertain quantities,\\nand sometimes may also need to deal with stochastic (non-deterministic) quantities.\\nUncertainty and stochasticity can arise from many sources. Researchers have made\\ncompelling arguments for quantifying uncertainty using probability since at least\\nthe 1980s. Many of the arguments presented here are summarized from or inspired\\nby Pearl (1988).\\nNearly all activities require some ability to reason in the presence of uncertainty.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In fact, beyond mathematical statements that are true by definition, it is difficult\\nto think of any proposition that is absolutely true or any event that is absolutely\\nguaranteed to occur.\\nThere are three possible sources of uncertainty:\\n1. Inherent stochasticity in the system being modeled. For example, most\\ninterpretations of quantum mechanics describe the dynamics of subatomic\\nparticles as being probabilistic. We can also create theoretical scenarios that\\nwe postulate to have random dynamics, such as a hypothetical card game\\nwhere we assume that the cards are truly shuffled into a random order.\\n2. Incomplete observability. Even deterministic systems can appear stochastic\\nwhen we cannot observe all of the variables that drive the behavior of the\\nsystem. For example, in the Monty Hall problem, a game show contestant is\\nasked to choose between three doors and wins a prize held behind the chosen\\ndoor. Two doors lead to a goat while a third leads to a car. The outcome'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='given the contestant’s choice is deterministic, but from the contestant’s point\\nof view, the outcome is uncertain.\\n3. Incomplete modeling. When we use a model that must discard some of\\nthe information we have observed, the discarded information results in\\nuncertainty in the model’s predictions. For example, suppose we build a\\nrobot that can exactly observe the location of every object around it. If the\\n54\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nrobot discretizes space when predicting the future location of these objects,\\nthen the discretization makes the robot immediately become uncertain about\\nthe precise position of objects: each object could be anywhere within the\\ndiscrete cell that it was observed to occupy.\\nIn many cases, it is more practical to use a simple but uncertain rule rather\\nthan a complex but certain one, even if the true rule is deterministic and our\\nmodeling system has the fidelity to accommodate a complex rule. For example, the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='simple rule “Most birds fly” is cheap to develop and is broadly useful, while a rule\\nof the form, “Birds fly, except for very young birds that have not yet learned to\\nfly, sick or injured birds that have lost the ability to fly, flightless species of birds\\nincluding the cassowary, ostrich and kiwi...” is expensive to develop, maintain and\\ncommunicate, and after all of this effort is still very brittle and prone to failure.\\nWhile it should be clear that we need a means of representing and reasoning\\nabout uncertainty, it is not immediately obvious that probability theory can provide\\nall of the tools we want for artificial intelligence applications. Probability theory\\nwas originally developed to analyze the frequencies of events. It is easy to see\\nhow probability theory can be used to study events like drawing a certain hand of\\ncards in a game of poker. These kinds of events are often repeatable. When we\\nsay that an outcome has a probability p of occurring, it means that if we repeated'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the experiment (e.g., draw a hand of cards) infinitely many times, then proportion\\np of the repetitions would result in that outcome. This kind of reasoning does not\\nseem immediately applicable to propositions that are not repeatable. If a doctor\\nanalyzes a patient and says that the patient has a 40% chance of having the flu,\\nthis means something very different—we can not make infinitely many replicas of\\nthe patient, nor is there any reason to believe that different replicas of the patient\\nwould present with the same symptoms yet have varying underlying conditions. In\\nthe case of the doctor diagnosing the patient, we use probability to represent a\\ndegree of belief, with 1 indicating absolute certainty that the patient has the flu\\nand 0 indicating absolute certainty that the patient does not have the flu. The\\nformer kind of probability, related directly to the rates at which events occur, is\\nknown as frequentist probability, while the latter, related to qualitative levels'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of certainty, is known as Bayesian probability.\\nIf we list several properties that we expect common sense reasoning about\\nuncertainty to have, then the only way to satisfy those properties is to treat\\nBayesian probabilities as behaving exactly the same as frequentist probabilities.\\nFor example, if we want to compute the probability that a player will win a poker\\ngame given that she has a certain set of cards, we use exactly the same formulas\\nas when we compute the probability that a patient has a disease given that she\\n55\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nhas certain symptoms. For more details about why a small set of common sense\\nassumptions implies that the same axioms must control both kinds of probability,\\nsee Ramsey (1926).\\nProbability can be seen as the extension of logic to deal with uncertainty. Logic\\nprovides a set of formal rules for determining what propositions are implied to\\nbe true or false given the assumption that some other set of propositions is true'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='or false. Probability theory provides a set of formal rules for determining the\\nlikelihood of a proposition being true given the likelihood of other propositions.\\n3.2 Random Variables\\nA random variable is a variable that can take on different values randomly. We\\ntypically denote the random variable itself with a lower case letter in plain typeface,\\nand the values it can take on with lower case script letters. For example, x and x\\n1 2\\nare both possible values that the random variable x can take on. For vector-valued\\nvariables, we would write the random variable as x and one of its values as x. On\\nits own, a random variable is just a description of the states that are possible; it\\nmust be coupled with a probability distribution that specifies how likely each of\\nthese states are.\\nRandom variables may be discrete or continuous. A discrete random variable\\nis one that has a finite or countably infinite number of states. Note that these'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='states are not necessarily the integers; they can also just be named states that\\nare not considered to have any numerical value. A continuous random variable is\\nassociated with a real value.\\n3.3 Probability Distributions\\nA probability distribution is a description of how likely a random variable or\\nset of random variables is to take on each of its possible states. The way we\\ndescribe probability distributions depends on whether the variables are discrete or\\ncontinuous.\\n3.3.1 Discrete Variables and Probability Mass Functions\\nA probability distribution over discrete variables may be described using a proba-\\nbility mass function (PMF). We typically denote probability mass functions with\\na capital P. Often we associate each random variable with a different probability\\n56\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nmass function and the reader must infer which probability mass function to use\\nbased on the identity of the random variable, rather than the name of the function;'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='P(x) is usually not the same as P(y).\\nThe probability mass function maps from a state of a random variable to\\nthe probability of that random variable taking on that state. The probability\\nthat x = x is denoted as P(x), with a probability of 1 indicating that x = x is\\ncertain and a probability of 0 indicating that x = x is impossible. Sometimes\\nto disambiguate which PMF to use, we write the name of the random variable\\nexplicitly: P(x = x). Sometimes we define a variable first, then use notation to\\n∼\\nspecify which distribution it follows later: x P(x).\\n∼\\nProbability mass functions can act on many variables at the same time. Such\\na probability distribution over many variables is known as a joint probability\\ndistribution. P(x = x,y = y) denotes the probability that x = x and y = y\\nsimultaneously. We may also write P(x,y) for brevity.\\nTo be a probability mass function on a random variable x, a function P must\\nsatisfy the following properties:'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The domain of P must be the set of all possible states of x.\\n•\\nx x,0 P(x) 1. An impossible event has probability 0 and no state can\\n• ∀ ∈ ≤ ≤\\nbe less probable than that. Likewise, an event that is guaranteed to happen\\nhas probability 1, and no state can have a greater chance of occurring.\\nP(x) = 1. We refer to this property as being normalized. Without\\n• x x\\nthis∈property, we could obtain probabilities greater than one by computing\\n\\ue050\\nthe probability of one of many events occurring.\\nFor example, consider a single discrete random variable x with k different\\nstates. We can place a uniform distribution on x—that is, make each of its\\nstates equally likely—by setting its probability mass function to\\n1\\nP(x = x ) = (3.1)\\ni\\nk\\nfor all i. We can see that this fits the requirements for a probability mass function.\\nThe value 1 is positive because k is a positive integer. We also see that\\nk\\n1 k\\nP(x = x ) = = = 1, (3.2)\\ni\\nk k\\ni i\\nso the distribution is properly normalized.\\n57\\n\\ue058 \\ue058'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n3.3.2 Continuous Variables and Probability Density Functions\\nWhen working with continuous random variables, we describe probability distri-\\nbutions using a probability density function (PDF) rather than a probability\\nmass function. To be a probability density function, a function p must satisfy the\\nfollowing properties:\\nThe domain of p must be the set of all possible states of x.\\n•\\nx x,p(x) 0. Note that we do not require p(x) 1.\\n• ∀ ∈ ≥ ≤\\np(x)dx = 1.\\n•\\nA\\ue052probability density function p(x) does not give the probability of a specific\\nstate directly, instead the probability of landing inside an infinitesimal region with\\nvolume δx is given by p(x)δx.\\nWe can integrate the density function to find the actual probability mass of a\\nS\\nset of points. Specifically, the probability that x lies in some set is given by the\\nintegral of p(x) over that set. In the univariate example, the probability that x\\nlies in the interval [a,b] is given by p(x)dx.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='[a,b]\\nFor an example of a probability density function corresponding to a specific\\n\\ue052\\nprobability density over a continuous random variable, consider a uniform distribu-\\ntion on an interval of the real numbers. We can do this with a function u(x;a,b),\\nwhere a and b are the endpoints of the interval, with b > a. The “;” notation means\\n“parametrized by”; we consider x to be the argument of the function, while a and\\nb are parameters that define the function. To ensure that there is no probability\\nmass outside the interval, we say u(x;a,b) = 0 for all x [a,b]. Within [a,b],\\n\\ue036∈\\nu(x;a,b) = 1 . We can see that this is nonnegative everywhere. Additionally, it\\nb a\\nintegrates to−1. We often denote that x follows the uniform distribution on [a,b]\\nby writing x U(a,b).\\n∼\\n3.4 Marginal Probability\\nSometimes we know the probability distribution over a set of variables and we want\\nto know the probability distribution over just a subset of them. The probability'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='distribution over the subset is known as the marginal probability distribution.\\nFor example, suppose we have discrete random variables x and y, and we know\\nP(x,y). We can find P(x) with the sum rule:\\nx x,P(x = x) = P(x = x,y = y). (3.3)\\n∀ ∈\\ny\\n58\\n\\ue058\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nThe name “marginal probability” comes from the process of computing marginal\\nprobabilities on paper. When the values of P(x,y) are written in a grid with\\ndifferent values of x in rows and different values of y in columns, it is natural to\\nsum across a row of the grid, then write P(x) in the margin of the paper just to\\nthe right of the row.\\nFor continuous variables, we need to use integration instead of summation:\\np(x) = p(x,y)dy. (3.4)\\n\\ue05a\\n3.5 Conditional Probability\\nIn many cases, we are interested in the probability of some event, given that some\\nother event has happened. This is called a conditional probability. We denote\\nthe conditional probability that y = y given x = x as P(y = y x = x). This'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='|\\nconditional probability can be computed with the formula\\nP(y = y,x = x)\\nP(y = y x = x) = . (3.5)\\n| P(x = x)\\nThe conditional probability is only defined when P(x= x) > 0. We cannot compute\\nthe conditional probability conditioned on an event that never happens.\\nIt is important not to confuse conditional probability with computing what\\nwould happen if some action were undertaken. The conditional probability that\\na person is from Germany given that they speak German is quite high, but if\\na randomly selected person is taught to speak German, their country of origin\\ndoes not change. Computing the consequences of an action is called making an\\nintervention query. Intervention queries are the domain of causal modeling,\\nwhich we do not explore in this book.\\n3.6 The Chain Rule of Conditional Probabilities\\nAny joint probability distribution over many random variables may be decomposed\\ninto conditional distributions over only one variable:'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='P(x(1),...,x(n)) = P(x(1))Πn P(x(i) x(1),...,x(i 1)). (3.6)\\ni=2 −\\n|\\nThis observation is known as the chain rule or product rule of probability.\\nIt follows immediately from the definition of conditional probability in equation 3.5.\\n59\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nFor example, applying the definition twice, we get\\nP(a,b,c) = P(a b,c)P(b,c)\\n|\\nP(b,c) = P(b c)P(c)\\n|\\nP(a,b,c) = P(a b,c)P(b c)P(c).\\n| |\\n3.7 Independence and Conditional Independence\\nTwo random variables x and y are independent if their probability distribution\\ncan be expressed as a product of two factors, one involving only x and one involving\\nonly y:\\nx x,y y, p(x = x,y = y) = p(x = x)p(y = y). (3.7)\\n∀ ∈ ∈\\nTworandomvariablesx and yareconditionally independent givena random\\nvariable z if the conditional probability distribution over x and y factorizes in this\\nway for every value of z:\\nx x,y y,z z, p(x = x,y = y z = z) = p(x = x z = z)p(y = y z = z).\\n∀ ∈ ∈ ∈ | | |\\n(3.8)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='We can denote independence and conditional independence with compact\\nnotation: x y means that xand y are independent, while x y z means that x\\n⊥ ⊥ |\\nand y are conditionally independent given z.\\n3.8 Expectation, Variance and Covariance\\nThe expectation or expected value of some function f(x) with respect to a\\nprobability distribution P(x) is the average or mean value that f takes on when x\\nis drawn from P. For discrete variables this can be computed with a summation:\\nE\\n[f(x)] = P(x)f(x), (3.9)\\nx P\\n∼\\nx\\n\\ue058\\nwhile for continuous variables, it is computed with an integral:\\nE\\n[f(x)] = p(x)f(x)dx. (3.10)\\nx p\\n∼\\n60\\n\\ue05a\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nWhen the identity of the distribution is clear from the context, we may simply\\nE\\nwrite the name of the random variable that the expectation is over, as in [f(x)].\\nx\\nIf it is clear which random variable the expectation is over, we may omit the\\nE E\\nsubscript entirely, as in [f(x)]. By default, we can assume that [ ] averages over\\n·'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the values of all the random variables inside the brackets. Likewise, when there is\\nno ambiguity, we may omit the square brackets.\\nExpectations are linear, for example,\\nE E E\\n[αf(x)+ βg(x)] = α [f(x)]+ β [g(x)], (3.11)\\nx x x\\nwhen α and β are not dependent on x.\\nThe variancegives a measure of how much the values of a function of a random\\nvariable x vary as we sample different values of x from its probability distribution:\\nE E 2\\nVar(f(x)) = (f(x) [f(x)]) . (3.12)\\n−\\n\\ue068 \\ue069\\nWhen the variance is low, the values of f(x) cluster near their expected value. The\\nsquare root of the variance is known as the standard deviation.\\nThe covariance gives some sense of how much two values are linearly related\\nto each other, as well as the scale of these variables:\\nE E E\\nCov(f(x),g(y)) = [(f(x) [f(x)])(g(y) [g(y)])]. (3.13)\\n− −\\nHigh absolute values of the covariance mean that the values change very much\\nand are both far from their respective means at the same time. If the sign of the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='covariance is positive, then both variables tend to take on relatively high values\\nsimultaneously. If the sign of the covariance is negative, then one variable tends to\\ntake on a relatively high value at the times that the other takes on a relatively\\nlow value and vice versa. Other measures such as correlation normalize the\\ncontribution of each variable in order to measure only how much the variables are\\nrelated, rather than also being affected by the scale of the separate variables.\\nThe notions of covariance and dependence are related, but are in fact distinct\\nconcepts. They are related because two variables that are independent have zero\\ncovariance, and two variables that have non-zero covariance are dependent. How-\\never, independence is a distinct property from covariance. For two variables to have\\nzero covariance, there must be no linear dependence between them. Independence\\nis a stronger requirement than zero covariance, because independence also excludes'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='nonlinear relationships. It is possible for two variables to be dependent but have\\nzero covariance. For example, suppose we first sample a real number x from a\\nuniform distribution over the interval [ 1, 1]. We next sample a random variable\\n−\\n61\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\ns. With probability 1, we choose the value of s to be 1. Otherwise, we choose\\n2\\nthe value of s to be 1. We can then generate a random variable y by assigning\\n−\\ny = sx. Clearly, x and y are not independent, because x completely determines\\nthe magnitude of y. However, Cov(x,y) = 0.\\nThe covariance matrix of a random vector x\\nRn\\nis an n n matrix, such\\n∈ ×\\nthat\\nCov(x) = Cov(x ,x ). (3.14)\\ni,j i j\\nThe diagonal elements of the covariance give the variance:\\nCov(x ,x ) = Var(x ). (3.15)\\ni i i\\n3.9 Common Probability Distributions\\nSeveral simple probability distributions are useful in many contexts in machine\\nlearning.\\n3.9.1 Bernoulli Distribution'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The Bernoulli distribution is a distribution over a single binary random variable.\\nIt is controlled by a single parameter φ [0,1], which gives the probability of the\\n∈\\nrandom variable being equal to 1. It has the following properties:\\nP(x = 1) = φ (3.16)\\nP(x = 0) = 1 φ (3.17)\\n−\\nP(x = x) = φx(1 φ)1 x (3.18)\\n−\\n−\\nE\\n[x] = φ (3.19)\\nx\\nVar (x) = φ(1 φ) (3.20)\\nx\\n−\\n3.9.2 Multinoulli Distribution\\nThe multinoulli orcategorical distribution is a distribution over a single discrete\\nvariable with k different states, where k is finite.1 The multinoulli distribution is\\n1 “Multinoulli” is a term that was recently coined by Gustavo Lacerdo and popularized by\\nMurphy (2012). The multinoulli distribution is a special case of the multinomial distribution.\\nA multinomial distribution is the distribution over vectors in 0,...,n k representing how many\\n{ }\\ntimes each of the k categories is visited when nsamples are drawn from a multinoulli distribution.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Many texts use the term “multinomial” to refer to multinoulli distributions without clarifying\\nthat they refer only to the n= 1 case.\\n62\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nparametrized by a vector p [0,1]k 1, where p gives the probability of the i-th\\n− i\\n∈\\nstate. The final, k-th state’s probability is given by 1 1 p. Note that we must\\n\\ue03e\\n−\\nconstrain 1 p 1. Multinoulli distributions are often used to refer to distributions\\n\\ue03e\\n≤\\nover categories of objects, so we do not usually assume that state 1 has numerical\\nvalue 1, etc. For this reason, we do not usually need to compute the expectation\\nor variance of multinoulli-distributed random variables.\\nThe Bernoulli and multinoulli distributions are sufficient to describe any distri-\\nbution over their domain. They are able to describe any distribution over their\\ndomain not so much because they are particularly powerful but rather because\\ntheir domain is simple; they model discrete variables for which it is feasible to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='enumerate all of the states. When dealing with continuous variables, there are\\nuncountably many states, so any distribution described by a small number of\\nparameters must impose strict limits on the distribution.\\n3.9.3 Gaussian Distribution\\nThe most commonly used distribution over real numbers is the normal distribu-\\ntion, also known as the Gaussian distribution:\\n1 1\\n(x;µ,σ2) = exp (x µ)2 . (3.21)\\nN 2πσ2 −2σ2 −\\n\\ue072 \\ue012 \\ue013\\nSee figure 3.1 for a plot of the density function.\\nR\\nThe two parameters µ and σ (0, ) control the normal distribution.\\n∈ ∈ ∞\\nThe parameter µ gives the coordinate of the central peak. This is also the mean of\\nE\\nthe distribution: [x] =µ. The standard deviation of the distribution is given by\\nσ, and the variance by σ2.\\nWhen we evaluate the PDF, we need to square and invert σ. When we need to\\nfrequently evaluate the PDF with different parameter values, a more efficient way\\nof parametrizing the distribution is to use a parameter β (0, ) to control the\\n∈ ∞'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='precision or inverse variance of the distribution:\\nβ 1\\n(x;µ,β 1) = exp β(x µ)2 . (3.22)\\n−\\nN 2π −2 −\\n\\ue072 \\ue012 \\ue013\\nNormal distributions are a sensible choice for many applications. In the absence\\nof prior knowledge about what form a distribution over the real numbers should\\ntake, the normal distribution is a good default choice for two major reasons.\\n63\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n0.40\\n0.35\\n0.30\\n0.25\\n0.20\\n0.15\\n0.10\\n0.05\\n0.00\\n2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0\\n− − − −\\nx\\n)x(p\\nMaximum at x = µ\\nInflection points at\\nx = µ σ\\n±\\nFigure 3.1: The normal distribution: The normal distribution (x;µ,σ2) exhibits\\nN\\na classic “bell curve” shape, with the x coordinate of its central peak given by µ, and\\nthe width of its peak controlled by σ. In this example, we depict the standard normal\\ndistribution, with µ = 0 and σ = 1.\\nFirst, many distributions we wish to model are truly close to being normal\\ndistributions. The central limit theorem shows that the sum of many indepen-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='dent random variables is approximately normally distributed. This means that\\nin practice, many complicated systems can be modeled successfully as normally\\ndistributed noise, even if the system can be decomposed into parts with more\\nstructured behavior.\\nSecond, out of all possible probability distributions with the same variance,\\nthe normal distribution encodes the maximum amount of uncertainty over the\\nreal numbers. We can thus think of the normal distribution as being the one\\nthat inserts the least amount of prior knowledge into a model. Fully developing\\nand justifying this idea requires more mathematical tools, and is postponed to\\nsection 19.4.2.\\nThe normal distribution generalizes to\\nRn,\\nin which case it is known as the\\nmultivariate normal distribution. It may be parametrized with a positive\\ndefinite symmetric matrix Σ:\\n1 1\\n(x;µ,Σ) = exp (x µ) Σ 1(x µ) . (3.23)\\n\\ue03e −\\nN (2π)ndet(Σ) −2 − −\\n64\\n\\ue073 \\ue012 \\ue013\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The parameter µ still gives the mean of the distribution, though now it is\\nvector-valued. The parameter Σ gives the covariance matrix of the distribution.\\nAs in the univariate case, when we wish to evaluate the PDF several times for\\nmany different values of the parameters, the covariance is not a computationally\\nefficient way to parametrize the distribution, since we need to invert Σ to evaluate\\nthe PDF. We can instead use a precision matrix β:\\ndet(β) 1\\n(x;µ,β 1) = exp (x µ) β(x µ) . (3.24)\\n− \\ue03e\\nN \\ue073 (2π)n −2 − −\\n\\ue012 \\ue013\\nWe often fix the covariance matrix to be a diagonal matrix. An even simpler\\nversion is the isotropic Gaussian distribution, whose covariance matrix is a scalar\\ntimes the identity matrix.\\n3.9.4 Exponential and Laplace Distributions\\nIn the context of deep learning, we often want to have a probability distribution\\nwith a sharp point at x = 0. To accomplish this, we can use the exponential\\ndistribution:\\np(x;λ) = λ1 exp( λx). (3.25)\\nx 0\\n≥ −'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The exponential distribution uses the indicator function 1 to assign probability\\nx 0\\n≥\\nzero to all negative values of x.\\nA closely related probability distribution that allows us to place a sharp peak\\nof probability mass at an arbitrary point µ is the Laplace distribution\\n1 x µ\\nLaplace(x;µ,γ) = exp | − | . (3.26)\\n2γ − γ\\n\\ue012 \\ue013\\n3.9.5 The Dirac Distribution and Empirical Distribution\\nIn some cases, we wish to specify that all of the mass in a probability distribution\\nclusters around a single point. This can be accomplished by defining a PDF using\\nthe Dirac delta function, δ(x):\\np(x) = δ(x µ). (3.27)\\n−\\nThe Dirac delta function is defined such that it is zero-valued everywhere except\\n0, yet integrates to 1. The Dirac delta function is not an ordinary function that\\nassociates each value x with a real-valued output, instead it is a different kind of\\n65\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nmathematical object called a generalized function that is defined in terms of its'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='properties when integrated. We can think of the Dirac delta function as being the\\nlimit point of a series of functions that put less and less mass on all points other\\nthan zero.\\nBy defining p(x) to be δ shifted by µ we obtain an infinitely narrow and\\n−\\ninfinitely high peak of probability mass where x = µ.\\nA common use of the Dirac delta distribution is as a componentof an empirical\\ndistribution,\\nm\\n1\\npˆ(x) = δ(x x(i)) (3.28)\\nm −\\ni=1\\n\\ue058\\nwhich puts probability mass 1 on each of the m points x(1),...,x(m) forming a\\nm\\ngiven dataset or collection of samples. The Dirac delta distribution is only necessary\\nto define the empirical distribution over continuous variables. For discrete variables,\\nthe situation is simpler: an empirical distribution can be conceptualized as a\\nmultinoulli distribution, with a probability associated to each possible input value\\nthat is simply equal to the empirical frequency of that value in the training set.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='We can view the empirical distribution formed from a dataset of training\\nexamples as specifying the distribution that we sample from when we train a model\\non this dataset. Another important perspective on the empirical distribution is\\nthat it is the probability density that maximizes the likelihood of the training data\\n(see section 5.5).\\n3.9.6 Mixtures of Distributions\\nIt is also common to define probability distributions by combining other simpler\\nprobability distributions. One common way of combining distributions is to\\nconstruct a mixture distribution. A mixture distribution is made up of several\\ncomponent distributions. On each trial, the choice of which component distribution\\ngenerates the sample is determined by sampling a component identity from a\\nmultinoulli distribution:\\nP(x) = P(c = i)P(x c = i) (3.29)\\n|\\ni\\n\\ue058\\nwhere P(c) is the multinoulli distribution over component identities.\\nWe have already seen one example of a mixture distribution: the empirical'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='distribution over real-valued variables is a mixture distribution with one Dirac\\ncomponent for each training example.\\n66\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nThemixture model is onesimple strategy for combiningprobabilitydistributions\\nto create a richer distribution. In chapter 16, we explore the art of building complex\\nprobability distributions from simple ones in more detail.\\nThe mixture model allows us to briefly glimpse a concept that will be of\\nparamount importance later—the latent variable. A latent variable is a random\\nvariable that we cannot observe directly. The component identity variable c of the\\nmixture model provides an example. Latent variables may be related to x through\\nthe joint distribution, in this case, P(x,c) = P(x c)P(c). The distribution P(c)\\n|\\nover the latent variable and the distribution P(x c) relating the latent variables\\n|\\nto the visible variables determines the shape of the distribution P(x) even though'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='it is possible to describe P(x) without reference to the latent variable. Latent\\nvariables are discussed further in section 16.5.\\nA very powerful and common type of mixture model is the Gaussian mixture\\nmodel, in which the components p(x c = i) are Gaussians. Each component has\\n|\\na separately parametrized mean µ(i) and covariance Σ(i). Some mixtures can have\\nmore constraints. For example, the covariances could be shared across components\\nvia the constraint Σ(i) = Σ, i. As with a single Gaussian distribution, the mixture\\n∀\\nof Gaussians might constrain the covariance matrix for each component to be\\ndiagonal or isotropic.\\nIn addition to the means and covariances, the parameters of a Gaussian mixture\\nspecify the prior probability α =P(c = i) given to each component i. The word\\ni\\n“prior” indicates that it expresses the model’s beliefs about c before it has observed\\nx. By comparison, P(c x) is a posterior probability, because it is computed\\n|'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='after observation of x. A Gaussian mixture model is a universal approximator\\nof densities, in the sense that any smooth density can be approximated with any\\nspecific, non-zero amount of error by a Gaussian mixture model with enough\\ncomponents.\\nFigure 3.2 shows samples from a Gaussian mixture model.\\n3.10 Useful Properties of Common Functions\\nCertain functions arise often while working with probability distributions, especially\\nthe probability distributions used in deep learning models.\\nOne of these functions is the logistic sigmoid:\\n1\\nσ(x) = . (3.30)\\n1+exp( x)\\n−\\nThe logistic sigmoid is commonly used to produce the φ parameter of a Bernoulli\\n67\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nx\\n1\\n2x\\nFigure 3.2: Samples from a Gaussian mixture model. In this example, there are three\\ncomponents. From left to right, the first component has an isotropic covariance matrix,\\nmeaning it has the same amount of variance in each direction. The second has a diagonal'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='covariance matrix, meaning it can control the variance separately along each axis-aligned\\ndirection. This example has more variance along the x axis than along the x axis. The\\n2 1\\nthird component has a full-rank covariance matrix, allowing it to control the variance\\nseparately along an arbitrary basis of directions.\\ndistribution because its range is (0,1), which lies within the valid range of values\\nfor the φ parameter. See figure 3.3 for a graph of the sigmoid function. The\\nsigmoid function saturates when its argument is very positive or very negative,\\nmeaning that the function becomes very flat and insensitive to small changes in its\\ninput.\\nAnother commonly encountered function is the softplus function (Dugas et al.,\\n2001):\\nζ(x) = log(1+exp(x)). (3.31)\\nThe softplus function can be useful for producing the β or σ parameter of a normal\\ndistribution because its range is (0, ). It also arises commonly when manipulating\\n∞'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='expressions involving sigmoids. The name of the softplus function comes from the\\nfact that it is a smoothed or “softened” version of\\nx+ = max(0,x). (3.32)\\nSee figure 3.4 for a graph of the softplus function.\\nThe following properties are all useful enough that you may wish to memorize\\nthem:\\n68\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n10 5 0 5 10\\n− −\\nx\\n)x(σ\\nFigure 3.3: The logistic sigmoid function.\\n10\\n8\\n6\\n4\\n2\\n0\\n10 5 0 5 10\\n− −\\nx\\n)x(ζ\\nFigure 3.4: The softplus function.\\n69\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nexp(x)\\nσ(x) = (3.33)\\nexp(x)+exp(0)\\nd\\nσ(x) = σ(x)(1 σ(x)) (3.34)\\ndx −\\n1 σ(x) = σ( x) (3.35)\\n− −\\nlogσ(x) = ζ( x) (3.36)\\n− −\\nd\\nζ(x) = σ(x) (3.37)\\ndx\\nx\\nx (0,1), σ 1(x) = log (3.38)\\n−\\n∀ ∈ 1 x\\n\\ue012 − \\ue013\\nx > 0, ζ 1(x) = log(exp(x) 1) (3.39)\\n−\\n∀ −\\nx\\nζ(x) = σ(y)dy (3.40)\\n\\ue05a\\n−∞\\nζ(x) ζ( x) = x (3.41)\\n− −\\nThe function σ 1(x) is called the logit in statistics, but this term is more rarely\\n−\\nused in machine learning.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Equation 3.41 provides extra justification for the name “softplus.” The softplus\\nfunction is intended as a smoothed version of the positive part function, x+ =\\nmax 0,x . The positive part function is the counterpart of the negative part\\n{ }\\nfunction, x = max 0, x . To obtain a smooth function that is analogous to the\\n−\\n{ − }\\nnegative part, one can use ζ( x). Just as x can be recovered from its positive part\\n−\\nand negative part via the identity x+ x = x, it is also possible to recover x\\n−\\n−\\nusing the same relationship between ζ(x) and ζ( x), as shown in equation 3.41.\\n−\\n3.11 Bayes’ Rule\\nWe often find ourselves in a situation where we know P(y x) and need to know\\n|\\nP(x y). Fortunately, if we also know P(x), we can compute the desired quantity\\n|\\nusing Bayes’ rule:\\nP(x)P(y x)\\nP(x y) = | . (3.42)\\n| P(y)\\nNote that while P(y) appears in the formula, it is usually feasible to compute\\nP(y) = P(y x)P(x), so we do not need to begin with knowledge of P(y).\\nx |\\n70\\n\\ue050'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nBayes’ rule is straightforward to derive from the definition of conditional\\nprobability, but it is useful to know the name of this formula since many texts\\nrefer to it by name. It is named after the Reverend Thomas Bayes, who first\\ndiscovered a special case of the formula. The general version presented here was\\nindependently discovered by Pierre-Simon Laplace.\\n3.12 Technical Details of Continuous Variables\\nA proper formal understanding of continuous random variables and probability\\ndensity functions requires developing probability theory in terms of a branch of\\nmathematics known as measure theory. Measure theory is beyond the scope of\\nthis textbook, but we can briefly sketch some of the issues that measure theory is\\nemployed to resolve.\\nIn section 3.3.2, we saw that the probability of a continuous vector-valued x\\nS S\\nlying in some set is given by the integral of p(x) over the set . Some choices\\nS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of set can produce paradoxes. For example, it is possible to construct two sets\\nS S S S S S\\nand such that p(x ) + p(x ) > 1 but = . These sets\\n1 2 1 2 1 2\\n∈ ∈ ∩ ∅\\nare generally constructed making very heavy use of the infinite precision of real\\nnumbers, for example by making fractal-shaped sets or sets that are defined by\\ntransforming the set of rational numbers.2 One of the key contributions of measure\\ntheory is to provide a characterization of the set of sets that we can compute the\\nprobability of without encountering paradoxes. In this book, we only integrate\\nover sets with relatively simple descriptions, so this aspect of measure theory never\\nbecomes a relevant concern.\\nFor our purposes, measure theory is more useful for describing theorems that\\napply to most points in\\nRn\\nbut do not apply to some corner cases. Measure theory\\nprovides a rigorous way of describing that a set of points is negligibly small. Such'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a set is said to have measure zero. We do not formally define this concept in this\\ntextbook. For our purposes, it is sufficient to understand the intuition that a set\\nof measure zero occupies no volume in the space we are measuring. For example,\\nwithin\\nR2,\\na line has measure zero, while a filled polygon has positive measure.\\nLikewise, an individual point has measure zero. Any union of countably many sets\\nthat each have measure zero also has measure zero (so the set of all the rational\\nnumbers has measure zero, for instance).\\nAnother useful term from measure theory is almost everywhere. A property\\nthat holds almost everywhere holds throughout all of space except for on a set of\\n2The Banach-Tarski theorem provides a fun example of such sets.\\n71\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nmeasure zero. Because the exceptions occupy a negligible amount of space, they\\ncan be safely ignored for many applications. Some important results in probability'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='theory hold for all discrete values but only hold “almost everywhere” for continuous\\nvalues.\\nAnother technical detail of continuous variables relates to handling continuous\\nrandom variables that are deterministic functions of one another. Suppose we have\\ntwo random variables, xand y, such that y = g(x), where g is an invertible, con-\\ntinuous, differentiable transformation. One might expect that p (y) = p (g 1(y)).\\ny x −\\nThis is actually not the case.\\nAs a simple example, suppose we have scalar random variables x and y. Suppose\\ny = x and x U(0,1). If we use the rule p (y) = p (2y) then p will be 0\\n2 ∼ y x y\\neverywhere except the interval [0,1], and it will be 1 on this interval. This means\\n2\\n1\\np (y)dy = , (3.43)\\ny\\n2\\n\\ue05a\\nwhich violates the definition of a probabilitydistribution. This is a common mistake.\\nThe problem with this approach is that it fails to account for the distortion of\\nspace introduced by the function g. Recall that the probability of x lying in an'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='infinitesimally small region with volume δx is given by p(x)δx. Since g can expand\\nor contract space, the infinitesimal volume surrounding x in x space may have\\ndifferent volume in y space.\\nTo see how to correct the problem, we return to the scalar case. We need to\\npreserve the property\\np (g(x))dy = p (x)dx . (3.44)\\ny x\\n| | | |\\nSolving from this, we obtain\\n∂x\\np (y) = p (g 1(y)) (3.45)\\ny x −\\n∂y\\n\\ue00c \\ue00c\\n\\ue00c \\ue00c\\n\\ue00c \\ue00c\\nor equivalently\\n\\ue00c \\ue00c\\n∂g(x)\\np (x) = p (g(x)) . (3.46)\\nx y\\n∂x\\n\\ue00c \\ue00c\\n\\ue00c \\ue00c\\nIn higher dimensions, the derivative generalizes\\ue00c to the\\ue00cdeterminantof the Jacobian\\n\\ue00c \\ue00c\\nmatrix—the matrix with J = ∂x i. Thus, for real-valued vectors x and y,\\ni,j ∂y\\nj\\n∂g(x)\\np (x) = p (g(x)) det . (3.47)\\nx y\\n∂x\\n72\\n\\ue00c \\ue012 \\ue013\\ue00c\\n\\ue00c \\ue00c\\n\\ue00c \\ue00c\\n\\ue00c \\ue00c\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n3.13 Information Theory\\nInformation theory is a branch of applied mathematics that revolves around\\nquantifying how much information is present in a signal. It was originally invented'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to study sending messages from discrete alphabets over a noisy channel, such as\\ncommunication via radio transmission. In this context, information theory tells how\\nto design optimal codes and calculate the expected length of messages sampled from\\nspecific probability distributions using various encoding schemes. In the context of\\nmachine learning, we can also apply information theory to continuous variables\\nwhere some of these message length interpretations do not apply. This field is\\nfundamental to many areas of electrical engineering and computer science. In this\\ntextbook, we mostly use a few key ideas from information theory to characterize\\nprobability distributions or quantify similarity between probability distributions.\\nFor more detail on information theory, see Cover and Thomas (2006) or MacKay\\n(2003).\\nThe basic intuition behind information theory is that learning that an unlikely\\nevent has occurred is more informative than learning that a likely event has'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='occurred. A message saying “the sun rose this morning” is so uninformative as\\nto be unnecessary to send, but a message saying “there was a solar eclipse this\\nmorning” is very informative.\\nWe would like to quantify information in a way that formalizes this intuition.\\nSpecifically,\\nLikely events should have low information content, and in the extreme case,\\n•\\nevents that are guaranteed to happen should have no information content\\nwhatsoever.\\nLess likely events should have higher information content.\\n•\\nIndependent events should have additive information. For example, finding\\n•\\nout that a tossed coin has come up as heads twice should convey twice as\\nmuch information as finding out that a tossed coin has come up as heads\\nonce.\\nIn order to satisfy all three of these properties, we define the self-information\\nof an event x = x to be\\nI(x) = logP(x). (3.48)\\n−\\nIn this book, we always use log to mean the natural logarithm, with base e. Our'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='definition of I(x) is therefore written in units of nats. One nat is the amount of\\n73\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\ninformation gained by observing an event of probability 1. Other texts use base-2\\ne\\nlogarithms and units called bits or shannons; information measured in bits is\\njust a rescaling of information measured in nats.\\nWhen x is continuous, we use the same definition of information by analogy,\\nbut some of the properties from the discrete case are lost. For example, an event\\nwith unit density still has zero information, despite not being an event that is\\nguaranteed to occur.\\nSelf-information deals only with a single outcome. We can quantify the amount\\nof uncertainty in an entire probability distribution using the Shannon entropy:\\nE E\\nH(x) = [I(x)] = [logP(x)]. (3.49)\\nx P x P\\n∼ − ∼\\nalso denoted H(P). In other words, the Shannon entropy of a distribution is the\\nexpected amount of information in an event drawn from that distribution. It gives'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a lower bound on the number of bits (if the logarithm is base 2, otherwise the units\\nare different) needed on average to encode symbols drawn from a distribution P.\\nDistributions that are nearly deterministic (where the outcome is nearly certain)\\nhave low entropy; distributions that are closer to uniform have high entropy. See\\nfigure 3.5 for a demonstration. When x is continuous, the Shannon entropy is\\nknown as the differential entropy.\\nIf we have two separate probability distributions P (x) andQ(x) over the same\\nrandom variable x, we can measure how different these two distributions are using\\nthe Kullback-Leibler (KL) divergence:\\nP(x)\\nD (P Q) = E log = E [logP(x) logQ(x)]. (3.50)\\nKL x P x P\\n\\ue06b ∼ Q(x) ∼ −\\n\\ue014 \\ue015\\nIn the case of discrete variables, it is the extra amount of information (measured\\nin bits if we use the base 2 logarithm, but in machine learning we usually use nats\\nand the natural logarithm) needed to send a message containing symbols drawn'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='from probability distribution P, when we use a code that was designed to minimize\\nthe length of messages drawn from probability distribution Q.\\nThe KL divergence has many useful properties, most notably that it is non-\\nnegative. The KL divergence is 0 if and only if P and Qare the same distribution in\\nthe case of discrete variables, or equal “almost everywhere” in the case of continuous\\nvariables. Because the KL divergence is non-negative and measures the difference\\nbetween two distributions, it is often conceptualized as measuring some sort of\\ndistance between these distributions. However, it is not a true distance measure\\nbecause it is not symmetric: D (P Q) = D (Q P) for some P and Q. This\\nKL KL\\n\\ue06b \\ue036 \\ue06b\\n74\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n0.7\\n0.6\\n0.5\\n0.4\\n0.3\\n0.2\\n0.1\\n0.0\\n0.0 0.2 0.4 0.6 0.8 1.0\\np\\nstan\\nni\\nyportne\\nnonnahS\\nFigure 3.5: This plot shows how distributions that are closer to deterministic have low'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Shannon entropy while distributions that are close to uniform have high Shannon entropy.\\nOn the horizontal axis, we plot p, the probability of a binary random variable being equal\\nto 1. The entropy is given by (p 1)log(1 p) plogp. When pis near 0, the distribution\\n− − −\\nis nearly deterministic, because the random variable is nearly always 0. When p is near 1,\\nthe distribution is nearly deterministic, because the random variable is nearly always 1.\\nWhen p = 0.5, the entropy is maximal, because the distribution is uniform over the two\\noutcomes.\\nasymmetry means that there are important consequences to the choice of whether\\nto use D (P Q) or D (Q P). See figure 3.6 for more detail.\\nKL KL\\n\\ue06b \\ue06b\\nA quantity that is closely related to the KL divergence is the cross-entropy\\nH(P,Q) = H(P)+D (P Q), which is similar to the KL divergence but lacking\\nKL\\n\\ue06b\\nthe term on the left:\\nE\\nH(P,Q) = logQ(x). (3.51)\\nx P\\n− ∼\\nMinimizing the cross-entropy with respect to Q is equivalent to minimizing the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='KL divergence, because Q does not participate in the omitted term.\\nWhen computing many of these quantities, it is common to encounter expres-\\nsions of the form 0log0. By convention, in the context of information theory, we\\ntreat these expressions as lim xlogx = 0.\\nx 0\\n→\\n3.14 Structured Probabilistic Models\\nMachine learning algorithms often involve probability distributions over a very\\nlarge number of random variables. Often, these probability distributions involve\\ndirect interactions between relatively few variables. Using a single function to\\n75\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\nx\\nytisneD\\nytilibaborP\\nq = argmin D (p q)\\n∗ q KL \\ue06b\\np(x)\\nq (x)\\n∗\\nx\\nytisneD\\nytilibaborP\\nq = argmin D (q p)\\n∗ q KL \\ue06b\\np(x)\\nq (x)\\n∗\\nFigure 3.6: The KL divergence is asymmetric. Suppose we have a distribution p(x) and\\nwish to approximate it with another distribution q(x). We have the choice of minimizing\\neither D (p q) or D (q p). We illustrate the effect of this choice using a mixture of\\nKL KL\\n\\ue06b \\ue06b'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='two Gaussians for p, and a single Gaussian for q. The choice of which direction of the\\nKL divergence to use is problem-dependent. Some applications require an approximation\\nthat usually places high probability anywhere that the true distribution places high\\nprobability, while other applications require an approximation that rarely places high\\nprobability anywhere that the true distribution places low probability. The choice of the\\ndirection of the KL divergence reflects which of these considerations takes priority for each\\napplication. (Left)The effect of minimizing D (p q). In this case, we select a qthat has\\nKL\\n\\ue06b\\nhigh probability where phas high probability. When p has multiple modes, q chooses to\\nblur the modes together, in order to put high probability mass on all of them. (Right)The\\neffect of minimizing D (q p). In this case, we select a q that has low probability where\\nKL\\n\\ue06b\\np has low probability. When p has multiple modes that are sufficiently widely separated,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='as in this figure, the KL divergence is minimized by choosing a single mode, in order to\\navoid putting probability mass in the low-probability areas between modes of p. Here, we\\nillustrate the outcome when q is chosen to emphasize the left mode. We could also have\\nachieved an equal value of the KL divergence by choosing the right mode. If the modes\\nare not separated by a sufficiently strong low probability region, then this direction of the\\nKL divergence can still choose to blur the modes.\\n76\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\ndescribe the entire joint probability distribution can be very inefficient (both\\ncomputationally and statistically).\\nInstead of using a single function to represent a probability distribution, we\\ncan split a probability distribution into many factors that we multiply together.\\nFor example, suppose we have three random variables: a, b and c. Suppose that\\na influences the value of b and b influences the value of c, but that a and c are'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='independent given b. We can represent the probability distribution over all three\\nvariables as a product of probability distributions over two variables:\\np(a,b,c) = p(a)p(b a)p(c b). (3.52)\\n| |\\nThese factorizations can greatly reduce the number of parameters needed\\nto describe the distribution. Each factor uses a number of parameters that is\\nexponential in the number of variables in the factor. This means that we can greatly\\nreduce the cost of representing a distribution if we are able to find a factorization\\ninto distributions over fewer variables.\\nWe can describe these kinds of factorizations using graphs. Hereweuse the word\\n“graph” in the sense of graph theory: a set of vertices that may be connected to each\\nother with edges. When we represent the factorization of a probability distribution\\nwith a graph, we call it a structured probabilistic model or graphical model.\\nThere are two main kinds of structured probabilistic models: directed and'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='undirected. Both kinds of graphical models use a graph in which each node\\nG\\nin the graph corresponds to a random variable, and an edge connecting two\\nrandom variables means that the probability distribution is able to represent direct\\ninteractions between those two random variables.\\nDirected models use graphs with directed edges, and they represent fac-\\ntorizations into conditional probability distributions, as in the example above.\\nSpecifically, a directed model contains one factor for every random variable x in\\ni\\nthe distribution, and that factor consists of the conditional distribution over x\\ni\\ngiven the parents of x , denoted Pa (x ):\\ni i\\nG\\np(x) = p(x Pa (x )). (3.53)\\ni i\\n| G\\ni\\n\\ue059\\nSee figure 3.7 for an example of a directed graph and the factorization of probability\\ndistributions it represents.\\nUndirected models use graphs with undirected edges, and they represent\\nfactorizations into a set of functions; unlike in the directed case, these functions\\n77'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 3. PROBABILITY AND INFORMATION THEORY\\naa bb\\ncc dd\\nee\\nFigure 3.7: A directed graphical model over random variables a, b, c, dand e. This graph\\ncorresponds to probability distributions that can be factored as\\np(a,b,c,d,e) = p(a)p(b a)p(c a,b)p(d b)p(e c). (3.54)\\n| | | |\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\nare usually not probability distributions of any kind. Any set of nodes that are all\\nconnected to each other in is called a clique. Each clique (i) in an undirected\\nG C\\nmodel is associated with a factor φ(i)( (i)). These factors are just functions, not\\nC\\nprobability distributions. The output of each factor must be non-negative, but\\nthere is no constraint that the factor must sum or integrate to 1 like a probability\\ndistribution.\\nThe probability of a configuration of random variables is proportional to the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='product of all of these factors—assignments that result in larger factor values are\\nmore likely. Of course, there is no guarantee that this product will sum to 1. We\\ntherefore divide by a normalizing constant Z, defined to be the sum or integral\\nover all states of the product of the φ functions, in order to obtain a normalized\\nprobability distribution:\\n1\\np(x) = φ(i) (i) . (3.55)\\nZ C\\n\\ue059i \\ue010 \\ue011\\nSee figure 3.8 for an example of an undirected graph and the factorization of\\nprobability distributions it represents.\\nKeep in mind that these graphical representations of factorizations are a\\nlanguage for describing probability distributions. They are not mutually exclusive\\nfamilies of probability distributions. Being directed or undirected is not a property\\nof a probability distribution; it is a property of a particular description of a\\n78\\nCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\naa bb\\ncc dd\\nee\\nFigure 3.8: An undirected graphical model over random variables a, b, c, d ande. This'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='graph corresponds to probability distributions that can be factored as\\n1\\np(a,b,c,d,e) = φ(1)(a,b,c)φ(2)(b,d)φ(3)(c,e). (3.56)\\nZ\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\nprobability distribution, but any probability distribution may be described in both\\nways.\\nThroughout parts I and II of this book, we will use structured probabilistic\\nmodels merely as a language to describe which direct probabilistic relationships\\ndifferent machine learning algorithms choose to represent. No further understanding\\nof structured probabilistic models is needed until the discussion of research topics,\\nin part III, where we will explore structured probabilistic models in much greater\\ndetail.\\nThis chapter has reviewed the basic concepts of probability theory that are\\nmost relevant to deep learning. One more set of fundamental mathematical tools\\nremains: numerical methods.\\n79\\nChapter 4'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Numerical Computation\\nMachine learning algorithms usually require a high amount of numerical compu-\\ntation. This typically refers to algorithms that solve mathematical problems by\\nmethods that update estimates of the solution via an iterative process, rather than\\nanalytically deriving a formula providing a symbolic expression for the correct so-\\nlution. Common operations include optimization (finding the value of an argument\\nthat minimizes or maximizes a function) and solving systems of linear equations.\\nEven just evaluating a mathematical function on a digital computer can be difficult\\nwhen the function involves real numbers, which cannot be represented precisely\\nusing a finite amount of memory.\\n4.1 Overflow and Underflow\\nThe fundamental difficulty in performing continuous math on a digital computer\\nis that we need to represent infinitely many real numbers with a finite number\\nof bit patterns. This means that for almost all real numbers, we incur some'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='approximation error when we represent the number in the computer. In many\\ncases, this is just rounding error. Rounding error is problematic, especially when\\nit compounds across many operations, and can cause algorithms that work in\\ntheory to fail in practice if they are not designed to minimize the accumulation of\\nrounding error.\\nOne form of rounding error that is particularly devastating is underflow.\\nUnderflow occurs when numbers near zero are rounded to zero. Many functions\\nbehave qualitatively differently when their argument is zero rather than a small\\npositive number. For example, we usually want to avoid division by zero (some\\n80\\nCHAPTER 4. NUMERICAL COMPUTATION\\nsoftware environments will raise exceptions when this occurs, others will return a\\nresult with a placeholder not-a-number value) or taking the logarithm of zero (this\\nis usually treated as , which then becomes not-a-number if it is used for many\\n−∞\\nfurther arithmetic operations).'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Another highly damaging form of numerical error is overflow. Overflow occurs\\nwhen numbers with large magnitude are approximated as or . Further\\n∞ −∞\\narithmetic will usually change these infinite values into not-a-number values.\\nOne example of a function that must be stabilized against underflow and\\noverflow is the softmax function. The softmax function is often used to predict the\\nprobabilities associated with a multinoulli distribution. The softmax function is\\ndefined to be\\nexp(x )\\ni\\nsoftmax(x) = . (4.1)\\ni n\\nexp(x )\\nj=1 j\\nConsider what happens when all of the x\\ni\\na\\ue050re equal to some constant c. Analytically,\\nwe can see that all of the outputs should be equal to 1. Numerically, this may\\nn\\nnot occur when c has large magnitude. If c is very negative, then exp(c) will\\nunderflow. This means the denominator of the softmax will become 0, so the final\\nresult is undefined. When c is very large and positive, exp(c) will overflow, again'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='resulting in the expression as a whole being undefined. Both of these difficulties\\ncan be resolved by instead evaluating softmax(z) where z =x max x . Simple\\ni i\\n−\\nalgebra shows that the value of the softmax function is not changed analytically by\\nadding or subtracting a scalar from the input vector. Subtracting max x results\\ni i\\nin the largest argument to exp being 0, which rules out the possibility of overflow.\\nLikewise, at least one term in the denominator has a value of 1, which rules out\\nthe possibility of underflow in the denominator leading to a division by zero.\\nThere is still one small problem. Underflow in the numerator can still cause\\nthe expression as a whole to evaluate to zero. This means that if we implement\\nlogsoftmax(x) by first running the softmax subroutine then passing the result to\\nthe log function, we could erroneously obtain . Instead, we must implement\\n−∞\\na separate function that calculates logsoftmax in a numerically stable way. The'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='logsoftmax function can be stabilized using the same trick as we used to stabilize\\nthe softmax function.\\nFor the most part, we do not explicitly detail all of the numerical considerations\\ninvolved in implementing the various algorithms described in this book. Developers\\nof low-level libraries should keep numerical issues in mind when implementing\\ndeep learning algorithms. Most readers of this book can simply rely on low-\\nlevel libraries that provide stable implementations. In some cases, it is possible\\nto implement a new algorithm and have the new implementation automatically\\n81\\nCHAPTER 4. NUMERICAL COMPUTATION\\nstabilized. Theano (Bergstra et al., 2010; Bastien et al., 2012) is an example\\nof a software package that automatically detects and stabilizes many common\\nnumerically unstable expressions that arise in the context of deep learning.\\n4.2 Poor Conditioning\\nConditioning refers to how rapidly a function changes with respect to small changes'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='in its inputs. Functions that changerapidly when their inputs are perturbed slightly\\ncan be problematic for scientific computation because rounding errors in the inputs\\ncan result in large changes in the output.\\nConsider the function f(x) = A 1x. When A Rn n has an eigenvalue\\n− ×\\n∈\\ndecomposition, its condition number is\\nλ\\ni\\nmax . (4.2)\\ni,j λ j\\n\\ue00c \\ue00c\\n\\ue00c \\ue00c\\n\\ue00c \\ue00c\\nThis is the ratio of the magnitude of the largest and smallest eigenvalue. When\\n\\ue00c \\ue00c\\nthis number is large, matrix inversion is particularly sensitive to error in the input.\\nThis sensitivity is an intrinsic property of the matrix itself, not the result\\nof rounding error during matrix inversion. Poorly conditioned matrices amplify\\npre-existing errors when we multiply by the true matrix inverse. In practice, the\\nerror will be compounded further by numerical errors in the inversion process itself.\\n4.3 Gradient-Based Optimization\\nMost deep learning algorithms involve optimization of some sort. Optimization'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='refers to the task of either minimizing or maximizing some function f(x) by altering\\nx. We usually phrase most optimization problems in terms of minimizing f(x).\\nMaximization may be accomplished via a minimization algorithm by minimizing\\nf(x).\\n−\\nThe function we want to minimize or maximize is called the objective func-\\ntion or criterion. When we are minimizing it, we may also call it the cost\\nfunction, loss function, or error function. In this book, we use these terms\\ninterchangeably, though some machine learning publications assign special meaning\\nto some of these terms.\\nWe often denote the value that minimizes or maximizes a function with a\\nsuperscript . For example, we might say x = argminf(x).\\n∗\\n∗\\n82\\nCHAPTER 4. NUMERICAL COMPUTATION\\n2.0\\n1.5 Global minimum at x= 0.\\nSince f\\ue030(x) = 0, gradient\\ndescent halts here.\\n1.0\\n0.5\\n0.0\\nFor x<0, we have f (x) <0, For x>0, we have f (x) >0,\\n\\ue030 \\ue030\\nso we can decrease f by so we can decrease f by\\n0.5 moving rightward. moving leftward.\\n−\\n1.0\\n−'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='f(x) = 1x2\\n2\\n1.5\\n− f (x) = x\\n\\ue030\\n2.0\\n−\\n2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0\\n− − − −\\nx\\nFigure 4.1: An illustration of how the gradient descent algorithm uses the derivatives of a\\nfunction can be used to follow the function downhill to a minimum.\\nWe assume the reader is already familiar with calculus, but provide a brief\\nreview of how calculus concepts relate to optimization here.\\nSuppose we have a function y =f(x), where both x and y are real numbers.\\ndy\\nThe derivative of this function is denoted as f (x) or as . The derivative f (x)\\n\\ue030 dx \\ue030\\ngives the slope of f(x) at the point x. In other words, it specifies how to scale\\na small change in the input in order to obtain the corresponding change in the\\noutput: f(x +\\ue00f) f(x)+\\ue00ff (x).\\n\\ue030\\n≈\\nThe derivative is therefore useful for minimizing a function because it tells\\nus how to change x in order to make a small improvement in y. For example,\\nwe know that f(x \\ue00fsign(f (x))) is less than f(x) for small enough \\ue00f. We can\\n\\ue030\\n−'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='thus reduce f(x) by moving x in small steps with opposite sign of the derivative.\\nThis technique is called gradient descent (Cauchy, 1847). See figure 4.1 for an\\nexample of this technique.\\nWhen f (x) = 0, the derivative provides no information about which direction\\n\\ue030\\nto move. Points where f (x) = 0 are known as critical points or stationary\\n\\ue030\\npoints. A local minimum is a point where f(x) is lower than at all neighboring\\npoints, so it is no longer possible to decrease f(x) by making infinitesimal steps.\\nA local maximum is a point where f(x) is higher than at all neighboring points,\\n83\\nCHAPTER 4. NUMERICAL COMPUTATION\\nMinimum Maximum Saddle point\\nFigure 4.2: Examples of each of the three types of critical points in 1-D. A critical point is\\na point with zero slope. Such a point can either be a local minimum, which is lower than\\nthe neighboring points, a local maximum, which is higher than the neighboring points, or'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a saddle point, which has neighbors that are both higher and lower than the point itself.\\nso it is not possible to increase f(x) by making infinitesimal steps. Some critical\\npoints are neither maxima nor minima. These are known as saddle points. See\\nfigure 4.2 for examples of each type of critical point.\\nA point that obtains the absolute lowest value of f(x) is a global minimum.\\nIt is possible for there to be only one global minimum or multiple global minima of\\nthe function. It is also possible for there to be local minima that are not globally\\noptimal. In the context of deep learning, we optimize functions that may have\\nmany local minima that are not optimal, and many saddle points surrounded by\\nvery flat regions. All of this makes optimization very difficult, especially when the\\ninput to the function is multidimensional. We therefore usually settle for finding a\\nvalue of f that is very low, but not necessarily minimal in any formal sense. See\\nfigure 4.3 for an example.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='We often minimize functions that have multiple inputs: f :\\nRn R\\n. For the\\n→\\nconcept of “minimization” to make sense, there must still be only one (scalar)\\noutput.\\nFor functions with multiple inputs, we must make use of the concept of partial\\nderivatives. The partial derivative ∂ f(x) measures how f changes as only the\\n∂x\\ni\\nvariable x increases at point x. The gradient generalizes the notion of derivative\\ni\\nto the case where the derivative is with respect to a vector: the gradient of f is the\\nvector containing all of the partial derivatives, denoted f(x). Element i of the\\nx\\n∇\\ngradient is the partial derivative of f with respect to x . In multiple dimensions,\\ni\\n84\\nCHAPTER 4. NUMERICAL COMPUTATION\\nx\\n)x(f\\nThis local minimum\\nperforms nearly as well as\\nthe global one,\\nso it is an acceptable\\nhalting point.\\nIdeally, we would like\\nto arrive at the global\\nminimum, but this\\nmight not be possible.\\nThis local minimum performs\\npoorly and should be avoided.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Figure 4.3: Optimization algorithms may fail to find a global minimum when there are\\nmultiple local minima or plateaus present. In the context of deep learning, we generally\\naccept such solutions even though they are not truly minimal, so long as they correspond\\nto significantly low values of the cost function.\\ncritical points are points where every element of the gradient is equal to zero.\\nThe directional derivative in direction u (a unit vector) is the slope of the\\nfunction f in direction u. In other words, the directional derivative is the derivative\\nof the function f(x+ αu) with respect to α, evaluated at α= 0. Using the chain\\nrule, we can see that ∂ f(x+αu) evaluates to u f(x) when α = 0.\\n∂α \\ue03e ∇x\\nTo minimize f, we would like to find the direction in which f decreases the\\nfastest. We can do this using the directional derivative:\\nmin u f(x) (4.3)\\n\\ue03e x\\nu,u u=1 ∇\\n\\ue03e\\n= min u f(x) cosθ (4.4)\\n2 x 2\\nu,u u=1|| || ||∇ ||\\n\\ue03e'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='where θ is the angle between u and the gradient. Substituting in u = 1 and\\n2\\n|| ||\\nignoring factors that do not depend on u, this simplifies to min cosθ. This is\\nu\\nminimized when u points in the opposite direction as the gradient. In other\\nwords, the gradient points directly uphill, and the negative gradient points directly\\ndownhill. We can decrease f by moving in the direction of the negative gradient.\\nThis is known as the method of steepest descent or gradient descent.\\nSteepest descent proposes a new point\\nx = x \\ue00f f(x) (4.5)\\n\\ue030 x\\n− ∇\\n85\\nCHAPTER 4. NUMERICAL COMPUTATION\\nwhere \\ue00f is the learning rate, a positive scalar determining the size of the step.\\nWe can choose \\ue00f in several different ways. A popular approach is to set \\ue00f to a small\\nconstant. Sometimes, we can solve for the step size that makes the directional\\nderivative vanish. Another approach is to evaluate f (x \\ue00f f(x)) for several\\nx\\n− ∇\\nvalues of \\ue00f and choose the one that results in the smallest objective function value.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='This last strategy is called a line search.\\nSteepest descent converges when every element of the gradient is zero (or, in\\npractice, very close to zero). In some cases, we may be able to avoid running this\\niterative algorithm, and just jump directly to the critical point by solving the\\nequation f(x) = 0 for x.\\nx\\n∇\\nAlthough gradient descent is limited to optimization in continuous spaces, the\\ngeneral concept of repeatedly making a small move (that is approximately the best\\nsmall move) towards better configurations can be generalized to discrete spaces.\\nAscending an objective function of discrete parameters is called hill climbing\\n(Russel and Norvig, 2003).\\n4.3.1 Beyond the Gradient: Jacobian and Hessian Matrices\\nSometimes we need to find all of the partial derivatives of a function whose input\\nand output are both vectors. The matrix containing all such partial derivatives is\\nknown as a Jacobian matrix. Specifically, if we have a function f :\\nRm Rn,\\n→'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='then the Jacobian matrix J Rn m of f is defined such that J = ∂ f(x) .\\n∈ × i,j ∂x j i\\nWe are also sometimes interested in a derivative of a derivative. This is known\\nas a second derivative. For example, for a function f :\\nRn R\\n, the derivative\\nwith respect to x of the derivative of f with respect to x\\nis→\\ndenoted as\\n∂2\\nf.\\ni j ∂x ∂x\\ni j\\nIn a single dimension, we can denote\\nd2\\nf by f (x). The second derivative tells\\ndx2 \\ue030\\ue030\\nus how the first derivative will change as we vary the input. This is important\\nbecause it tells us whether a gradient step will cause as much of an improvement\\nas we would expect based on the gradient alone. We can think of the second\\nderivative as measuring curvature. Suppose we have a quadratic function (many\\nfunctions that arise in practice are not quadratic but can be approximated well\\nas quadratic, at least locally). If such a function has a second derivative of zero,\\nthen there is no curvature. It is a perfectly flat line, and its value can be predicted'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='using only the gradient. If the gradient is 1, then we can make a step of size \\ue00f\\nalong the negative gradient, and the cost function will decrease by \\ue00f. If the second\\nderivative is negative, the function curves downward, so the cost function will\\nactually decrease by more than \\ue00f. Finally, if the second derivative is positive, the\\nfunction curves upward, so the cost function can decrease by less than \\ue00f. See\\n86\\nCHAPTER 4. NUMERICAL COMPUTATION\\nx\\n)x(f\\nNegative curvature\\nx\\n)x(f\\nNo curvature\\nx\\n)x(f\\nPositive curvature\\nFigure 4.4: The second derivative determines the curvature of a function. Here we show\\nquadratic functions with various curvature. The dashed line indicates the value of the cost\\nfunction we would expect based on the gradient information alone as we make a gradient\\nstep downhill. In the case of negative curvature, the cost function actually decreases faster\\nthan the gradient predicts. In the case of no curvature, the gradient predicts the decrease'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='correctly. In the case of positive curvature, the function decreases slower than expected\\nand eventually begins to increase, so steps that are too large can actually increase the\\nfunction inadvertently.\\nfigure 4.4 to see how different forms of curvature affect the relationship between\\nthe value of the cost function predicted by the gradient and the true value.\\nWhen our function has multiple input dimensions, there are many second\\nderivatives. These derivatives can be collected together into a matrix called the\\nHessian matrix. The Hessian matrix H(f)(x) is defined such that\\n∂2\\nH(f)(x) = f(x). (4.6)\\ni,j\\n∂x ∂x\\ni j\\nEquivalently, the Hessian is the Jacobian of the gradient.\\nAnywhere that the second partial derivatives are continuous, the differential\\noperators are commutative, i.e. their order can be swapped:\\n∂2 ∂2\\nf(x) = f(x). (4.7)\\n∂x ∂x ∂x ∂x\\ni j j i\\nThis implies that H = H , so the Hessian matrix is symmetric at such points.\\ni,j j,i'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Most of the functions we encounter in the contextof deep learning have a symmetric\\nHessian almost everywhere. Because the Hessian matrix is real and symmetric,\\nwe can decompose it into a set of real eigenvalues and an orthogonal basis of\\n87\\nCHAPTER 4. NUMERICAL COMPUTATION\\neigenvectors. The second derivative in a specific direction represented by a unit\\nvector d is given by d Hd. When dis an eigenvector of H, the second derivative\\n\\ue03e\\nin that direction is given by the corresponding eigenvalue. For other directions of\\nd, the directional second derivative is a weighted average of all of the eigenvalues,\\nwith weights between 0 and 1, and eigenvectors that have smaller angle with d\\nreceiving more weight. The maximum eigenvalue determines the maximum second\\nderivative and the minimum eigenvalue determines the minimum second derivative.\\nThe (directional) second derivative tells us how well we can expect a gradient\\ndescent step to perform. We can make a second-order Taylor series approximation'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to the function f(x) around the current point x(0):\\n1\\nf(x) f(x(0))+(x x(0)) g + (x x(0)) H(x x(0)). (4.8)\\n\\ue03e \\ue03e\\n≈ − 2 − −\\nwhere g is the gradient and H is the Hessian at x(0). If we use a learning rate\\nof \\ue00f, then the new point xwill be given by x(0) \\ue00fg. Substituting this into our\\n−\\napproximation, we obtain\\n1\\nf(x(0) \\ue00fg) f(x(0)) \\ue00fg g + \\ue00f2g Hg. (4.9)\\n\\ue03e \\ue03e\\n− ≈ − 2\\nThere are three terms here: the original value of the function, the expected\\nimprovement due to the slope of the function, and the correction we must apply\\nto account for the curvature of the function. When this last term is too large, the\\ngradient descent step can actually move uphill. When g Hg is zero or negative,\\n\\ue03e\\nthe Taylor series approximation predicts that increasing \\ue00f forever will decrease f\\nforever. In practice, the Taylor series is unlikely to remain accurate for large \\ue00f, so\\none must resort to more heuristic choices of \\ue00f in this case. When g Hg is positive,\\n\\ue03e'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='solving for the optimal step size that decreases the Taylor series approximation of\\nthe function the most yields\\ng g\\n\\ue03e\\n\\ue00f = . (4.10)\\n∗\\ng Hg\\n\\ue03e\\nIn the worst case, when g aligns with the eigenvector of H corresponding to the\\nmaximal eigenvalue λ , then this optimal step size is given by 1 . To the\\nmax λmax\\nextent that the function we minimize can be approximated well by a quadratic\\nfunction, the eigenvalues of the Hessian thus determine the scale of the learning\\nrate.\\nThe second derivative can be used to determine whether a critical point is\\na local maximum, a local minimum, or saddle point. Recall that on a critical\\npoint, f (x) = 0. When the second derivative f (x) >0, the first derivative f (x)\\n\\ue030 \\ue030\\ue030 \\ue030\\nincreases as we move to the right and decreases as we move to the left. This means\\n88\\nCHAPTER 4. NUMERICAL COMPUTATION\\nf (x \\ue00f)< 0 and f (x + \\ue00f) > 0 for small enough \\ue00f. In other words, as we move\\n\\ue030 \\ue030\\n−\\nright, the slope begins to point uphill to the right, and as we move left, the slope'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='begins to point uphill to the left. Thus, when f (x) = 0 and f (x) > 0, we can\\n\\ue030 \\ue030\\ue030\\nconclude that x is a local minimum. Similarly, when f (x) = 0 and f (x) < 0, we\\n\\ue030 \\ue030\\ue030\\ncan conclude that x is a local maximum. This is known as the second derivative\\ntest. Unfortunately, when f (x) = 0, the test is inconclusive. In this case x may\\n\\ue030\\ue030\\nbe a saddle point, or a part of a flat region.\\nIn multiple dimensions, we need to examine all of the second derivatives of the\\nfunction. Using the eigendecomposition of the Hessian matrix, we can generalize\\nthe second derivative test to multiple dimensions. At a critical point, where\\nf(x) = 0, we can examine the eigenvalues of the Hessian to determine whether\\nx\\n∇\\nthe critical point is a local maximum, local minimum, or saddle point. When the\\nHessian is positive definite (all its eigenvalues are positive), the point is a local\\nminimum. This can be seen by observing that the directional second derivative'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='in any direction must be positive, and making reference to the univariate second\\nderivative test. Likewise, when the Hessian is negative definite (all its eigenvalues\\nare negative), the point is a local maximum. In multiple dimensions, it is actually\\npossible to find positive evidence of saddle points in some cases. When at least\\none eigenvalue is positive and at least one eigenvalue is negative, we know that\\nx is a local maximum on one cross section of f but a local minimum on another\\ncross section. See figure 4.5 for an example. Finally, the multidimensional second\\nderivative test can be inconclusive, just like the univariate version. The test is\\ninconclusive whenever all of the non-zero eigenvalues have the same sign, but at\\nleast one eigenvalue is zero. This is because the univariate second derivative test is\\ninconclusive in the cross section corresponding to the zero eigenvalue.\\nIn multiple dimensions, there is a different second derivative for each direction'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='at a single point. The condition number of the Hessian at this point measures\\nhow much the second derivatives differ from each other. When the Hessian has a\\npoor condition number, gradient descent performs poorly. This is because in one\\ndirection, the derivative increases rapidly, while in another direction, it increases\\nslowly. Gradient descent is unaware of this change in the derivative so it does not\\nknow that it needs to explore preferentially in the direction where the derivative\\nremains negative for longer. It also makes it difficult to choose a good step size.\\nThe step size must be small enough to avoid overshooting the minimum and going\\nuphill in directions with strong positive curvature. This usually means that the\\nstep size is too small to make significant progress in other directions with less\\ncurvature. See figure 4.6 for an example.\\nThis issue can be resolvedbyusing information from the Hessian matrix to guide\\n89\\nCHAPTER 4. NUMERICAL COMPUTATION\\n\\ue035\\ue030\\ue030\\ue029\\n\\ue032\\n\\ue078\\n\\ue03b\\n\\ue030 \\ue078\\ue031\\n\\ue028\\n\\ue066\\n\\uf091\\ue035\\ue030\\ue030'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='\\ue031\\ue035\\n\\uf091\\ue031\\ue035\\n\\ue030\\ue078\\n\\ue032\\n\\ue078 \\ue030 \\uf091\\ue031\\ue035\\n\\ue031 \\ue031\\ue035\\nFigure 4.5: A saddle point containing both positive and negative curvature. The function\\nin this example is f(x) = x2 x2. Along the axis corresponding to x , the function\\n1 2 1\\n−\\ncurves upward. This axis is an eigenvector of the Hessian and has a positive eigenvalue.\\nAlong the axis corresponding to x , the function curves downward. This direction is an\\n2\\neigenvector of the Hessian with negative eigenvalue. The name “saddle point” derives from\\nthe saddle-like shape of this function. This is the quintessential example of a function\\nwith a saddle point. In more than one dimension, it is not necessary to have an eigenvalue\\nof 0 in order to get a saddle point: it is only necessary to have both positive and negative\\neigenvalues. We can think of a saddle point with both signs of eigenvalues as being a local\\nmaximum within one cross section and a local minimum within another cross section.\\n90\\nCHAPTER 4. NUMERICAL COMPUTATION\\n20\\n10\\n0\\n10\\n−\\n20\\n−\\n30\\n− 30 20 10 0 10 20'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='− − −\\nx1\\nx\\n2\\nFigure 4.6: Gradient descent fails to exploit the curvature information contained in the\\nHessian matrix. Here we use gradientdescent to minimize a quadratic function f(x) whose\\nHessian matrix has condition number 5. This means that the direction of most curvature\\nhas five times more curvature than the direction of least curvature. In this case, the most\\ncurvature is in the direction [1,1] and the least curvature is in the direction [1, 1] . The\\n\\ue03e \\ue03e\\n−\\nred lines indicate the path followed by gradient descent. This very elongated quadratic\\nfunction resembles a long canyon. Gradient descent wastes time repeatedly descending\\ncanyon walls, because they are the steepest feature. Because the step size is somewhat\\ntoo large, it has a tendency to overshoot the bottom of the function and thus needs to\\ndescend the opposite canyon wall on the next iteration. The large positive eigenvalue\\nof the Hessian corresponding to the eigenvector pointed in this direction indicates that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='this directional derivative is rapidly increasing, so an optimization algorithm based on\\nthe Hessian could predict that the steepest direction is not actually a promising search\\ndirection in this context.\\n91\\nCHAPTER 4. NUMERICAL COMPUTATION\\nthe search. The simplest method for doing so is known as Newton’s method.\\nNewton’s method is based on using a second-order Taylor series expansion to\\napproximate f(x) near some point x(0):\\n1\\nf(x) f(x(0))+(x x(0)) f(x(0))+ (x x(0)) H(f)(x(0))(x x(0)). (4.11)\\n\\ue03e x \\ue03e\\n≈ − ∇ 2 − −\\nIf we then solve for the critical point of this function, we obtain:\\nx = x(0) H(f)(x(0)) 1 f(x(0)). (4.12)\\n∗ − x\\n− ∇\\nWhen f is a positive definite quadratic function, Newton’s method consists of\\napplying equation 4.12 once to jump to the minimum of the function directly.\\nWhen f is not truly quadratic but can be locally approximated as a positive\\ndefinite quadratic, Newton’s method consists of applying equation 4.12 multiple'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='times. Iteratively updating the approximation and jumping to the minimum of\\nthe approximation can reach the critical point much faster than gradient descent\\nwould. This is a useful property near a local minimum, but it can be a harmful\\nproperty near a saddle point. As discussed in section 8.2.3, Newton’s method is\\nonly appropriate when the nearby critical point is a minimum (all the eigenvalues\\nof the Hessian are positive), whereas gradient descent is not attracted to saddle\\npoints unless the gradient points toward them.\\nOptimization algorithms that use only the gradient, such as gradient descent,\\nare called first-order optimization algorithms. Optimization algorithms that\\nalso use the Hessian matrix, such as Newton’s method, are called second-order\\noptimization algorithms (Nocedal and Wright, 2006).\\nThe optimization algorithms employed in most contexts in this book are\\napplicable to a wide variety of functions, but come with almost no guarantees.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Deep learning algorithms tend to lack guarantees because the family of functions\\nused in deep learning is quite complicated. In many other fields, the dominant\\napproach to optimization is to design optimization algorithms for a limited family\\nof functions.\\nIn the context of deep learning, we sometimes gain some guarantees by restrict-\\ning ourselves to functions that are either Lipschitz continuous or have Lipschitz\\ncontinuous derivatives. A Lipschitz continuous function is a function f whose rate\\nof change is bounded by a Lipschitz constant :\\nL\\nx, y, f(x) f(y) x y . (4.13)\\n2\\n∀ ∀ | − | ≤ L|| − ||\\nThis property is useful because it allows us to quantify our assumption that a\\nsmall change in the input made by an algorithm such as gradient descent will have\\n92\\nCHAPTER 4. NUMERICAL COMPUTATION\\na small change in the output. Lipschitz continuity is also a fairly weak constraint,\\nand many optimization problems in deep learning can be made Lipschitz continuous'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='with relatively minor modifications.\\nPerhaps the most successful field of specialized optimization is convex op-\\ntimization. Convex optimization algorithms are able to provide many more\\nguarantees by making stronger restrictions. Convex optimization algorithms are\\napplicable only to convex functions—functions for which the Hessian is positive\\nsemidefinite everywhere. Such functions are well-behaved because they lack saddle\\npoints and all of their local minima are necessarily global minima. However, most\\nproblems in deep learning are difficult to express in terms of convex optimization.\\nConvex optimization is used only as a subroutine of some deep learning algorithms.\\nIdeas from the analysis of convex optimization algorithms can be useful for proving\\nthe convergence of deep learning algorithms. However, in general, the importance\\nof convex optimization is greatly diminished in the context of deep learning. For\\nmore information about convex optimization, see Boyd and Vandenberghe (2004)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='or Rockafellar (1997).\\n4.4 Constrained Optimization\\nSometimes we wish not only to maximize or minimize a function f(x) over all\\npossible values of x. Instead we may wish to find the maximal or minimal\\nS\\nvalue of f(x) for values of x in some set . This is known as constrained\\nS\\noptimization. Points x that lie within the set are called feasible points in\\nconstrained optimization terminology.\\nWe often wish to find a solution that is small in some sense. A common\\napproach in such situations is to impose a norm constraint, such as x 1.\\n|| || ≤\\nOne simple approach to constrained optimization is simply to modify gradient\\ndescent taking the constraint into account. If we use a small constant step size \\ue00f,\\nS\\nwe can make gradient descent steps, then project the result back into . If we use\\na line search, we can search only over step sizes \\ue00f that yield new x points that are\\nfeasible, or we can project each point on the line back into the constraint region.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='When possible, this method can be made more efficient by projecting the gradient\\ninto the tangent space of the feasible region before taking the step or beginning\\nthe line search (Rosen, 1960).\\nA more sophisticated approach is to design a different, unconstrained opti-\\nmization problem whose solution can be converted into a solution to the original,\\nconstrained optimization problem. For example, if we want to minimize f(x) for\\n93\\nCHAPTER 4. NUMERICAL COMPUTATION\\nx R2 with x constrained to have exactly unit L2 norm, we can instead minimize\\n∈\\ng(θ) =f ([cosθ,sinθ] ) with respect to θ, then return [cosθ,sinθ] as the solution\\n\\ue03e\\nto the original problem. This approach requires creativity; the transformation\\nbetween optimization problems must be designed specifically for each case we\\nencounter.\\nThe Karush–Kuhn–Tucker (KKT) approach1 provides a very general so-\\nlution to constrained optimization. With the KKT approach, we introduce a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='new function called the generalized Lagrangian or generalized Lagrange\\nfunction.\\nS\\nTo define the Lagrangian, we first need to describe in terms of equations\\nand inequalities. We want a description of S in terms of m functions g(i) and n\\nfunctions h(j) so thatS = x i,g(i)(x) = 0 and j,h(j)(x) 0 . The equations\\n{ | ∀ ∀ ≤ }\\ninvolving g(i) are called the equality constraints and the inequalities involving\\nh(j) are called inequality constraints.\\nWe introduce new variables λ andα for each constraint, these are called the\\ni j\\nKKT multipliers. The generalized Lagrangian is then defined as\\nL(x,λ,α) = f(x)+ λ g(i)(x)+ α h(j)(x). (4.14)\\ni j\\ni j\\n\\ue058 \\ue058\\nWe can now solve a constrained minimization problem using unconstrained\\noptimization of the generalized Lagrangian. Observe that, so long as at least one\\nfeasible point exists and f(x) is not permitted to have value , then\\n∞\\nminmax max L(x,λ,α). (4.15)\\nx λ α,α 0\\n≥\\nhas the same optimal objective function value and set of optimal points x as'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='minf(x). (4.16)\\nx S\\n∈\\nThis follows because any time the constraints are satisfied,\\nmax max L(x,λ,α) = f(x), (4.17)\\nλ α,α 0\\n≥\\nwhile any time a constraint is violated,\\nmax max L(x,λ,α) = . (4.18)\\nλ α,α 0 ∞\\n≥\\n1The KKT approach generalizes the method ofLagrange multipliers which allows equality\\nconstraints but not inequality constraints.\\n94\\nCHAPTER 4. NUMERICAL COMPUTATION\\nThese properties guarantee that no infeasible point can be optimal, and that the\\noptimum within the feasible points is unchanged.\\nTo perform constrained maximization, we can construct the generalized La-\\ngrange function of f(x), which leads to this optimization problem:\\n−\\nminmax max f(x)+ λ g(i)(x)+ α h(j)(x). (4.19)\\ni j\\nx λ α,α 0−\\n≥ i j\\n\\ue058 \\ue058\\nWe may also convert this to a problem with maximization in the outer loop:\\nmaxmin min f(x)+ λ g(i)(x) α h(j)(x). (4.20)\\ni j\\nx λ α,α 0 −\\n≥ i j\\n\\ue058 \\ue058\\nThe sign of the term for the equality constraints does not matter; we may define it'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='with addition or subtraction as we wish, because the optimization is free to choose\\nany sign for each λ .\\ni\\nThe inequality constraints are particularly interesting. We say that a constraint\\nh(i)(x) isactive if h(i)(x ) = 0. If a constraint is not active, then the solution to\\n∗\\nthe problem found using that constraint would remain at least a local solution if\\nthat constraint were removed. It is possible that an inactive constraint excludes\\nother solutions. For example, a convex problem with an entire region of globally\\noptimal points (a wide, flat, region of equal cost) could have a subset of this\\nregion eliminated by constraints, or a non-convex problem could have better local\\nstationary points excluded by a constraint that is inactive at convergence. However,\\nthe point found at convergence remains a stationary point whether or not the\\ninactive constraints are included. Because an inactive h(i) has negative value, then\\nthe solution to min max max L(x,λ,α) will have α = 0. We can thus'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='x λ α,α 0 i\\n≥\\nobserve that at the solution, α h(x) = 0. In other words, for all i, we know\\n\\ue00c\\nthat at least one of the constraints α 0 and h(i)(x) 0 must be active at the\\ni\\n≥ ≤\\nsolution. To gain some intuition for this idea, we can say that either the solution\\nis on the boundary imposed by the inequality and we must use its KKT multiplier\\nto influence the solution to x, or the inequality has no influence on the solution\\nand we represent this by zeroing out its KKT multiplier.\\nA simple set of properties describe the optimal points of constrained opti-\\nmization problems. These properties are called the Karush-Kuhn-Tucker (KKT)\\nconditions (Karush, 1939; Kuhn and Tucker, 1951). They are necessary conditions,\\nbut not always sufficient conditions, for a point to be optimal. The conditions are:\\nThe gradient of the generalized Lagrangian is zero.\\n•\\nAll constraints on both x and the KKT multipliers are satisfied.\\n•\\n95\\nCHAPTER 4. NUMERICAL COMPUTATION'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The inequality constraints exhibit “complementary slackness”: α h(x) = 0.\\n• \\ue00c\\nFor more information about the KKT approach, see Nocedal and Wright (2006).\\n4.5 Example: Linear Least Squares\\nSuppose we want to find the value of x that minimizes\\n1\\nf(x) = Ax b 2. (4.21)\\n2|| − ||2\\nThereare specialized linearalgebra algorithms that cansolvethis problemefficiently.\\nHowever, we can also explore how to solve it using gradient-based optimization as\\na simple example of how these techniques work.\\nFirst, we need to obtain the gradient:\\nf(x) = A (Ax b) = A Ax A b. (4.22)\\nx \\ue03e \\ue03e \\ue03e\\n∇ − −\\nWe can then follow this gradient downhill, taking small steps. See algorithm 4.1\\nfor details.\\nAlgorithm 4.1 An algorithm to minimize f(x) = 1 Ax b 2 with respect to x\\n2|| − ||2\\nusing gradient descent, starting from an arbitrary value of x.\\nSet the step size (\\ue00f) and tolerance (δ) to small, positive numbers.\\nwhile A Ax A b > δ do\\n\\ue03e \\ue03e 2\\n|| − ||\\nx x \\ue00f A Ax A b\\n\\ue03e \\ue03e\\n← − −\\nend while\\n\\ue000 \\ue001'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='One can also solve this problem using Newton’s method. In this case, because\\nthe true function is quadratic, the quadratic approximation employed by Newton’s\\nmethod is exact, and the algorithm converges to the global minimum in a single\\nstep.\\nNow suppose we wish to minimize the same function, but subject to the\\nconstraint x x 1. To do so, we introduce the Lagrangian\\n\\ue03e\\n≤\\nL(x,λ) = f(x)+λ x x 1 . (4.23)\\n\\ue03e\\n−\\n\\ue010 \\ue011\\nWe can now solve the problem\\nmin max L(x,λ). (4.24)\\nx λ,λ 0\\n≥\\n96\\nCHAPTER 4. NUMERICAL COMPUTATION\\nThe smallest-norm solution to the unconstrained least squares problem may be\\nfound using the Moore-Penrose pseudoinverse: x =A+b. If this point is feasible,\\nthen it is the solution to the constrained problem. Otherwise, we must find a\\nsolution where the constraint is active. By differentiating the Lagrangian with\\nrespect to x, we obtain the equation\\nA Ax A b+2λx = 0. (4.25)\\n\\ue03e \\ue03e\\n−\\nThis tells us that the solution will take the form\\nx = (A A+2λI) 1A b. (4.26)\\n\\ue03e − \\ue03e'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The magnitude of λ must be chosen such that the result obeys the constraint. We\\ncan find this value by performing gradient ascent on λ. To do so, observe\\n∂\\nL(x,λ) = x x 1. (4.27)\\n\\ue03e\\n∂λ −\\nWhen the norm of xexceeds 1, this derivative is positive, so to follow the derivative\\nuphill and increase the Lagrangian with respect to λ, we increase λ. Because the\\ncoefficient on the x xpenalty has increased, solving the linear equation for x will\\n\\ue03e\\nnow yield a solution with smaller norm. The process of solving the linear equation\\nand adjusting λ continues until x has the correct norm and the derivative on λ is\\n0.\\nThis concludes the mathematical preliminaries that we use to develop machine\\nlearning algorithms. We are now ready to build and analyze some full-fledged\\nlearning systems.\\n97\\nChapter 5\\nMachine Learning Basics\\nDeep learning is a specific kind of machine learning. In order to understand\\ndeep learning well, one must have a solid understanding of the basic principles of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='machinelearning. Thischapterprovidesabriefcourseinthemostimportantgeneral\\nprinciples that will be applied throughout the rest of the book. Novice readers or\\nthose who want a wider perspective are encouraged to consider machine learning\\ntextbookswith amore comprehensivecoverage ofthefundamentals, suchasMurphy\\n(2012) or Bishop (2006). If you are already familiar with machine learning basics,\\nfeel free to skip ahead to section 5.11. That section covers some perspectives\\non traditional machine learning techniques that have strongly influenced the\\ndevelopment of deep learning algorithms.\\nWe begin with a definition of what a learning algorithm is, and present an\\nexample: the linear regression algorithm. We then proceed to describe how the\\nchallenge of fitting the training data differs from the challenge of finding patterns\\nthat generalize to new data. Most machine learning algorithms have settings\\ncalled hyperparameters that must be determined external to the learning algorithm'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='itself; we discuss how to set these using additional data. Machine learning is\\nessentially a form of applied statistics with increased emphasis on the use of\\ncomputers to statistically estimate complicated functions and a decreased emphasis\\non proving confidence intervals around these functions; we therefore present the\\ntwo central approaches to statistics: frequentist estimators and Bayesian inference.\\nMost machine learning algorithms can be divided into the categories of supervised\\nlearning and unsupervised learning; we describe these categories and give some\\nexamples of simple learning algorithms from each category. Most deep learning\\nalgorithms are based on an optimization algorithm called stochastic gradient\\ndescent. We describe how to combine various algorithm components such as\\n98\\nCHAPTER 5. MACHINE LEARNING BASICS\\nan optimization algorithm, a cost function, a model, and a dataset to build a\\nmachine learning algorithm. Finally, in section 5.11, we describe some of the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='factors that have limited the ability of traditional machine learning to generalize.\\nThese challenges have motivated the development of deep learning algorithms that\\novercome these obstacles.\\n5.1 Learning Algorithms\\nA machine learning algorithm is an algorithm that is able to learn from data. But\\nwhat do we mean by learning? Mitchell (1997) provides the definition “A computer\\nprogram is said to learn from experience E with respect to some class of tasks T\\nand performance measure P, if its performance at tasks in T, as measured by P,\\nimproves with experience E.” One can imagine a very wide variety of experiences\\nE, tasks T, and performance measures P, and we do not make any attempt in this\\nbook to provide a formal definition of what may be used for each of these entities.\\nInstead, the following sections provide intuitive descriptions and examples of the\\ndifferent kinds of tasks, performance measures and experiences that can be used\\nto construct machine learning algorithms.\\nT'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='5.1.1 The Task,\\nMachine learning allows us to tackle tasks that are too difficult to solve with\\nfixed programs written and designed by human beings. From a scientific and\\nphilosophical point of view, machine learning is interesting because developing our\\nunderstanding of machine learning entails developing our understanding of the\\nprinciples that underlie intelligence.\\nIn this relatively formal definition of the word “task,” the process of learning\\nitself is not the task. Learning is our means of attaining the ability to perform the\\ntask. For example, if we want a robot to be able to walk, then walking is the task.\\nWe could program the robot to learn to walk, or we could attempt to directly write\\na program that specifies how to walk manually.\\nMachine learning tasks are usually described in terms of how the machine\\nlearningsystem shouldprocess an example. Anexample isa collectionof features\\nthat have been quantitatively measured from some object or event that we want'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the machine learning system to process. We typically represent an example as a\\nvector x\\nRn\\nwhere each entry x of the vector is another feature. For example,\\ni\\n∈\\nthe features of an image are usually the values of the pixels in the image.\\n99\\nCHAPTER 5. MACHINE LEARNING BASICS\\nMany kinds of tasks can be solved with machine learning. Some of the most\\ncommon machine learning tasks include the following:\\nClassification: In this typeof task, the computerprogram is askedto specify\\n•\\nwhich of k categories some input belongs to. To solve this task, the learning\\nalgorithm is usually asked to produce a function f : Rn 1,...,k . When\\n→ { }\\ny = f(x), the model assigns an input described by vector x to a category\\nidentified by numeric code y. There are other variants of the classification\\ntask, for example, where f outputs a probability distribution over classes.\\nAn example of a classification task is object recognition, where the input'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='is an image (usually described as a set of pixel brightness values), and the\\noutput is a numeric code identifying the object in the image. For example,\\nthe Willow Garage PR2 robot is able to act as a waiter that can recognize\\ndifferent kinds of drinks and deliver them to people on command (Good-\\nfellow et al., 2010). Modern object recognition is best accomplished with\\ndeep learning (Krizhevsky et al., 2012; Ioffe and Szegedy, 2015). Object\\nrecognition is the same basic technology that allows computers to recognize\\nfaces (Taigman et al., 2014), which can be used to automatically tag people\\nin photo collections and allow computers to interact more naturally with\\ntheir users.\\nClassification with missing inputs: Classification becomes more chal-\\n•\\nlenging if the computer program is not guaranteed that every measurement\\nin its input vector will always be provided. In order to solve the classification\\ntask, the learning algorithm only has to define a single function mapping'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='from a vector input to a categorical output. When some of the inputs may\\nbe missing, rather than providing a single classification function, the learning\\nalgorithm must learn a set of functions. Each function corresponds to classi-\\nfying x with a different subset of its inputs missing. This kind of situation\\narises frequently in medical diagnosis, because many kinds of medical tests\\nare expensive or invasive. One way to efficiently define such a large set\\nof functions is to learn a probability distribution over all of the relevant\\nvariables, then solve the classification task by marginalizing out the missing\\nvariables. With n input variables, we can now obtain all 2n different classifi-\\ncation functions needed for each possible set of missing inputs, but we only\\nneed to learn a single function describing the joint probability distribution.\\nSee Goodfellow et al. (2013b) for an example of a deep probabilistic model'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='applied to such a task in this way. Many of the other tasks described in this\\nsection can also be generalized to work with missing inputs; classification\\nwith missing inputs is just one example of what machine learning can do.\\n100\\nCHAPTER 5. MACHINE LEARNING BASICS\\nRegression: In this type of task, the computer program is asked to predict a\\n•\\nnumerical value given some input. To solve this task, the learning algorithm\\nis asked to output a function f :\\nRn R\\n. This type of task is similar to\\n→\\nclassification, except that the format of output is different. An example of\\na regression task is the prediction of the expected claim amount that an\\ninsured person will make (used to set insurance premiums), or the prediction\\nof future prices of securities. These kinds of predictions are also used for\\nalgorithmic trading.\\nTranscription: In this type of task, the machine learning system is asked\\n•\\nto observe a relatively unstructured representation of some kind of data and'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='transcribe it into discrete, textual form. For example, in optical character\\nrecognition, the computer program is shown a photograph containing an\\nimage of text and is asked to return this text in the form of a sequence\\nof characters (e.g., in ASCII or Unicode format). Google Street View uses\\ndeep learning to process address numbers in this way (Goodfellow et al.,\\n2014d). Another example is speech recognition, where the computer program\\nis provided an audio waveform and emits a sequence of characters or word\\nID codes describing the words that were spoken in the audio recording. Deep\\nlearning is a crucial component of modern speech recognition systems used\\nat major companies including Microsoft, IBM and Google (Hinton et al.,\\n2012b).\\nMachine translation: In a machine translation task, the input already\\n•\\nconsistsof asequence ofsymbols insome language, andthe computerprogram\\nmust convert this into a sequence of symbols in another language. This is'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='commonly applied to natural languages, such as translating from English to\\nFrench. Deep learning has recently begun to have an important impact on\\nthis kind of task (Sutskever et al., 2014; Bahdanau et al., 2015).\\nStructured output: Structured output tasks involve any task where the\\n•\\noutput is a vector (or other data structure containing multiple values) with\\nimportant relationships between the different elements. This is a broad\\ncategory, and subsumes the transcription and translation tasks described\\nabove, but also many other tasks. One example is parsing—mapping a\\nnatural language sentence into a tree that describes its grammatical structure\\nand tagging nodes of the trees as being verbs, nouns, or adverbs, and so on.\\nSee Collobert (2011) for an example of deep learning applied to a parsing\\ntask. Another example is pixel-wise segmentation of images, where the\\ncomputer program assigns every pixel in an image to a specific category. For\\n101\\nCHAPTER 5. MACHINE LEARNING BASICS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='example, deep learning can be used to annotate the locations of roads in\\naerial photographs (Mnih and Hinton, 2010). The output need not have its\\nform mirror the structure of the input as closely as in these annotation-style\\ntasks. For example, in image captioning, the computer program observes an\\nimage and outputs a natural language sentence describing the image (Kiros\\net al., 2014a,b; Mao et al., 2015; Vinyals et al., 2015b; Donahue et al., 2014;\\nKarpathy and Li, 2015; Fang et al., 2015; Xu et al., 2015). These tasks are\\ncalled structured output tasks because the program must output several\\nvalues that are all tightly inter-related. For example, the words produced by\\nan image captioning program must form a valid sentence.\\nAnomaly detection: In this type of task, the computer program sifts\\n•\\nthrough a set of events or objects, and flags some of them as being unusual\\nor atypical. An example of an anomaly detection task is credit card fraud'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='detection. By modeling your purchasing habits, a credit card company can\\ndetect misuse of your cards. If a thief steals your credit card or credit card\\ninformation, the thief’s purchases will often come from a different probability\\ndistribution over purchase types than your own. The credit card company\\ncan prevent fraud by placing a hold on an account as soon as that card has\\nbeen used for an uncharacteristic purchase. See Chandola et al. (2009) for a\\nsurvey of anomaly detection methods.\\nSynthesis and sampling: In this type of task, the machine learning al-\\n•\\ngorithm is asked to generate new examples that are similar to those in the\\ntraining data. Synthesis and sampling via machine learning can be useful\\nfor media applications where it can be expensive or boring for an artist to\\ngenerate large volumes of content by hand. For example, video games can\\nautomatically generate textures for large objects or landscapes, rather than'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='requiring an artist to manually label each pixel (Luo et al., 2013). In some\\ncases, we want the sampling or synthesis procedure to generate some specific\\nkind of output given the input. For example, in a speech synthesis task, we\\nprovide a written sentence and ask the program to emit an audio waveform\\ncontaining a spoken version of that sentence. This is a kind of structured\\noutput task, but with the added qualification that there is no single correct\\noutput for each input, and we explicitly desire a large amount of variation in\\nthe output, in order for the output to seem more natural and realistic.\\nImputation of missing values: In this type of task, the machine learning\\n•\\nalgorithm is given a new example x\\nRn,\\nbut with some entries x of x\\ni\\n∈\\nmissing. The algorithm must provide a prediction of the values of the missing\\nentries.\\n102\\nCHAPTER 5. MACHINE LEARNING BASICS\\nDenoising: In this type of task, the machine learning algorithm is given in\\n•\\ninputa corrupted example x˜\\nRn'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='obtainedbyan unknowncorruption process\\n∈\\nfrom a clean example x\\nRn.\\nThe learner must predict the clean example\\n∈\\nx from its corrupted version x˜, or more generally predict the conditional\\nprobability distribution p(x x˜).\\n|\\nDensity estimation or probability mass function estimation: In\\n•\\nthe density estimation problem, the machine learning algorithm is asked\\nto learn a function p :\\nRn R\\n, where p (x) can be interpreted\\nmodel model\\n→\\nas a probability density function (if x is continuous) or a probability mass\\nfunction (if x is discrete) on the space that the examples were drawn from.\\nTo do such a task well (we will specify exactly what that means when we\\ndiscuss performance measures P), the algorithm needs to learn the structure\\nof the data it has seen. It must know where examples cluster tightly and\\nwhere they are unlikely to occur. Most of the tasks described above require\\nthe learning algorithm to at least implicitly capture the structure of the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='probability distribution. Density estimation allows us to explicitly capture\\nthat distribution. In principle, we can then perform computations on that\\ndistribution in order to solve the other tasks as well. For example, if we\\nhave performed density estimation to obtain a probability distribution p(x),\\nwe can use that distribution to solve the missing value imputation task. If\\na value x is missing and all of the other values, denoted x , are given,\\ni i\\n−\\nthen we know the distribution over it is given by p(x x ). In practice,\\ni i\\n| −\\ndensity estimation does not always allow us to solve all of these related tasks,\\nbecause in many cases the required operations on p(x) are computationally\\nintractable.\\nOf course, many other tasks and types of tasks are possible. The types of tasks\\nwe list here are intended only to provide examples of what machine learning can\\ndo, not to define a rigid taxonomy of tasks.\\nP\\n5.1.2 The Performance Measure,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In order to evaluate the abilities of a machine learning algorithm, we must design\\na quantitative measure of its performance. Usually this performance measure P is\\nspecific to the task T being carried out by the system.\\nFor tasks such as classification, classification with missing inputs, and tran-\\nscription, we often measure the accuracy of the model. Accuracy is just the\\nproportion of examples for which the model produces the correct output. We can\\n103\\nCHAPTER 5. MACHINE LEARNING BASICS\\nalso obtain equivalent information by measuring the error rate, the proportion\\nof examples for which the model produces an incorrect output. We often refer to\\nthe error rate as the expected 0-1 loss. The 0-1 loss on a particular example is 0\\nif it is correctly classified and 1 if it is not. For tasks such as density estimation,\\nit does not make sense to measure accuracy, error rate, or any other kind of 0-1\\nloss. Instead, we must use a different performance metric that gives the model'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a continuous-valued score for each example. The most common approach is to\\nreport the average log-probability the model assigns to some examples.\\nUsually we are interested in how well the machine learning algorithm performs\\non data that it has not seen before, since this determines how well it will work when\\ndeployed in the real world. We therefore evaluate these performance measures using\\na test set of data that is separate from the data used for training the machine\\nlearning system.\\nThe choice of performance measure may seem straightforward and objective,\\nbut it is often difficult to choose a performance measure that corresponds well to\\nthe desired behavior of the system.\\nIn some cases, this is because it is difficult to decide what should be measured.\\nFor example, when performing a transcription task, should wemeasure the accuracy\\nof the system at transcribing entire sequences, or should we use a more fine-grained'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='performance measure that gives partial credit for getting some elements of the\\nsequence correct? When performing a regression task, should we penalize the\\nsystem more if it frequently makes medium-sized mistakes or if it rarely makes\\nvery large mistakes? These kinds of design choices depend on the application.\\nIn other cases, we know what quantity we would ideally like to measure, but\\nmeasuring it is impractical. For example, this arises frequently in the context of\\ndensity estimation. Many of the best probabilistic models represent probability\\ndistributions only implicitly. Computing the actual probability value assigned to\\na specific point in space in many such models is intractable. In these cases, one\\nmust design an alternative criterion that still corresponds to the design objectives,\\nor design a good approximation to the desired criterion.\\nE\\n5.1.3 The Experience,\\nMachine learning algorithms can be broadly categorized as unsupervised or'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='supervised by what kind of experience they are allowed to have during the\\nlearning process.\\nMost of the learning algorithms in this book can be understood as being allowed\\nto experience an entire dataset. A dataset is a collection of many examples, as\\n104\\nCHAPTER 5. MACHINE LEARNING BASICS\\ndefined in section 5.1.1. Sometimes we will also call examples data points.\\nOne of the oldest datasets studied by statisticians and machine learning re-\\nsearchers is the Iris dataset (Fisher, 1936). It is a collection of measurements of\\ndifferent parts of 150 iris plants. Each individual plant corresponds to one example.\\nThe features within each example are the measurements of each of the parts of the\\nplant: the sepal length, sepal width, petal length and petal width. The dataset\\nalso records which species each plant belonged to. Three different species are\\nrepresented in the dataset.\\nUnsupervised learning algorithms experience a dataset containing many'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='features, then learn useful properties of the structure of this dataset. In the context\\nof deep learning, we usually want to learn the entire probability distribution that\\ngenerated a dataset, whether explicitly as in density estimation or implicitly for\\ntasks like synthesis or denoising. Some other unsupervised learning algorithms\\nperform other roles, like clustering, which consists of dividing the dataset into\\nclusters of similar examples.\\nSupervised learning algorithms experience a dataset containing features,\\nbut each example is also associated with a label or target. For example, the Iris\\ndataset is annotated with the species of each iris plant. A supervised learning\\nalgorithm can study the Iris dataset and learn to classify iris plants into three\\ndifferent species based on their measurements.\\nRoughly speaking, unsupervised learning involves observing several examples\\nof a random vector x, and attempting to implicitly or explicitly learn the proba-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='bility distribution p(x), or some interesting properties of that distribution, while\\nsupervised learning involves observing several examples of a random vector x and\\nan associated value or vector y, and learning to predict y from x, usually by\\nestimating p(y x). The term supervised learning originates from the view of\\n|\\nthe target y being provided by an instructor or teacher who shows the machine\\nlearning system what to do. In unsupervised learning, there is no instructor or\\nteacher, and the algorithm must learn to make sense of the data without this guide.\\nUnsupervised learning and supervised learning are not formally defined terms.\\nThe lines between them are often blurred. Many machine learning technologies can\\nbe used to perform both tasks. For example, the chain rule of probability states\\nthat for a vector x\\nRn,\\nthe joint distribution can be decomposed as\\n∈\\nn\\np(x) = p(x x ,...,x ). (5.1)\\ni 1 i 1\\n| −\\ni=1\\n\\ue059'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='This decomposition means that we can solvethe ostensibly unsupervised problem of\\nmodeling p(x) by splitting it into n supervised learning problems. Alternatively, we\\n105\\nCHAPTER 5. MACHINE LEARNING BASICS\\ncan solve the supervised learning problem of learning p(y x) by using traditional\\n|\\nunsupervised learning technologies to learn the joint distribution p(x,y) and\\ninferring\\np(x,y)\\np(y x) = . (5.2)\\n| p(x,y )\\ny \\ue030\\n\\ue030\\nThough unsupervised learning and superv\\ue050ised learning are not completely formal or\\ndistinct concepts, they do help to roughly categorize some of the things we do with\\nmachine learning algorithms. Traditionally, people refer to regression, classification\\nand structured output problems as supervised learning. Density estimation in\\nsupport of other tasks is usually considered unsupervised learning.\\nOther variants of the learning paradigm are possible. For example, in semi-\\nsupervised learning, some examples include a supervision target but others do'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='not. In multi-instance learning, an entire collection of examples is labeled as\\ncontaining or not containing an example of a class, but the individual members\\nof the collection are not labeled. For a recent example of multi-instance learning\\nwith deep models, see Kotzias et al. (2015).\\nSome machine learning algorithms do not just experience a fixed dataset. For\\nexample, reinforcement learning algorithms interact with an environment, so\\nthere is a feedback loop between the learning system and its experiences. Such\\nalgorithms are beyond the scope of this book. Please see Sutton and Barto (1998)\\nor Bertsekas and Tsitsiklis (1996) for information about reinforcement learning,\\nand Mnih et al. (2013) for the deep learning approach to reinforcement learning.\\nMost machine learning algorithms simply experience a dataset. A dataset can\\nbe described in many ways. In all cases, a dataset is a collection of examples,\\nwhich are in turn collections of features.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='One common way of describing a dataset is with a design matrix. A design\\nmatrix is a matrix containing a different example in each row. Each column of the\\nmatrix corresponds to a different feature. For instance, the Iris dataset contains\\n150 examples with four features for each example. This means we can represent\\nthe dataset with a design matrix X R150 4, where X is the sepal length of\\n× i,1\\n∈\\nplant i, X is the sepal width of plant i, etc. We will describe most of the learning\\ni,2\\nalgorithms in this book in terms of how they operate on design matrix datasets.\\nOf course, to describe a dataset as a design matrix, it must be possible to\\ndescribe each example as a vector, and each of these vectors must be the same size.\\nThis is not always possible. For example, if you have a collection of photographs\\nwith different widths and heights, then different photographs will contain different\\nnumbers of pixels, so not all of the photographs may be described with the same'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='length of vector. Section 9.7 and chapter 10 describe how to handle different\\n106\\nCHAPTER 5. MACHINE LEARNING BASICS\\ntypes of such heterogeneous data. In cases like these, rather than describing the\\ndataset as a matrix with m rows, we will describe it as a set containing m elements:\\nx(1),x(2),...,x(m) . This notation does not imply that any two example vectors\\n{ }\\nx(i) and x(j) have the same size.\\nIn the case of supervised learning, the example contains a label or target as\\nwell as a collection of features. For example, if we want to use a learning algorithm\\nto perform object recognition from photographs, we need to specify which object\\nappears in each of the photos. We might do this with a numeric code, with 0\\nsignifying a person, 1 signifying a car, 2 signifying a cat, etc. Often when working\\nwith a dataset containing a design matrix of feature observations X, we also\\nprovide a vector of labels y, with y providing the label for example i.\\ni'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Of course, sometimes the label may be more than just a single number. For\\nexample, if we want to train a speech recognition system to transcribe entire\\nsentences, then the label for each example sentence is a sequence of words.\\nJust as there is no formal definition of supervised and unsupervised learning,\\nthere is no rigid taxonomy of datasets or experiences. The structures described here\\ncover most cases, but it is always possible to design new ones for new applications.\\n5.1.4 Example: Linear Regression\\nOur definition of a machine learning algorithm as an algorithm that is capable\\nof improving a computer program’s performance at some task via experience is\\nsomewhat abstract. To make this more concrete, we present an example of a\\nsimple machine learning algorithm: linear regression. We will return to this\\nexample repeatedly as we introduce more machine learning concepts that help to\\nunderstand its behavior.\\nAs the name implies, linear regression solves a regression problem. In other'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='words, the goal is to build a system that can take a vector x\\nRn\\nas input and\\n∈\\nR\\npredict the value of a scalar y as its output. In the case of linear regression,\\n∈\\nthe output is a linear function of the input. Let yˆ be the value that our model\\npredicts y should take on. We define the output to be\\nyˆ= w x (5.3)\\n\\ue03e\\nwhere w\\nRn\\nis a vector of parameters.\\n∈\\nParameters are values that control the behaviorof the system. In this case, w is\\ni\\nthe coefficient that we multiply by feature x before summing up the contributions\\ni\\nfrom all the features. We can think of w as a set of weights that determine how\\neach feature affects the prediction. If a feature x receives a positive weight w ,\\ni i\\n107\\nCHAPTER 5. MACHINE LEARNING BASICS\\nthen increasing the value of that feature increases the value of our prediction yˆ.\\nIf a feature receives a negative weight, then increasing the value of that feature\\ndecreases the value of our prediction. If a feature’s weight is large in magnitude,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='then it has a large effect on the prediction. If a feature’s weight is zero, it has no\\neffect on the prediction.\\nWe thus have a definition of our task T: to predict y from x by outputting\\nyˆ= w x. Next we need a definition of our performance measure, P.\\n\\ue03e\\nSuppose that we have a design matrix of m example inputs that we will not\\nuse for training, only for evaluating how well the model performs. We also have\\na vector of regression targets providing the correct value of y for each of these\\nexamples. Because this dataset will only be used for evaluation, we call it the test\\nset. We refer to the design matrix of inputs as X(test) and the vector of regression\\ntargets as y(test).\\nOne way of measuring the performance of the model is to compute the mean\\nsquared error of the model on the test set. If yˆ(test) gives the predictions of the\\nmodel on the test set, then the mean squared error is given by\\n1\\nMSE = (yˆ(test) y(test))2. (5.4)\\ntest i\\nm −\\ni\\n\\ue058'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Intuitively, one can see that this error measure decreases to 0 when yˆ(test) =y(test).\\nWe can also see that\\n1\\nMSE = ˆy(test) y(test) 2, (5.5)\\ntest m|| − ||2\\nso the error increases whenever the Euclidean distance between the predictions\\nand the targets increases.\\nTo make a machine learning algorithm, we need to design an algorithm that\\nwill improve the weights w in a way that reduces MSE when the algorithm\\ntest\\nis allowed to gain experience by observing a training set (X(train),y(train)). One\\nintuitive way of doing this (which we will justify later, in section 5.5.1) is just to\\nminimize the mean squared error on the training set, MSE .\\ntrain\\nTo minimize MSE , we can simply solve for where its gradient is 0:\\ntrain\\nMSE = 0 (5.6)\\nw train\\n∇\\n1\\nyˆ(train) y(train) 2 = 0 (5.7)\\nw 2\\n⇒ ∇ m|| − ||\\n1\\nX(train)w y(train) 2 = 0 (5.8)\\n⇒\\nm∇w\\n|| −\\n||2\\n108\\nCHAPTER 5. MACHINE LEARNING BASICS\\n3\\n2\\n1\\n0\\n1\\n−\\n2\\n−\\n3\\n− 1.0 0.5 0.0 0.5 1.0\\n− −\\nx\\n1\\ny\\nLinear regression example\\n0.55\\n0.50\\n0.45\\n0.40\\n0.35\\n0.30\\n0.25\\n0.20'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='0.5 1.0 1.5\\nw\\n1\\n)niart(ESM\\nOptimization of w\\nFigure 5.1: A linear regression problem, with a training set consisting of ten data points,\\neach containing one feature. Because there is only one feature, the weight vector w\\ncontains only a single parameter to learn, w . (Left)Observe that linear regression learns\\n1\\nto set w such that the line y =w x comes as close as possible to passing through all the\\n1 1\\ntraining points. (Right)The plotted point indicates the value of w found by the normal\\n1\\nequations, which we can see minimizes the mean squared error on the training set.\\nX(train)w y(train) \\ue03e X(train)w y(train) = 0 (5.9)\\nw\\n⇒ ∇ − −\\n\\ue010 \\ue011 \\ue010 \\ue011\\nw X(train) X(train)w 2w X(train) y(train)+ y(train) y(train) = 0\\nw \\ue03e \\ue03e \\ue03e \\ue03e \\ue03e\\n⇒ ∇ −\\n\\ue010 \\ue011(5.10)\\n2X(train) X(train)w 2X(train) y(train) = 0 (5.11)\\n\\ue03e \\ue03e\\n⇒ −\\n1\\nw = X(train) X(train) − X(train) y(train) (5.12)\\n\\ue03e \\ue03e\\n⇒\\n\\ue010 \\ue011\\nThe system of equations whose solution is given by equation 5.12 is known as'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the normal equations. Evaluating equation 5.12 constitutes a simple learning\\nalgorithm. For an example of the linear regression learning algorithm in action,\\nsee figure 5.1.\\nIt is worth noting that the term linear regression is often used to refer to\\na slightly more sophisticated model with one additional parameter—an intercept\\nterm b. In this model\\nyˆ= w x + b (5.13)\\n\\ue03e\\nso the mapping from parameters to predictions is still a linear function but the\\nmapping from features to predictions is now an affine function. This extension to\\naffine functions means that the plot of the model’s predictions still looks like a\\nline, but it need not pass through the origin. Instead of adding the bias parameter\\n109\\nCHAPTER 5. MACHINE LEARNING BASICS\\nb, one can continue to use the model with only weights but augment x with an\\nextra entry that is always set to 1. The weight corresponding to the extra 1 entry\\nplays the role of the bias parameter. We will frequently use the term “linear” when'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='referring to affine functions throughout this book.\\nThe intercept term b is often called the bias parameter of the affine transfor-\\nmation. This terminology derives from the point of view that the output of the\\ntransformation is biased toward being b in the absence of any input. This term\\nis different from the idea of a statistical bias, in which a statistical estimation\\nalgorithm’s expected estimate of a quantity is not equal to the true quantity.\\nLinearregressionis ofcourse anextremelysimple andlimitedlearning algorithm,\\nbut it provides an example of how a learning algorithm can work. In the subsequent\\nsections we will describe some of the basic principles underlying learning algorithm\\ndesign and demonstrate how these principles can be used to build more complicated\\nlearning algorithms.\\n5.2 Capacity, Overfitting and Underfitting\\nThe central challenge in machine learning is that we must perform well on new,\\npreviously unseen inputs—not just those on which our model was trained. The'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='ability to perform well on previously unobserved inputs is called generalization.\\nTypically, when training a machine learning model, we have access to a training\\nset, we can compute some error measure on the training set called the training\\nerror, and we reduce this training error. So far, what we have described is simply\\nan optimization problem. What separates machine learning from optimization is\\nthat we want the generalization error, also called the test error, to be low as\\nwell. The generalization error is defined as the expected value of the error on a\\nnew input. Here the expectation is taken across different possible inputs, drawn\\nfrom the distribution of inputs we expect the system to encounter in practice.\\nWe typically estimate the generalization error of a machine learning model by\\nmeasuring its performance on a test setof examples that were collected separately\\nfrom the training set.\\nIn our linear regression example, we trained the model by minimizing the\\ntraining error,\\n1'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='X(train)w y(train) 2, (5.14)\\nm(train)|| − ||2\\nbut we actually care about the test error, 1 X(test)w y(test) 2.\\nm(test) || − ||2\\nHow can we affect performance on the test set when we get to observe only the\\n110\\nCHAPTER 5. MACHINE LEARNING BASICS\\ntraining set? The field of statistical learning theory provides some answers. If\\nthe training and the test set are collected arbitrarily, there is indeed little we can\\ndo. If we are allowed to make some assumptions about how the training and test\\nset are collected, then we can make some progress.\\nThe train and test data are generated bya probability distribution over datasets\\ncalled the data generating process. We typically make a set of assumptions\\nknown collectively as the i.i.d. assumptions. These assumptions are that the\\nexamples in each dataset are independent from each other, and that the train\\nset and test set are identically distributed, drawn from the same probability'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='distribution as each other. This assumption allows us to describe the data gen-\\nerating process with a probability distribution over a single example. The same\\ndistribution is then used to generate every train example and every test example.\\nWe call that shared underlying distribution the data generating distribution,\\ndenoted p . This probabilistic framework and the i.i.d. assumptions allow us to\\ndata\\nmathematically study the relationship between training error and test error.\\nOne immediate connection we can observe between the training and test error\\nis that the expected training error of a randomly selected model is equal to the\\nexpected test error of that model. Suppose we have a probability distribution\\np(x,y) and we sample from it repeatedly to generate the train set and the test\\nset. For some fixed value w, the expected training set error is exactly the same as\\nthe expected test set error, because both expectations are formed using the same'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='dataset sampling process. The only difference between the two conditions is the\\nname we assign to the dataset we sample.\\nOf course, when we use a machine learning algorithm, we do not fix the\\nparameters ahead of time, then sample both datasets. We sample the training set,\\nthen use it to choose the parameters to reduce training set error, then sample the\\ntest set. Under this process, the expected test error is greater than or equal to\\nthe expected value of training error. The factors determining how well a machine\\nlearning algorithm will perform are its ability to:\\n1. Make the training error small.\\n2. Make the gap between training and test error small.\\nThese two factors correspond to the two central challenges in machine learning:\\nunderfitting and overfitting. Underfitting occurs when the model is not able to\\nobtain a sufficiently low error value on the training set. Overfitting occurs when\\nthe gap between the training error and test error is too large.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='We can control whether a model is more likely to overfit or underfit by altering\\nits capacity. Informally, a model’s capacity is its ability to fit a wide variety of\\n111\\nCHAPTER 5. MACHINE LEARNING BASICS\\nfunctions. Models with low capacity may struggle to fit the training set. Models\\nwith high capacity can overfit by memorizing properties of the training set that do\\nnot serve them well on the test set.\\nOne way to control the capacity of a learning algorithm is by choosing its\\nhypothesis space, the set of functions that the learning algorithm is allowed to\\nselect as being the solution. For example, the linear regression algorithm has the\\nset of all linear functions of its input as its hypothesis space. We can generalize\\nlinear regression to include polynomials, rather than just linear functions, in its\\nhypothesis space. Doing so increases the model’s capacity.\\nA polynomial of degree one gives us the linear regression model with which we\\nare already familiar, with prediction'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='yˆ= b + wx. (5.15)\\nBy introducing x2 as another feature provided to the linear regression model, we\\ncan learn a model that is quadratic as a function of x:\\nyˆ= b + w x + w x2. (5.16)\\n1 2\\nThough this model implements a quadratic function of its input, the output is\\nstill a linear function of the parameters, so we can still use the normal equations\\nto train the model in closed form. We can continue to add more powers of x as\\nadditional features, for example to obtain a polynomial of degree 9:\\n9\\nyˆ= b + w xi. (5.17)\\ni\\ni=1\\n\\ue058\\nMachine learning algorithms will generally perform best when their capacity\\nis appropriate for the true complexity of the task they need to perform and the\\namount of training data they are provided with. Models with insufficient capacity\\nare unable to solve complex tasks. Models with high capacity can solve complex\\ntasks, but when their capacity is higher than needed to solve the present task they\\nmay overfit.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Figure 5.2 shows this principle in action. We compare a linear, quadratic\\nand degree-9 predictor attempting to fit a problem where the true underlying\\nfunction is quadratic. The linear function is unable to capture the curvature in\\nthe true underlying problem, so it underfits. The degree-9 predictor is capable of\\nrepresenting the correct function, but it is also capable of representing infinitely\\nmany other functions that pass exactly through the training points, because we\\n112\\nCHAPTER 5. MACHINE LEARNING BASICS\\nhave more parameters than training examples. We have little chance of choosing\\na solution that generalizes well when so many wildly different solutions exist. In\\nthis example, the quadratic model is perfectly matched to the true structure of\\nthe task so it generalizes well to new data.\\n\\ue078\\n\\ue030\\n\\ue079\\n\\ue055\\ue06e\\ue064\\ue065\\ue072\\ue066\\ue069\\ue074\\ue074\\ue069\\ue06e\\ue067\\n\\ue078\\n\\ue030\\n\\ue079\\n\\ue041\\ue070\\ue070\\ue072\\ue06f\\ue070\\ue072\\ue069\\ue061\\ue074\\ue065\\ue020\\ue063\\ue061\\ue070\\ue061\\ue063\\ue069\\ue074\\ue079\\n\\ue078\\n\\ue030\\n\\ue079\\n\\ue04f\\ue076\\ue065\\ue072\\ue066\\ue069\\ue074\\ue074\\ue069\\ue06e\\ue067\\nFigure 5.2: We fit three models to this example training set. The training data was'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='generated synthetically, by randomly sampling x values and choosing y deterministically\\nby evaluating a quadratic function. (Left)A linear function fit to the data suffers from\\nunderfitting—it cannot capture the curvature that is present in the data. (Center)A\\nquadratic function fit to the data generalizes well to unseen points. It does not suffer from\\na significant amount of overfitting or underfitting. (Right)A polynomial of degree 9 fit to\\nthe data suffers from overfitting. Here we used the Moore-Penrose pseudoinverse to solve\\nthe underdetermined normal equations. The solution passes through all of the training\\npoints exactly, but we have not been lucky enough for it to extract the correct structure.\\nIt now has a deep valley in between two training points that does not appear in the true\\nunderlying function. It also increases sharply on the left side of the data, while the true\\nfunction decreases in this area.\\nSo far we have described only one way of changing a model’s capacity: by'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='changing the number of input features it has, and simultaneously adding new\\nparameters associated with those features. There are in fact many ways of changing\\na model’s capacity. Capacity is not determined only by the choice of model. The\\nmodel specifies which family of functions the learning algorithm can choose from\\nwhen varying the parameters in order to reduce a training objective. This is called\\nthe representational capacity of the model. In many cases, finding the best\\nfunction within this family is a very difficult optimization problem. In practice,\\nthe learning algorithm does not actually find the best function, but merely one\\nthat significantly reduces the training error. These additional limitations, such as\\n113\\nCHAPTER 5. MACHINE LEARNING BASICS\\nthe imperfection of the optimization algorithm, mean that the learning algorithm’s\\neffective capacity may be less than the representational capacity of the model\\nfamily.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Our modern ideas about improving the generalization of machine learning\\nmodels are refinements of thought dating back to philosophers at least as early\\nas Ptolemy. Many early scholars invoke a principle of parsimony that is now\\nmost widely known as Occam’s razor (c. 1287-1347). This principle states that\\namong competing hypotheses that explain known observations equally well, one\\nshould choose the “simplest” one. This idea was formalized and made more precise\\nin the 20th century by the founders of statistical learning theory (Vapnik and\\nChervonenkis, 1971; Vapnik, 1982; Blumer et al., 1989; Vapnik, 1995).\\nStatistical learning theory provides various means of quantifying model capacity.\\nAmong these, the most well-known is the Vapnik-Chervonenkis dimension, or\\nVC dimension. The VC dimension measures the capacity of a binary classifier. The\\nVC dimension is defined as being the largest possible value of m for which there'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='exists a training set of m different x points that the classifier can label arbitrarily.\\nQuantifying the capacity of the model allows statistical learning theory to\\nmake quantitative predictions. The most important results in statistical learning\\ntheory show that the discrepancy between training error and generalization error\\nis bounded from above by a quantity that grows as the model capacity grows but\\nshrinks as the number of training examples increases (Vapnik and Chervonenkis,\\n1971; Vapnik, 1982; Blumer et al., 1989; Vapnik, 1995). These bounds provide\\nintellectual justification that machine learning algorithms can work, but they are\\nrarely used in practice when working with deep learning algorithms. This is in\\npart because the bounds are often quite loose and in part because it can be quite\\ndifficult to determine the capacity of deep learning algorithms. The problem of\\ndetermining the capacity of a deep learning model is especially difficult because the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='effective capacity is limited by the capabilities of the optimization algorithm, and\\nwe have little theoretical understanding of the very general non-convex optimization\\nproblems involved in deep learning.\\nWe must remember that while simpler functions are more likely to generalize\\n(to have a small gap between training and test error) we must still choose a\\nsufficiently complex hypothesis to achieve low training error. Typically, training\\nerror decreases until it asymptotes to the minimum possible error value as model\\ncapacity increases (assuming the error measure has a minimum value). Typically,\\ngeneralization error has a U-shaped curve as a function of model capacity. This is\\nillustrated in figure 5.3.\\nTo reach the most extreme case of arbitrarily high capacity, we introduce\\n114\\nCHAPTER 5. MACHINE LEARNING BASICS\\n0 Optimal Capacity\\nCapacity\\nrorrE\\nTraining error\\nUnderfitting zone Overfitting zone\\nGeneralization error\\nGeneralization gap'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Figure 5.3: Typical relationship between capacity and error. Training and test error\\nbehave differently. At the left end of the graph, training error and generalization error\\nare both high. This is the underfitting regime. As we increase capacity, training error\\ndecreases, but the gap between training and generalization error increases. Eventually,\\nthe size of this gap outweighs the decrease in training error, and we enter the overfitting\\nregime, where capacity is too large, above the optimal capacity.\\nthe concept of non-parametric models. So far, we have seen only parametric\\nmodels, such as linear regression. Parametric models learn a function described\\nby a parameter vector whose size is finite and fixed before any data is observed.\\nNon-parametric models have no such limitation.\\nSometimes, non-parametric models are just theoretical abstractions (such as\\nan algorithm that searches over all possible probability distributions) that cannot'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='be implemented in practice. However, we can also design practical non-parametric\\nmodels by making their complexity a function of the training set size. One example\\nof such an algorithm is nearest neighbor regression. Unlike linear regression,\\nwhich has a fixed-length vector of weights, the nearest neighbor regression model\\nsimply stores the X and y from the training set. When asked to classify a test\\npoint x, the model looks up the nearest entry in the training set and returns the\\nassociated regression target. In other words, yˆ = y wherei =argmin X x 2.\\ni i,: 2\\n|| − ||\\nThe algorithm can also be generalized to distance metrics other than the L2 norm,\\nsuch as learned distance metrics (Goldberger et al., 2005). If the algorithm is\\nallowed to break ties by averaging the y values for all X that are tied for nearest,\\ni i,:\\nthen this algorithm is able to achieve the minimum possible training error (which\\nmight be greater than zero, if two identical inputs are associated with different'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='outputs) on any regression dataset.\\nFinally, we can also create a non-parametric learning algorithm by wrapping a\\n115\\nCHAPTER 5. MACHINE LEARNING BASICS\\nparametric learning algorithm inside another algorithm that increases the number\\nof parameters as needed. For example, we could imagine an outer loop of learning\\nthat changes the degree of the polynomial learned by linear regression on top of a\\npolynomial expansion of the input.\\nThe ideal model is an oracle that simply knows the true probability distribution\\nthat generates the data. Even such a model will still incur some error on many\\nproblems, because there may still be some noise in the distribution. In the case\\nof supervised learning, the mapping from x to y may be inherently stochastic,\\nor y may be a deterministic function that involves other variables besides those\\nincluded in x. The error incurred by an oracle making predictions from the true\\ndistribution p(x,y) is called the Bayes error.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Training and generalization error vary as the size of the training set varies.\\nExpected generalization error can neverincrease as the number of training examples\\nincreases. For non-parametric models, more data yields better generalization until\\nthe best possible error is achieved. Any fixed parametric model with less than\\noptimal capacity will asymptote to an error value that exceeds the Bayes error. See\\nfigure 5.4 for an illustration. Note that it is possible for the model to have optimal\\ncapacity and yet still have a large gap between training and generalization error.\\nIn this situation, we may be able to reduce this gap by gathering more training\\nexamples.\\n5.2.1 The No Free Lunch Theorem\\nLearning theory claims that a machine learning algorithm can generalize well from\\na finite training set of examples. This seems to contradict some basic principles of\\nlogic. Inductive reasoning, or inferring general rules from a limited set of examples,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='is not logically valid. To logically infer a rule describing every member of a set,\\none must have information about every member of that set.\\nIn part, machinelearning avoids this problembyoffering only probabilistic rules,\\nrather than the entirely certain rules used in purely logical reasoning. Machine\\nlearning promises to find rules that are probably correct about most members of\\nthe set they concern.\\nUnfortunately, even this does not resolve the entire problem. The no free\\nlunch theorem for machine learning (Wolpert, 1996) states that, averaged over\\nall possible data generating distributions, every classification algorithm has the\\nsame error rate when classifying previously unobserved points. In other words,\\nin some sense, no machine learning algorithm is universally any better than any\\nother. The most sophisticated algorithm we can conceive of has the same average\\n116\\nCHAPTER 5. MACHINE LEARNING BASICS\\n\\ue033\\ue02e\\ue035\\n\\ue033\\ue02e\\ue030\\n\\ue032\\ue02e\\ue035\\n\\ue032\\ue02e\\ue030\\n\\ue031\\ue02e\\ue035\\n\\ue031\\ue02e\\ue030\\n\\ue030\\ue02e\\ue035\\n\\ue030\\ue02e\\ue030\\n\\ue030 \\ue031 \\ue032 \\ue033 \\ue034 \\ue035\\n\\ue031\\ue030 \\ue031\\ue030 \\ue031\\ue030 \\ue031\\ue030 \\ue031\\ue030 \\ue031\\ue030'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='\\ue04e\\ue075\\ue06d\\ue062\\ue065\\ue072\\ue020\\ue06f\\ue066\\ue020\\ue074\\ue072\\ue061\\ue069\\ue06e\\ue069\\ue06e\\ue067\\ue020\\ue065\\ue078\\ue061\\ue06d\\ue070\\ue06c\\ue065\\ue073\\n\\ue029\\ue045\\ue053\\ue04d\\ue028\\ue020\\ue072\\ue06f\\ue072\\ue072\\ue045\\n\\ue042\\ue061\\ue079\\ue065\\ue073\\ue020\\ue065\\ue072\\ue072\\ue06f\\ue072\\n\\ue054\\ue072\\ue061\\ue069\\ue06e\\ue020\\ue028\\ue071\\ue075\\ue061\\ue064\\ue072\\ue061\\ue074\\ue069\\ue063\\ue029\\n\\ue054\\ue065\\ue073\\ue074\\ue020\\ue028\\ue071\\ue075\\ue061\\ue064\\ue072\\ue061\\ue074\\ue069\\ue063\\ue029\\n\\ue054\\ue065\\ue073\\ue074\\ue020\\ue028\\ue06f\\ue070\\ue074\\ue069\\ue06d\\ue061\\ue06c\\ue020\\ue063\\ue061\\ue070\\ue061\\ue063\\ue069\\ue074\\ue079\\ue029\\n\\ue054\\ue072\\ue061\\ue069\\ue06e\\ue020\\ue028\\ue06f\\ue070\\ue074\\ue069\\ue06d\\ue061\\ue06c\\ue020\\ue063\\ue061\\ue070\\ue061\\ue063\\ue069\\ue074\\ue079\\ue029\\n\\ue032\\ue030\\n\\ue031\\ue035\\n\\ue031\\ue030\\n\\ue035\\n\\ue030\\n\\ue030 \\ue031 \\ue032 \\ue033 \\ue034 \\ue035\\n\\ue031\\ue030 \\ue031\\ue030 \\ue031\\ue030 \\ue031\\ue030 \\ue031\\ue030 \\ue031\\ue030\\n\\ue04e\\ue075\\ue06d\\ue062\\ue065\\ue072\\ue020\\ue06f\\ue066\\ue020\\ue074\\ue072\\ue061\\ue069\\ue06e\\ue069\\ue06e\\ue067\\ue020\\ue065\\ue078\\ue061\\ue06d\\ue070\\ue06c\\ue065\\ue073\\n\\ue029\\ue065\\ue065\\ue072\\ue067\\ue065\\ue064\\ue020\\ue06c\\ue061\\ue069\\ue06d\\ue06f\\ue06e\\ue079\\ue06c\\ue06f\\ue070\\ue028\\ue020\\ue079\\ue074\\ue069\\ue063\\ue061\\ue070\\ue061\\ue063\\ue020\\ue06c\\ue061\\ue06d\\ue069\\ue074\\ue070\\ue04f\\nFigure 5.4: The effect of the training dataset size on the train and test error, as well as\\non the optimal model capacity. We constructed a synthetic regression problem based on\\nadding a moderate amount of noise to a degree-5 polynomial, generated a single test set,\\nand then generated several different sizes of training set. For each size, we generated 40\\ndifferent training sets in order to plot error bars showing 95 percent confidence intervals.\\n(Top)The MSE on the training and test set for two different models: a quadratic model,\\nand a model with degree chosen to minimize the test error. Both are fit in closed form. For\\nthe quadratic model, the training error increases as the size of the training set increases.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='This is because larger datasets are harder to fit. Simultaneously, the test error decreases,\\nbecause fewer incorrect hypotheses are consistent with the training data. The quadratic\\nmodel does not have enough capacity to solve the task, so its test error asymptotes to\\na high value. The test error at optimal capacity asymptotes to the Bayes error. The\\ntraining error can fall below the Bayes error, due to the ability of the training algorithm\\nto memorize specific instances of the training set. As the training size increases to infinity,\\nthe training error of any fixed-capacity model (here, the quadratic model) must rise to at\\nleast the Bayes error. (Bottom)As the training set size increases, the optimal capacity\\n(shown here as the degree of the optimal polynomial regressor) increases. The optimal\\ncapacity plateaus after reaching sufficient complexity to solve the task.\\n117\\nCHAPTER 5. MACHINE LEARNING BASICS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='performance (over all possible tasks) as merely predicting that every point belongs\\nto the same class.\\nFortunately, these results hold only when we average over all possible data\\ngenerating distributions. If we make assumptions about the kinds of probability\\ndistributions we encounter in real-world applications, then we can design learning\\nalgorithms that perform well on these distributions.\\nThis means that the goal of machine learning research is not to seek a universal\\nlearning algorithm or the absolute best learning algorithm. Instead, our goal is to\\nunderstand what kinds of distributions are relevant to the “real world” that an AI\\nagent experiences, and what kinds of machine learning algorithms perform well on\\ndata drawn from the kinds of data generating distributions we care about.\\n5.2.2 Regularization\\nThe no free lunch theorem implies that we must design our machine learning\\nalgorithms to perform well on a specific task. We do so by building a set of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='preferences into the learning algorithm. When these preferences are aligned with\\nthe learning problems we ask the algorithm to solve, it performs better.\\nSofar, theonly methodof modifyingalearning algorithmthatwehavediscussed\\nconcretely is to increase or decrease the model’s representationalcapacity by adding\\nor removing functions from the hypothesis space of solutions the learning algorithm\\nis able to choose. We gave the specific example of increasing or decreasing the\\ndegree of a polynomial for a regression problem. The view we have described so\\nfar is oversimplified.\\nThe behavior of our algorithm is strongly affected not just by how large we\\nmake the set of functions allowed in its hypothesis space, but by the specific identity\\nof those functions. The learning algorithm we have studied so far, linear regression,\\nhas a hypothesis space consisting of the set of linear functions of its input. These\\nlinear functions can be very useful for problems where the relationship between'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='inputs and outputs truly is close to linear. They are less useful for problems\\nthat behave in a very nonlinear fashion. For example, linear regression would\\nnot perform very well if we tried to use it to predict sin(x) from x. We can thus\\ncontrol the performance of our algorithms by choosing what kind of functions we\\nallow them to draw solutions from, as well as by controlling the amount of these\\nfunctions.\\nWe can also give a learning algorithm a preference for one solution in its\\nhypothesis space to another. This means that both functions are eligible, but one\\nis preferred. The unpreferred solution will be chosen only if it fits the training\\n118\\nCHAPTER 5. MACHINE LEARNING BASICS\\ndata significantly better than the preferred solution.\\nFor example, we can modify the training criterion for linear regression to include\\nweight decay. To perform linear regression with weight decay, we minimize a sum\\ncomprising both the mean squared error on the training and a criterion J(w) that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='expresses a preference for the weights to have smaller squaredL2 norm. Specifically,\\nJ(w) = MSE + λw w, (5.18)\\ntrain \\ue03e\\nwhere λ is a value chosen ahead of time that controls the strength of our preference\\nfor smaller weights. When λ = 0, we impose no preference, and larger λ forces the\\nweights to become smaller. Minimizing J(w) results in a choice of weights that\\nmake a tradeoff between fitting the training data and being small. This gives us\\nsolutions that have a smaller slope, or put weight on fewer of the features. As an\\nexample of how we can control a model’s tendency to overfit or underfit via weight\\ndecay, we can train a high-degree polynomial regression model with different values\\nof λ. See figure 5.5 for the results.\\n\\ue078\\n\\ue030\\n\\ue079\\n\\ue055\\ue06e\\ue064\\ue065\\ue072\\ue066\\ue069\\ue074\\ue074\\ue069\\ue06e\\ue067\\n\\ue028\\ue045\\ue078\\ue063\\ue065\\ue073\\ue073\\ue069\\ue076\\ue065\\ue020\\ue0b8\\ue029\\n\\ue078\\n\\ue030\\n\\ue079\\n\\ue041\\ue070\\ue070\\ue072\\ue06f\\ue070\\ue072\\ue069\\ue061\\ue074\\ue065\\ue020\\ue077\\ue065\\ue069\\ue067\\ue068\\ue074\\ue020\\ue064\\ue065\\ue063\\ue061\\ue079\\n\\ue028\\ue04d\\ue065\\ue064\\ue069\\ue075\\ue06d\\ue020\\ue0b8\\ue029\\n\\ue078\\n\\ue030\\n\\ue079\\n\\ue04f\\ue076\\ue065\\ue072\\ue066\\ue069\\ue074\\ue074\\ue069\\ue06e\\ue067\\n\\ue028\\ue0b8 \\ue030\\ue029\\n\\ue021\\nFigure 5.5: We fit a high-degree polynomial regression model to our example training set'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='from figure 5.2. The true function is quadratic, but here we use only models with degree 9.\\nWe vary the amount of weight decay to prevent these high-degree models from overfitting.\\n(Left)With very large λ, we can force the model to learn a function with no slope at\\nall. This underfits because it can only represent a constant function. (Center)With a\\nmedium value of λ, the learning algorithm recovers a curve with the right general shape.\\nEven though the model is capable of representing functions with much more complicated\\nshape, weight decay has encouraged it to use a simpler function described by smaller\\ncoefficients. (Right)With weight decay approaching zero (i.e., using the Moore-Penrose\\npseudoinverse to solve the underdetermined problem with minimal regularization), the\\ndegree-9 polynomial overfits significantly, as we saw in figure 5.2.\\n119\\nCHAPTER 5. MACHINE LEARNING BASICS\\nMore generally, we can regularize a model that learns a function f(x;θ) by'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='adding a penalty called a regularizer to the cost function. In the case of weight\\ndecay, the regularizer is Ω(w) =w w. In chapter 7, we will see that many other\\n\\ue03e\\nregularizers are possible.\\nExpressing preferences for one function over another is a more general way\\nof controlling a model’s capacity than including or excluding members from the\\nhypothesis space. We can think of excluding a function from a hypothesis space as\\nexpressing an infinitely strong preference against that function.\\nIn our weight decay example, we expressed our preference for linear functions\\ndefined with smaller weights explicitly, via an extra term in the criterion we\\nminimize. There are many other ways of expressing preferences for different\\nsolutions, both implicitly and explicitly. Together, these different approaches\\nare known as regularization. Regularization is any modification we make to a\\nlearning algorithm that is intended to reduce its generalization error but not its'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='training error. Regularization is one of the central concerns of the field of machine\\nlearning, rivaled in its importance only by optimization.\\nThe no free lunch theorem has made it clear that there is no best machine\\nlearning algorithm, and, in particular, no best form of regularization. Instead\\nwe must choose a form of regularization that is well-suited to the particular task\\nwe want to solve. The philosophy of deep learning in general and this book in\\nparticular is that a very wide range of tasks (such as all of the intellectual tasks\\nthat people can do) may all be solved effectively using very general-purpose forms\\nof regularization.\\n5.3 Hyperparameters and Validation Sets\\nMost machine learning algorithms have several settings that we can use to control\\nthe behavior of the learning algorithm. These settings are called hyperparame-\\nters. The values of hyperparameters are not adapted by the learning algorithm'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='itself (though we can design a nested learning procedure where one learning\\nalgorithm learns the best hyperparameters for another learning algorithm).\\nIn the polynomial regression example we saw in figure 5.2, there is a single\\nhyperparameter: the degree of the polynomial, which acts as a capacity hyper-\\nparameter. The λ value used to control the strength of weight decay is another\\nexample of a hyperparameter.\\nSometimes a setting is chosen to be a hyperparameter that the learning al-\\ngorithm does not learn because it is difficult to optimize. More frequently, the\\n120\\nCHAPTER 5. MACHINE LEARNING BASICS\\nsetting must be a hyperparameter because it is not appropriate to learn that\\nhyperparameter on the training set. This applies to all hyperparameters that\\ncontrol model capacity. If learned on the training set, such hyperparameters would\\nalways choose the maximum possible model capacity, resulting in overfitting (refer'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to figure 5.3). For example, we can always fit the training set better with a higher\\ndegree polynomial and a weight decay setting of λ = 0 than we could with a lower\\ndegree polynomial and a positive weight decay setting.\\nTo solve this problem, we need a validation set of examples that the training\\nalgorithm does not observe.\\nEarlier we discussed how a held-out test set, composed of examples coming from\\nthe same distribution as the training set, can be used to estimate the generalization\\nerror of a learner, after the learning process has completed. It is important that the\\ntest examples are not used in any way to make choices about the model, including\\nits hyperparameters. For this reason, no example from the test set can be used\\nin the validation set. Therefore, we always construct the validation set from the\\ntraining data. Specifically, we split the training data into two disjoint subsets. One\\nof these subsets is used to learn the parameters. The other subset is our validation'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='set, used to estimate the generalization error during or after training, allowing\\nfor the hyperparameters to be updated accordingly. The subset of data used to\\nlearn the parameters is still typically called the training set, even though this\\nmay be confused with the larger pool of data used for the entire training process.\\nThe subset of data used to guide the selection of hyperparameters is called the\\nvalidation set. Typically, one uses about 80% of the training data for training and\\n20% for validation. Since the validation set is used to “train” the hyperparameters,\\nthe validation set error will underestimate the generalization error, though typically\\nby a smaller amount than the training error. After all hyperparameter optimization\\nis complete, the generalization error may be estimated using the test set.\\nIn practice, when the same test set has been used repeatedly to evaluate\\nperformance of different algorithms over many years, and especially if we consider'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='all the attempts from the scientific community at beating the reported state-of-\\nthe-art performance on that test set, we end up having optimistic evaluations with\\nthe test set as well. Benchmarks can thus become stale and then do not reflect the\\ntrue field performance of a trained system. Thankfully, the community tends to\\nmove on to new (and usually more ambitious and larger) benchmark datasets.\\n121\\nCHAPTER 5. MACHINE LEARNING BASICS\\n5.3.1 Cross-Validation\\nDividing the dataset into a fixed training set and a fixed test set can be problematic\\nif itresults in the test setbeing small. A small test set impliesstatistical uncertainty\\naround the estimated average test error, making it difficult to claim that algorithm\\nA works better than algorithm B on the given task.\\nWhen the dataset has hundreds of thousands of examples or more, this is not a\\nserious issue. When the dataset is too small, are alternative procedures enable one'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to use all of the examples in the estimation of the mean test error, at the price of\\nincreased computational cost. These procedures are based on the idea of repeating\\nthe training and testing computation on different randomly chosen subsets or splits\\nof the original dataset. The most common of these is the k-fold cross-validation\\nprocedure, shown in algorithm 5.1, in which a partition of the dataset is formed by\\nsplitting it into k non-overlapping subsets. The test error may then be estimated\\nby taking the average test error across k trials. On trial i, the i-th subset of the\\ndata is used as the test set and the rest of the data is used as the training set. One\\nproblem is that there exist no unbiased estimators of the variance of such average\\nerror estimators (Bengio and Grandvalet, 2004), but approximations are typically\\nused.\\n5.4 Estimators, Bias and Variance\\nThe field of statistics gives us many tools that can be used to achieve the machine'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='learning goal of solving a task not only on the training set but also to generalize.\\nFoundational concepts such as parameter estimation, bias and variance are useful\\nto formally characterize notions of generalization, underfitting and overfitting.\\n5.4.1 Point Estimation\\nPoint estimation is the attempt to provide the single “best” prediction of some\\nquantity of interest. In general the quantity of interest can be a single parameter\\nor a vector of parameters in some parametric model, such as the weights in our\\nlinear regression example in section 5.1.4, but it can also be a whole function.\\nIn order to distinguish estimates of parameters from their true value, our\\nˆ\\nconvention will be to denote a point estimate of a parameter θ by θ.\\nLet x(1),...,x(m) be a set of m independent and identically distributed\\n{ }\\n122\\nCHAPTER 5. MACHINE LEARNING BASICS\\nAlgorithm 5.1 The k-fold cross-validation algorithm. It can be used to estimate\\nD'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='generalization error of a learning algorithm A when the given dataset is too\\nsmall for a simple train/test or train/valid split to yield accurate estimation of\\ngeneralization error, because the mean of a loss L on a small test set may have too\\nhigh variance. The dataset D contains as elements the abstract examples z(i) (for\\nthe i-th example), which could stand for an (input,target) pair z(i) = (x(i),y(i))\\nin the case of supervised learning, or for just an input z(i) = x(i) in the case\\nof unsupervised learning. The algorithm returns the vector of errors e for each\\nD\\nexample in , whose mean is the estimated generalization error. The errors on\\nindividual examples can be used to compute a confidence interval around the mean\\n(equation 5.47). While these confidence intervals are not well-justified after the\\nuse of cross-validation, it is still common practice to use them to declare that\\nalgorithm A is better than algorithm B only if the confidence interval of the error'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of algorithm A lies below and does not intersect the confidence interval of algorithm\\nB.\\nDefine\\nKFoldXV(D\\n,A,L,k):\\nRequire: D , the given dataset, with elements z(i)\\nRequire: A, the learning algorithm, seen as a function that takes a dataset as\\ninput and outputs a learned function\\nRequire: L, the loss function, seen as a function from a learned function f and\\nan example z(i) D to a scalar R\\n∈ ∈\\nRequire: k, the number of folds\\nD D D\\nSplit into k mutually exclusive subsets , whose union is .\\ni\\nfor i from 1 to k do\\nD D\\nf = A( )\\ni i\\n\\\\\\nfor z(j) in D do\\ni\\ne = L(f ,z(j))\\nj i\\nend for\\nend for\\nReturn e\\n123\\nCHAPTER 5. MACHINE LEARNING BASICS\\n(i.i.d.) data points. A point estimator or statistic is any function of the data:\\nθˆ = g(x(1),...,x(m)). (5.19)\\nm\\nThe definition does not require that g return a value that is close to the true\\nθ or even that the range of g is the same as the set of allowable values of θ.\\nThis definition of a point estimator is very general and allows the designer of an'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='estimator great flexibility. While almost any function thus qualifies as an estimator,\\na good estimator is a function whose output is close to the true underlying θ that\\ngenerated the training data.\\nFor now, we take the frequentist perspective on statistics. That is, we assume\\nthat the true parameter value θ is fixed but unknown, while the point estimate\\nˆ\\nθ is a function of the data. Since the data is drawn from a random process, any\\nˆ\\nfunction of the data is random. Therefore θ is a random variable.\\nPoint estimation can also refer to the estimation of the relationship between\\ninput and target variables. We refer to these types of point estimates as function\\nestimators.\\nFunction Estimation As we mentioned above, sometimes we are interested in\\nperforming function estimation (or function approximation). Here we are trying to\\npredict a variable y given an input vector x. We assume that there is a function\\nf(x) that describes the approximate relationship between y and x. For example,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='we may assume that y = f(x) + \\ue00f, where \\ue00f stands for the part of y that is not\\npredictable from x. In function estimation, we are interested in approximating\\nˆ\\nf with a model or estimate f. Function estimation is really just the same as\\nˆ\\nestimating a parameter θ; the function estimator f is simply a point estimator in\\nfunction space. The linear regression example (discussed above in section 5.1.4) and\\nthe polynomial regression example (discussed in section 5.2) are both examples of\\nscenarios that may be interpreted either as estimating a parameter wor estimating\\nˆ\\na function f mapping from x to y.\\nWe now review the most commonly studied properties of point estimators and\\ndiscuss what they tell us about these estimators.\\n5.4.2 Bias\\nThe bias of an estimator is defined as:\\nbias(θˆ ) = E (θˆ ) θ (5.20)\\nm m\\n−\\n124\\nCHAPTER 5. MACHINE LEARNING BASICS\\nwhere the expectation is over the data (seen as samples from a random variable)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='and θ is the true underlying value of θ used to define the data generating distri-\\nˆ ˆ\\nbution. An estimator θ is said to be unbiased if bias(θ ) = 0, which implies\\nm m\\nE ˆ ˆ\\nthat (θ ) = θ. An estimator θ is said to be asymptotically unbiased if\\nm m\\nˆ E ˆ\\nlim bias(θ ) = 0, which implies that lim (θ ) = θ.\\nm m m m\\n→∞ →∞\\nExample: Bernoulli Distribution Consider a set of samples x(1),...,x(m)\\n{ }\\nthat are independently and identically distributed according to a Bernoulli distri-\\nbution with mean θ:\\nP(x(i);θ) = θx(i) (1 θ)(1 x(i)). (5.21)\\n−\\n−\\nA common estimator for the θ parameter of this distribution is the mean of the\\ntraining samples:\\nm\\n1\\nˆ θ = x(i). (5.22)\\nm\\nm\\ni=1\\n\\ue058\\nTo determine whether this estimator is biased, we can substitute equation 5.22\\ninto equation 5.20:\\nbias(ˆ θ ) = E [θˆ ] θ (5.23)\\nm m\\n−\\nm\\n1\\n= E x(i) θ (5.24)\\nm −\\n\\ue022 \\ue023\\ni=1\\n\\ue058\\nm\\n1\\n= E x(i) θ (5.25)\\nm −\\n\\ue058i=1 \\ue068 \\ue069\\nm 1\\n1\\n= x(i)θx(i) (1 θ)(1 x(i)) θ (5.26)\\n−\\nm − −\\n\\ue058i=1 x \\ue058(i)=0\\ue010 \\ue011\\nm\\n1\\n= (θ) θ (5.27)\\nm −\\ni=1\\n\\ue058\\n= θ θ = 0 (5.28)\\n−\\nˆ ˆ'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Since bias(θ) = 0, we say that our estimatorθ is unbiased.\\nExample: Gaussian Distribution Estimator of the Mean Now, consider\\na set of samples x(1),...,x(m) that are independently and identically distributed\\n{ }\\naccording to a Gaussian distribution p(x(i)) = (x(i);µ,σ2), where i 1,...,m .\\nN ∈ { }\\n125\\nCHAPTER 5. MACHINE LEARNING BASICS\\nRecall that the Gaussian probability density function is given by\\n1 1 (x(i) µ)2\\np(x(i);µ,σ2) = exp − . (5.29)\\n√2πσ2 \\ue020−2 σ2\\n\\ue021\\nA common estimator of the Gaussian mean parameter is known as the sample\\nmean:\\nm\\n1\\nµˆ = x(i) (5.30)\\nm\\nm\\ni=1\\n\\ue058\\nTo determine the bias of the sample mean, we are again interested in calculating\\nits expectation:\\nE\\nbias(µˆ ) = [µˆ ] µ (5.31)\\nm m\\n−\\nm\\n1\\n= E x(i) µ (5.32)\\nm −\\n\\ue022 \\ue023\\ni=1\\n\\ue058\\nm\\n1\\n= E x(i) µ (5.33)\\nm −\\n\\ue020 \\ue021\\n\\ue058i=1 \\ue068 \\ue069\\nm\\n1\\n= µ µ (5.34)\\nm −\\n\\ue020 \\ue021\\ni=1\\n\\ue058\\n= µ µ = 0 (5.35)\\n−\\nThus we find that the sample mean is an unbiased estimator of Gaussian mean\\nparameter.\\nExample: Estimators of the Variance of a Gaussian Distribution As an'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='example, we compare two different estimators of the variance parameter σ2 of a\\nGaussian distribution. We are interested in knowing if either estimator is biased.\\nThe first estimator of σ2 we consider is known as the sample variance:\\nm\\n1 2\\nσˆ2 = x(i) µˆ , (5.36)\\nm m\\nm −\\n\\ue058i=1 \\ue010 \\ue011\\nwhere µˆ is the sample mean, defined above. More formally, we are interested in\\nm\\ncomputing\\nbias(σˆ2 ) = E [σˆ2 ] σ2 (5.37)\\nm m\\n−\\n126\\nCHAPTER 5. MACHINE LEARNING BASICS\\nWe begin by evaluating the term E [σˆ2 ]:\\nm\\nm\\n1 2\\nE [σˆ2 ] =E x(i) µˆ (5.38)\\nm m\\nm −\\n\\ue022 \\ue023\\n\\ue058i=1 \\ue010 \\ue011\\nm 1\\n= − σ2 (5.39)\\nm\\nReturning to equation 5.37, we conclude that the bias of σˆ2 is σ2/m. Therefore,\\nm\\n−\\nthe sample variance is a biased estimator.\\nThe unbiased sample variance estimator\\nm\\n1 2\\nσ˜2 = x(i) µˆ (5.40)\\nm m 1 − m\\n− \\ue058i=1 \\ue010 \\ue011\\nprovides an alternative approach. As the name suggests this estimator is unbiased.\\nThat is, we find that E [σ˜2 ] = σ2:\\nm\\nm\\n1 2\\nE [σ˜2 ] = E x(i) µˆ (5.41)\\nm m\\nm 1 −\\n\\ue022 \\ue023\\n− \\ue058i=1 \\ue010 \\ue011\\nm\\n= E [σˆ2 ] (5.42)\\nm\\nm 1\\n−\\nm m 1'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='= − σ2 (5.43)\\nm 1 m\\n− \\ue012 \\ue013\\n= σ2. (5.44)\\nWe have two estimators: one is biased and the other is not. While unbiased\\nestimators are clearly desirable, they are not always the “best” estimators. As we\\nwill see we often use biased estimators that possess other important properties.\\n5.4.3 Variance and Standard Error\\nAnother property of the estimator that we might want to consider is how much\\nwe expect it to vary as a function of the data sample. Just as we computed the\\nexpectation of the estimator to determine its bias, we can compute its variance.\\nThe variance of an estimator is simply the variance\\nˆ\\nVar(θ) (5.45)\\nwhere the random variable is the training set. Alternately, the square root of the\\nˆ\\nvariance is called the standard error, denoted SE(θ).\\n127\\nCHAPTER 5. MACHINE LEARNING BASICS\\nThe variance or the standard error of an estimator provides a measure of how\\nwe would expect the estimate we compute from data to vary as we independently'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='resample the dataset from the underlying data generating process. Just as we\\nmight like an estimator to exhibit low bias we would also like it to have relatively\\nlow variance.\\nWhen we compute any statistic using a finite number of samples, our estimate\\nof the true underlying parameter is uncertain, in the sense that we could have\\nobtained other samples from the same distribution and their statistics would have\\nbeen different. The expected degree of variation in any estimator is a source of\\nerror that we want to quantify.\\nThe standard error of the mean is given by\\nm\\n1 σ\\nSE(µˆ ) = Var x(i) = , (5.46)\\nm\\n\\ue076 m √m\\n\\ue075 \\ue022 i=1 \\ue023\\n\\ue075 \\ue058\\n\\ue074\\nwhere σ2 is the true variance of the samples xi. The standard error is often\\nestimated by using an estimate of σ. Unfortunately, neither the square root of\\nthe sample variance nor the square root of the unbiased estimator of the variance\\nprovide an unbiased estimate of the standard deviation. Both approaches tend'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to underestimate the true standard deviation, but are still used in practice. The\\nsquare root of the unbiased estimator of the variance is less of an underestimate.\\nFor large m, the approximation is quite reasonable.\\nThe standard error of the mean is very useful in machine learning experiments.\\nWe often estimate the generalization error by computing the sample mean of the\\nerror on the test set. The number of examples in the test set determines the\\naccuracy of this estimate. Taking advantage of the central limit theorem, which\\ntells us that the mean will be approximately distributed with a normal distribution,\\nwe can use the standard error to compute the probability that the true expectation\\nfalls in any chosen interval. For example, the 95% confidence interval centered on\\nthe mean µˆ is\\nm\\n(µˆ 1.96SE(µˆ ),µˆ + 1.96SE(µˆ )), (5.47)\\nm m m m\\n−\\nunder the normal distribution with mean µˆ and variance SE(µˆ )2. In machine\\nm m'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='learning experiments, it is common to say that algorithmA is better than algorithm\\nB if the upper bound of the 95% confidence interval for the error of algorithmA is\\nless than the lower bound of the 95% confidence interval for the error of algorithm\\nB.\\n128\\nCHAPTER 5. MACHINE LEARNING BASICS\\nExample: Bernoulli Distribution We once again consider a set of samples\\nx(1),...,x(m) drawn independently and identically from a Bernoulli distribution\\n{ }\\n(recall P(x(i);θ) = θx(i) (1 θ)(1 x(i))). This time we are interested in computing\\n−\\n−\\nthe variance of the estimator ˆ θ = 1 m x(i).\\nm m i=1\\n\\ue050 m\\n1\\nVar θˆ = Var x(i) (5.48)\\nm\\nm\\n\\ue020 \\ue021\\n\\ue010 \\ue011 \\ue058i=1\\nm\\n1\\n= Var x(i) (5.49)\\nm2\\n\\ue058i=1 \\ue010 \\ue011\\nm\\n1\\n= θ(1 θ) (5.50)\\nm2 −\\ni=1\\n\\ue058\\n1\\n= mθ(1 θ) (5.51)\\nm2 −\\n1\\n= θ(1 θ) (5.52)\\nm −\\nThe variance of the estimator decreases as a function of m, the number of examples\\nin the dataset. This is a common property of popular estimators that we will\\nreturn to when we discuss consistency (see section 5.4.5).'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='5.4.4 Trading off Bias and Variance to Minimize Mean Squared\\nError\\nBias and variance measure two different sources of error in an estimator. Bias\\nmeasures the expected deviation from the true value of the function or parameter.\\nVariance on the other hand, provides a measure of the deviation from the expected\\nestimator value that any particular sampling of the data is likely to cause.\\nWhat happens when we are given a choice between two estimators, one with\\nmore bias and one with more variance? How do we choose between them? For\\nexample, imagine that we are interested in approximating the function shown in\\nfigure 5.2 and we are only offered the choice between a model with large bias and\\none that suffers from large variance. How do we choose between them?\\nThe most common way to negotiate this trade-off is to use cross-validation.\\nEmpirically, cross-validation is highly successful on many real-world tasks. Alter-\\nnatively, we can also compare the mean squared error (MSE) of the estimates:'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='MSE = E [(ˆ θ θ)2] (5.53)\\nm\\n−\\n= Bias(ˆ θ )2 + Var(θˆ ) (5.54)\\nm m\\n129\\nCHAPTER 5. MACHINE LEARNING BASICS\\nThe MSE measures the overall expected deviation—in a squared error sense—\\nbetween the estimator and the true value of the parameter θ. As is clear from\\nequation 5.54, evaluating the MSE incorporates both the bias and the variance.\\nDesirable estimators are those with small MSE and these are estimators that\\nmanage to keep both their bias and variance somewhat in check.\\nUnderfitting zone Overfitting zone\\nBias Generalization\\nerror\\nVariance\\nOptimal Capacity\\ncapacity\\nFigure 5.6: As capacity increases (x-axis), bias (dotted) tends to decrease and variance\\n(dashed) tends to increase, yielding another U-shaped curve for generalization error (bold\\ncurve). If we vary capacity along one axis, there is an optimal capacity, with underfitting\\nwhenthe capacityis belowthisoptimumandoverfittingwhenit isabove. Thisrelationship'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='is similar to the relationship between capacity, underfitting, and overfitting, discussed in\\nsection 5.2 and figure 5.3.\\nThe relationship between bias and variance is tightly linked to the machine\\nlearning concepts of capacity, underfitting and overfitting. In the case where gen-\\neralization error is measured by the MSE (where bias and variance are meaningful\\ncomponents of generalization error), increasing capacity tends to increase variance\\nand decrease bias. This is illustrated in figure 5.6, where we see again the U-shaped\\ncurve of generalization error as a function of capacity.\\n5.4.5 Consistency\\nSo far we have discussed the properties of various estimators for a training set of\\nfixed size. Usually, we are also concerned with the behavior of an estimator as the\\namount of training data grows. In particular, we usually wish that, as the number\\nof data points m in our dataset increases, our point estimates converge to the true\\n130\\nCHAPTER 5. MACHINE LEARNING BASICS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='value of the corresponding parameters. More formally, we would like that\\nˆ\\nplim θ = θ. (5.55)\\nm m\\n→∞\\nThe symbol plim indicates convergence in probability, meaning that for any \\ue00f > 0,\\nˆ\\nP(θ θ > \\ue00f) 0 as m . The condition described by equation 5.55 is\\nm\\n| − | → → ∞\\nknown as consistency. It is sometimes referred to as weak consistency, with\\nˆ\\nstrong consistency referring to the almost sure convergence of θ to θ. Almost\\nsure convergence of a sequence of random variables x(1),x(2),... to a valuex\\noccurs when p(lim x(m) = x) = 1.\\nm\\n→∞\\nConsistency ensures that the bias induced by the estimator diminishes as the\\nnumber of data examples grows. However, the reverse is not true—asymptotic\\nunbiasedness does not imply consistency. For example, consider estimating the\\nmean parameter µ of a normal distribution (x; µ,σ2), with a dataset consisting\\nN\\nof m samples: x(1),...,x(m) . We could use the first sample x(1) of the dataset\\n{ }'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='as an unbiased estimator: ˆ θ = x(1). In that case, E (θˆ ) = θ so the estimator\\nm\\nis unbiased no matter how many data points are seen. This, of course, implies\\nthat the estimate is asymptotically unbiased. However, this is not a consistent\\nˆ\\nestimator as it is not the case that θ θ as m .\\nm\\n→ → ∞\\n5.5 Maximum Likelihood Estimation\\nPreviously, we have seen some definitions of common estimators and analyzed\\ntheir properties. But where did these estimators come from? Rather than guessing\\nthat some function might make a good estimator and then analyzing its bias and\\nvariance, we would like to have some principle from which we can derive specific\\nfunctions that are good estimators for different models.\\nThe most common such principle is the maximum likelihood principle.\\nConsider a set of m examples X = x(1),...,x(m) drawn independently from\\n{ }\\nthe true but unknown data generating distribution p (x).\\ndata\\nLet p (x;θ) be a parametric family of probability distributions over the\\nmodel'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='same space indexed by θ. In other words, p (x;θ) maps any configurationx\\nmodel\\nto a real number estimating the true probability p (x).\\ndata\\nThe maximum likelihood estimator for θ is then defined as\\nX\\nθ = argmaxp ( ;θ) (5.56)\\nML model\\nθ\\nm\\n= argmax p (x(i);θ) (5.57)\\nmodel\\nθ\\ni=1\\n131\\n\\ue059\\nCHAPTER 5. MACHINE LEARNING BASICS\\nThisproduct overmanyprobabilitiescan beinconvenientfora varietyofreasons.\\nFor example, it is prone to numerical underflow. To obtain a more convenient\\nbut equivalent optimization problem, we observe that taking the logarithm of the\\nlikelihood does not change its argmax but does conveniently transform a product\\ninto a sum:\\nm\\nθ = argmax logp (x(i);θ). (5.58)\\nML model\\nθ\\ni=1\\n\\ue058\\nBecause the argmax does not change when we rescale the cost function, we can\\ndivide by m to obtain a version of the criterion that is expressed as an expectation\\nwith respect to the empirical distribution pˆ defined by the training data:\\ndata\\nθ = argmaxE logp (x;θ). (5.59)\\nML x pˆ model\\ndata\\nθ ∼'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='One way to interpret maximum likelihood estimation is to view it as minimizing\\nthe dissimilarity between the empirical distribution pˆ defined by the training\\ndata\\nset and the model distribution, with the degree of dissimilarity between the two\\nmeasured by the KL divergence. The KL divergence is given by\\nE\\nD (pˆ p ) = [log pˆ (x) logp (x)]. (5.60)\\nKL data model x pˆ data model\\n\\ue06b ∼ data −\\nThe term on the left is a function only of the data generating process, not the\\nmodel. This means when we train the model to minimize the KL divergence, we\\nneed only minimize\\nE\\n[logp (x)] (5.61)\\nx pˆ model\\n− ∼ data\\nwhich is of course the same as the maximization in equation 5.59.\\nMinimizing this KL divergence corresponds exactly to minimizing the cross-\\nentropy between the distributions. Many authors use the term “cross-entropy” to\\nidentifyspecifically thenegativelog-likelihood ofa Bernoulliorsoftmax distribution,\\nbut that is a misnomer. Any loss consisting of a negative log-likelihood is a cross-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='entropy between the empirical distribution defined by the training set and the\\nprobability distribution defined by model. For example, mean squared error is the\\ncross-entropy between the empirical distribution and a Gaussian model.\\nWe can thus see maximum likelihood as an attempt to make the model dis-\\ntribution match the empirical distribution pˆ . Ideally, we would like to match\\ndata\\nthe true data generating distribution p , but we have no direct access to this\\ndata\\ndistribution.\\nWhile the optimal θ is the same regardless of whether we are maximizing the\\nlikelihood or minimizing the KL divergence, the values of the objective functions\\n132\\nCHAPTER 5. MACHINE LEARNING BASICS\\nare different. In software, we often phrase both as minimizing a cost function.\\nMaximum likelihood thus becomes minimization of the negative log-likelihood\\n(NLL), or equivalently, minimization of the cross entropy. The perspective of\\nmaximum likelihood as minimum KL divergence becomes helpful in this case'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='because the KL divergence has a known minimum value of zero. The negative\\nlog-likelihood can actually become negative when x is real-valued.\\n5.5.1 Conditional Log-Likelihood and Mean Squared Error\\nThe maximum likelihood estimator can readily be generalized to the case where\\nour goal is to estimate a conditional probability P(y x;θ) in order to predict y\\n|\\ngiven x. This is actually the most common situation because it forms the basis for\\nmost supervised learning. If X represents all our inputs and Y all our observed\\ntargets, then the conditional maximum likelihood estimator is\\nθ = argmaxP(Y X;θ). (5.62)\\nML\\n|\\nθ\\nIf the examples are assumed to be i.i.d., then this can be decomposed into\\nm\\nθ = argmax logP(y(i) x(i);θ). (5.63)\\nML\\n|\\nθ\\ni=1\\n\\ue058\\nExample: Linear Regression as Maximum Likelihood Linear regression,\\nintroduced earlier in section 5.1.4, may be justified as a maximum likelihood\\nprocedure. Previously, we motivated linear regression as an algorithm that learns'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to take an input xand produce an output value yˆ. The mapping from x to yˆis\\nchosen to minimize mean squared error, a criterion that we introduced more or less\\narbitrarily. We now revisit linear regression from the point of view of maximum\\nlikelihood estimation. Instead of producing a single prediction yˆ, we now think\\nof the model as producing a conditional distribution p(y x). We can imagine\\n|\\nthat with an infinitely large training set, we might see several training examples\\nwith the same input value x but different values of y. The goal of the learning\\nalgorithm is now to fit the distribution p(y x) to all of those different y values\\n|\\nthat are all compatible with x. To derive the same linear regression algorithm\\nwe obtained before, we define p(y x) = (y;yˆ(x;w),σ2). The function yˆ(x;w)\\n| N\\ngives the prediction of the mean of the Gaussian. In this example, we assume that\\nthe variance is fixed to some constant σ2 chosen by the user. We will see that this'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='choice of the functional form of p(y x) causes the maximum likelihood estimation\\n|\\nprocedure to yield the same learning algorithm as we developed before. Since the\\n133\\nCHAPTER 5. MACHINE LEARNING BASICS\\nexamples are assumed to be i.i.d., the conditional log-likelihood (equation 5.63) is\\ngiven by\\nm\\nlogp(y(i) x(i);θ) (5.64)\\n|\\ni=1\\n\\ue058\\nm m yˆ(i) y(i) 2\\n= mlogσ log(2π) − , (5.65)\\n− − 2 − 2σ2\\n\\ue00d \\ue00d\\ni=1\\n\\ue058 \\ue00d \\ue00d\\nwhere yˆ(i) is the output of the linear regression on the i-th input x(i) and m is the\\nnumber of the training examples. Comparing the log-likelihood with the mean\\nsquared error,\\nm\\n1\\nMSE = yˆ(i) y(i) 2, (5.66)\\ntrain\\nm || − ||\\ni=1\\n\\ue058\\nwe immediately see that maximizing the log-likelihood with respect to w yields\\nthe same estimate of the parameters w as does minimizing the mean squared error.\\nThe two criteria have different values but the same location of the optimum. This\\njustifies the use of the MSE as a maximum likelihood estimation procedure. As we'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='will see, the maximum likelihood estimator has several desirable properties.\\n5.5.2 Properties of Maximum Likelihood\\nThe main appeal of the maximum likelihood estimator is that it can be shown to\\nbe the best estimator asymptotically, as the number of examples m , in terms\\n→ ∞\\nof its rate of convergence as m increases.\\nUnder appropriate conditions, the maximum likelihood estimator has the\\nproperty of consistency (see section 5.4.5 above), meaning that as the number\\nof training examples approaches infinity, the maximum likelihood estimate of a\\nparameter converges to the true value of the parameter. These conditions are:\\nThe true distribution p must lie within the model family p ( ;θ).\\ndata model\\n• ·\\nOtherwise, no estimator can recover p .\\ndata\\nThe true distribution p must correspond to exactly one value of θ. Other-\\ndata\\n•\\nwise, maximum likelihood can recover the correct p , but will not be able\\ndata\\nto determine which value of θ was used by the data generating processing.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='There are other inductive principles besides the maximum likelihood estima-\\ntor, many of which share the property of being consistent estimators. However,\\n134\\nCHAPTER 5. MACHINE LEARNING BASICS\\nconsistent estimators can differ in their statistic efficiency, meaning that one\\nconsistent estimator may obtain lower generalization error for a fixed number of\\nsamples m, or equivalently, may require fewer examples to obtain a fixed level of\\ngeneralization error.\\nStatistical efficiency is typically studied in the parametric case (like in linear\\nregression) where our goal is to estimate the value of a parameter (and assuming\\nit is possible to identify the true parameter), not the value of a function. A way to\\nmeasure how close we are to the true parameter is by the expected mean squared\\nerror, computing the squared difference between the estimated and true parameter\\nvalues, where the expectation is over m training samples from the data generating'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='distribution. That parametric mean squared error decreases as m increases, and\\nfor m large, the Cramér-Rao lower bound (Rao, 1945; Cramér, 1946) shows that no\\nconsistent estimator has a lower mean squared error than the maximum likelihood\\nestimator.\\nFor these reasons (consistency and efficiency), maximum likelihood is often\\nconsidered the preferred estimator to use for machine learning. When the number\\nof examples is small enough to yield overfitting behavior, regularization strategies\\nsuch as weight decay may be used to obtain a biased version of maximum likelihood\\nthat has less variance when training data is limited.\\n5.6 Bayesian Statistics\\nSo far we have discussed frequentist statistics and approaches based on estimat-\\ning a single value of θ, then making all predictions thereafter based on that one\\nestimate. Another approach is to consider all possible values of θ when making a\\nprediction. The latter is the domain of Bayesian statistics.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='As discussed in section 5.4.1, the frequentist perspective is that the true\\nˆ\\nparameter value θ is fixed but unknown, while the point estimate θ is a random\\nvariable on account of it being a function of the dataset (which is seen as random).\\nThe Bayesian perspective on statistics is quite different. The Bayesian uses\\nprobability to reflect degrees of certainty of states of knowledge. The dataset is\\ndirectly observed and so is not random. On the other hand, the true parameter θ\\nis unknown or uncertain and thus is represented as a random variable.\\nBefore observing the data, we represent our knowledge of θ using the prior\\nprobability distribution, p(θ) (sometimes referred to as simply “the prior”).\\nGenerally, the machine learning practitioner selects a prior distribution that is\\nquite broad (i.e. with high entropy) to reflect a high degree of uncertainty in the\\n135\\nCHAPTER 5. MACHINE LEARNING BASICS\\nvalue of θ before observing any data. For example, one might assume a priori that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='θ lies in some finite range or volume, with a uniform distribution. Many priors\\ninstead reflect a preference for “simpler” solutions (such as smaller magnitude\\ncoefficients, or a function that is closer to being constant).\\nNow consider that we have a set of data samples x(1),...,x(m) . We can\\n{ }\\nrecover the effect of data on our belief about θ by combining the data likelihood\\np(x(1),...,x(m) θ) with the prior via Bayes’ rule:\\n|\\np(x(1),...,x(m) θ)p(θ)\\np(θ x(1),...,x(m)) = | (5.67)\\n| p(x(1),...,x(m))\\nIn the scenarios where Bayesian estimation is typically used, the prior begins as a\\nrelatively uniform or Gaussian distribution with high entropy, and the observation\\nof the data usually causes the posterior to lose entropy and concentrate around a\\nfew highly likely values of the parameters.\\nRelative to maximum likelihood estimation, Bayesian estimation offers two\\nimportant differences. First, unlike the maximum likelihood approach that makes'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='predictionsusinga pointestimateof θ, theBayesian approachisto makepredictions\\nusing a full distribution over θ. For example, after observing m examples, the\\npredicted distribution over the next data sample, x(m+1), is given by\\np(x(m+1) x(1),...,x(m)) = p(x(m+1) θ)p(θ x(1),...,x(m)) dθ. (5.68)\\n| | |\\n\\ue05a\\nHere each value of θ with positive probability density contributes to the prediction\\nof the next example, with the contribution weighted by the posterior density itself.\\nAfter having observed x(1),...,x(m) , if we are still quite uncertain about the\\n{ }\\nvalue of θ, then this uncertainty is incorporated directly into any predictions we\\nmight make.\\nIn section 5.4, we discussed how the frequentist approach addresses the uncer-\\ntainty in a given point estimate of θ by evaluating its variance. The variance of\\nthe estimator is an assessment of how the estimate might change with alternative\\nsamplings of the observeddata. The Bayesian answer to the question of how to deal'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='with the uncertainty in the estimator is to simply integrate over it, which tends to\\nprotect well against overfitting. This integral is of course just an application of\\nthe laws of probability, making the Bayesian approach simple to justify, while the\\nfrequentist machinery for constructing an estimator is based on the rather ad hoc\\ndecision to summarize all knowledge contained in the dataset with a single point\\nestimate.\\nThe second important difference between the Bayesian approach to estimation\\nand the maximum likelihood approach is due to the contribution of the Bayesian\\n136\\nCHAPTER 5. MACHINE LEARNING BASICS\\nprior distribution. The prior has an influence by shifting probability mass density\\ntowards regions of the parameter space that are preferred a priori. In practice,\\nthe prior often expresses a preference for models that are simpler or more smooth.\\nCritics of the Bayesian approach identify the prior as a source of subjective human\\njudgment impacting the predictions.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Bayesian methods typically generalize much better when limited training data\\nis available, but typically suffer from high computational cost when the number of\\ntraining examples is large.\\nExample: Bayesian Linear Regression Here we consider the Bayesian esti-\\nmation approach to learning the linear regression parameters. In linear regression,\\nwe learn a linear mapping from an input vector x\\nRn\\nto predict the value of a\\n∈\\nscalar y\\nR\\n. The prediction is parametrized by the vector w\\nRn:\\n∈ ∈\\nyˆ= w x. (5.69)\\n\\ue03e\\nGiven a set of m training samples (X(train),y(train)), we can express the prediction\\nof y over the entire training set as:\\nyˆ(train) = X(train)w. (5.70)\\nExpressed as a Gaussian conditional distribution on y(train), we have\\np(y(train) X(train),w) = (y(train);X(train)w,I) (5.71)\\n| N\\n1\\nexp (y(train) X(train)w) (y(train) X(train)w) ,\\n\\ue03e\\n∝ −2 − −\\n\\ue012 \\ue013\\n(5.72)\\nwhere we follow the standard MSE formulation in assuming that the Gaussian'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='variance on y is one. In what follows, to reduce the notational burden, we refer to\\n(X(train),y(train)) as simply (X,y).\\nTo determine the posterior distribution over the model parameter vector w, we\\nfirst need to specify a prior distribution. The prior should reflect our naive belief\\nabout the value of these parameters. While it is sometimes difficult or unnatural\\nto express our prior beliefs in terms of the parameters of the model, in practice we\\ntypically assume a fairly broad distribution expressing a high degree of uncertainty\\nabout θ. For real-valued parameters it is common to use a Gaussian as a prior\\ndistribution:\\n1\\np(w) = (w;µ ,Λ ) exp (w µ ) Λ 1(w µ ) , (5.73)\\nN\\n0 0\\n∝ −2 −\\n0 \\ue03e −0\\n−\\n0\\n137\\n\\ue012 \\ue013\\nCHAPTER 5. MACHINE LEARNING BASICS\\nwhere µ and Λ are the prior distribution mean vector and covariance matrix\\n0 0\\nrespectively.1\\nWith the prior thus specified, we can now proceed in determining the posterior\\ndistribution over the model parameters.\\np(w X,y) p(y X,w)p(w) (5.74)\\n| ∝ |\\n1 1'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='exp (y Xw) (y Xw) exp (w µ ) Λ 1(w µ )\\n∝ −2 −\\n\\ue03e\\n− −2 −\\n0 \\ue03e −0\\n−\\n0\\n\\ue012 \\ue013 \\ue012 \\ue013\\n(5.75)\\n1\\nexp 2y Xw +w X Xw +w Λ 1w 2µ Λ 1w .\\n∝ −2 −\\n\\ue03e \\ue03e \\ue03e \\ue03e −0\\n−\\n\\ue03e0 −0\\n\\ue012 \\ue013\\n\\ue010 \\ue011\\n(5.76)\\n1 1 1\\nWe now define Λ m = X \\ue03eX + Λ −0 − and µ m = Λ m X \\ue03ey + Λ −0 µ 0 . Using\\nthese new variables, we find that the posterior may be rewritten as a Gaussian\\n\\ue000 \\ue001 \\ue000 \\ue001\\ndistribution:\\n1 1\\np(w X,y) exp (w µ ) Λ 1(w µ ) + µ Λ 1µ (5.77)\\nm \\ue03e −m m \\ue03em −m m\\n| ∝ −2 − − 2\\n\\ue012 \\ue013\\n1\\nexp (w µ ) Λ 1(w µ ) . (5.78)\\nm \\ue03e −m m\\n∝ −2 − −\\n\\ue012 \\ue013\\nAll terms that do not include the parameter vector w have been omitted; they\\nare implied by the fact that the distribution must be normalized to integrate to 1.\\nEquation 3.23 shows how to normalize a multivariate Gaussian distribution.\\nExamining this posterior distribution allows us to gain some intuition for the\\neffect of Bayesian inference. In most situations, we set µ to 0. If we set Λ = 1 I,\\n0 0 α\\nthen µ gives the same estimate of w as does frequentist linear regression with a\\nm'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='weight decay penalty of αw w. One difference is that the Bayesian estimate is\\n\\ue03e\\nundefined if α is set to zero—-we are not allowed to begin the Bayesian learning\\nprocess with an infinitely wide prior on w. The more important difference is that\\nthe Bayesian estimate provides a covariance matrix, showing how likely all the\\ndifferent values of w are, rather than providing only the estimate µ .\\nm\\nA Posteriori\\n5.6.1 Maximum (MAP) Estimation\\nWhile the most principled approach is to make predictions using the full Bayesian\\nposterior distribution over the parameter θ, it is still often desirable to have a\\n1Unless there is a reason to assume a particular covariance structure, we typically assume a\\ndiagonal covariance matrix Λ = diag(λ ).\\n0 0\\n138\\nCHAPTER 5. MACHINE LEARNING BASICS\\nsingle point estimate. One common reason for desiring a point estimate is that\\nmost operations involving the Bayesian posterior for most interesting models are'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='intractable, and a point estimate offers a tractable approximation. Rather than\\nsimply returning to the maximum likelihood estimate, we can still gain some of\\nthe benefit of the Bayesian approach by allowing the prior to influence the choice\\nof the point estimate. One rational way to do this is to choose the maximum\\na posteriori (MAP) point estimate. The MAP estimate chooses the point of\\nmaximal posterior probability (or maximal probability density in the more common\\ncase of continuous θ):\\nθ = argmax p(θ x) = argmaxlogp(x θ) +logp(θ). (5.79)\\nMAP\\n| |\\nθ θ\\nWe recognize, above on the right hand side, logp(x θ), i.e. the standard log-\\n|\\nlikelihood term, and logp(θ), corresponding to the prior distribution.\\nAs an example, consider a linear regression model with a Gaussian prior on\\nthe weights w. If this prior is given by (w;0, 1I2), then the log-prior term in\\nN λ\\nequation 5.79 is proportional to the familiar λw w weight decay penalty, plus a\\n\\ue03e'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='term that does not depend on w and does not affect the learning process. MAP\\nBayesian inference with a Gaussian prior on the weights thus corresponds to weight\\ndecay.\\nAs with full Bayesian inference, MAP Bayesian inference has the advantage of\\nleveraging information that is brought by the prior and cannot be found in the\\ntraining data. This additional information helps to reduce the variance in the\\nMAP point estimate (in comparison to the ML estimate). However, it does so at\\nthe price of increased bias.\\nMany regularized estimation strategies, such as maximum likelihood learning\\nregularized with weight decay, can be interpreted as making the MAP approxima-\\ntion to Bayesian inference. This view applies when the regularization consists of\\nadding an extra term to the objective function that corresponds to logp(θ). Not\\nall regularization penalties correspond to MAP Bayesian inference. For example,\\nsome regularizer terms may not be the logarithm of a probability distribution.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Other regularization terms depend on the data, which of course a prior probability\\ndistribution is not allowed to do.\\nMAP Bayesian inference provides a straightforward way to design complicated\\nyet interpretable regularization terms. For example, a more complicated penalty\\nterm can be derived by using a mixture of Gaussians, rather than a single Gaussian\\ndistribution, as the prior (Nowlan and Hinton, 1992).\\n139\\nCHAPTER 5. MACHINE LEARNING BASICS\\n5.7 Supervised Learning Algorithms\\nRecall from section 5.1.3 that supervised learning algorithms are, roughly speaking,\\nlearning algorithms that learn to associate some input with some output, given a\\ntraining set of examples of inputs x and outputs y. In many cases the outputs\\ny may be difficult to collect automatically and must be provided by a human\\n“supervisor,” but the term still applies even when the training set targets were\\ncollected automatically.\\n5.7.1 Probabilistic Supervised Learning'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Most supervised learning algorithms in this book are based on estimating a\\nprobability distribution p(y x). We can do this simply by using maximum\\n|\\nlikelihood estimation to find the best parameter vector θ for a parametric family\\nof distributions p(y x;θ).\\n|\\nWe have already seen that linear regression corresponds to the family\\np(y x;θ) = (y;θ x,I). (5.80)\\n\\ue03e\\n| N\\nWe can generalize linear regression to the classification scenario by defining a\\ndifferent family of probability distributions. If we have two classes, class 0 and\\nclass 1, then we need only specify the probability of one of these classes. The\\nprobability of class 1 determines the probability of class 0, because these two values\\nmust add up to 1.\\nThe normal distribution over real-valued numbers that we used for linear\\nregression is parametrized in terms of a mean. Any value we supply for this mean\\nis valid. A distribution over a binary variable is slightly more complicated, because'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='its mean must always be between 0 and 1. One way to solve this problem is to use\\nthe logistic sigmoid function to squash the output of the linear function into the\\ninterval (0, 1) and interpret that value as a probability:\\np(y = 1 x;θ) = σ(θ x). (5.81)\\n\\ue03e\\n|\\nThis approach is known as logistic regression (a somewhat strange name since\\nwe use the model for classification rather than regression).\\nIn the case of linear regression, we were able to find the optimal weights by\\nsolving the normal equations. Logistic regression is somewhat more difficult. There\\nis no closed-form solution for its optimal weights. Instead, we must search for\\nthem by maximizing the log-likelihood. We can do this by minimizing the negative\\nlog-likelihood (NLL) using gradient descent.\\n140\\nCHAPTER 5. MACHINE LEARNING BASICS\\nThissamestrategycanbeapplied toessentiallyanysupervisedlearningproblem,\\nby writing down a parametric family of conditional probability distributions over'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the right kind of input and output variables.\\n5.7.2 Support Vector Machines\\nOne of the most influential approaches to supervised learning is the support vector\\nmachine (Boser et al., 1992; Cortes and Vapnik, 1995). This model is similar to\\nlogistic regression in that it is driven by a linear function w x+b. Unlike logistic\\n\\ue03e\\nregression, the support vector machine does not provide probabilities, but only\\noutputs a class identity. The SVM predicts that the positive class is present when\\nw x+b is positive. Likewise, it predicts that the negative class is present when\\n\\ue03e\\nw x +b is negative.\\n\\ue03e\\nOne key innovation associated with support vector machines is the kernel\\ntrick. The kernel trickconsists of observing that manymachinelearning algorithms\\ncan be written exclusively in terms of dot products between examples. For example,\\nit can be shown that the linear function used by the support vector machine can\\nbe re-written as\\nm\\nw x +b = b + α x x(i) (5.82)\\n\\ue03e i \\ue03e\\ni=1\\n\\ue058'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='where x(i) is a training example and α is a vector of coefficients. Rewriting the\\nlearning algorithm this way allows us to replace x by the output of a given feature\\nfunction φ(x) and the dot product with a function k(x,x(i)) = φ(x) φ(x(i)) called\\n·\\na kernel. The operator represents an inner product analogous to φ(x) φ(x(i)).\\n\\ue03e\\n·\\nFor some feature spaces, we may not use literally the vector inner product. In\\nsome infinite dimensional spaces, we need to use other kinds of inner products, for\\nexample, inner products based on integration rather than summation. A complete\\ndevelopment of these kinds of inner products is beyond the scope of this book.\\nAfter replacing dot products with kernel evaluations, we can make predictions\\nusing the function\\nf(x) = b + α k(x,x(i)). (5.83)\\ni\\ni\\n\\ue058\\nThis function is nonlinear with respect to x, but the relationship between φ(x)\\nand f (x) is linear. Also, the relationship between α and f(x) is linear. The'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='kernel-based function is exactly equivalent to preprocessing the data by applying\\nφ(x) to all inputs, then learning a linear model in the new transformed space.\\nThe kernel trick is powerful for two reasons. First, it allows us to learn models\\nthat are nonlinear as a function of x using convex optimization techniques that are\\n141\\nCHAPTER 5. MACHINE LEARNING BASICS\\nguaranteed to converge efficiently. This is possible because we consider φ fixed and\\noptimize only α, i.e., the optimization algorithm can view the decision function\\nas being linear in a different space. Second, the kernel function k often admits\\nan implementation that is significantly more computational efficient than naively\\nconstructing two φ(x) vectors and explicitly taking their dot product.\\nIn some cases, φ(x) can even be infinite dimensional, which would result in\\nan infinite computational cost for the naive, explicit approach. In many cases,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='k(x,x ) is a nonlinear, tractable function of x even when φ(x) is intractable. As\\n\\ue030\\nan example of an infinite-dimensional feature space with a tractable kernel, we\\nconstruct a feature mapping φ(x) over the non-negative integers x. Suppose that\\nthis mapping returns a vector containing x ones followed by infinitely many zeros.\\nWe can write a kernel function k(x,x(i)) = min(x,x(i)) that is exactly equivalent\\nto the corresponding infinite-dimensional dot product.\\nThe most commonly used kernel is the Gaussian kernel\\nk(u,v) = (u v;0,σ2I) (5.84)\\nN −\\nwhere (x;µ,Σ) is the standard normal density. This kernel is also known as\\nN\\nthe radial basis function (RBF) kernel, because its value decreases along lines\\nin v space radiating outward from u. The Gaussian kernel corresponds to a dot\\nproduct in an infinite-dimensional space, but the derivation of this space is less\\nstraightforward than in our example of the min kernel over the integers.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='We can think of the Gaussian kernel as performing a kind of template match-\\ning. A training example xassociated with training label y becomes a template\\nfor class y. When a test point x is near x according to Euclidean distance, the\\n\\ue030\\nGaussian kernel has a large response, indicating that x is very similar to the x\\n\\ue030\\ntemplate. The model then puts a large weight on the associated training label y.\\nOverall, the prediction will combine many such training labels weighted by the\\nsimilarity of the corresponding training examples.\\nSupport vector machines are not the only algorithm that can be enhanced\\nusing the kernel trick. Many other linear models can be enhanced in this way. The\\ncategory of algorithms that employ the kernel trick is known as kernel machines\\nor kernel methods (Williams and Rasmussen, 1996; Schölkopf et al., 1999).\\nA major drawback to kernel machines is that the cost of evaluating the decision\\nfunction is linear in the number of training examples, because the i-th example'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='contributes a term α k(x,x(i)) to the decision function. Support vector machines\\ni\\nare able to mitigate this by learning an α vector that contains mostly zeros.\\nClassifying a new example then requires evaluating the kernel function only for\\nthe training examples that have non-zero α . These training examples are known\\ni\\n142\\nCHAPTER 5. MACHINE LEARNING BASICS\\nas support vectors.\\nKernel machines also suffer from a high computational cost of training when\\nthe dataset is large. We will revisit this idea in section 5.9. Kernel machines with\\ngeneric kernels struggle to generalize well. We will explain why in section 5.11. The\\nmodern incarnation of deep learning was designed to overcome these limitations of\\nkernel machines. The current deep learning renaissance began when Hinton et al.\\n(2006) demonstrated that a neural network could outperform the RBF kernel SVM\\non the MNIST benchmark.\\n5.7.3 Other Simple Supervised Learning Algorithms'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='We have already briefly encountered another non-probabilistic supervised learning\\nalgorithm, nearest neighbor regression. More generally, k-nearest neighbors is\\na family of techniques that can be used for classification or regression. As a\\nnon-parametric learning algorithm, k-nearest neighbors is not restricted to a fixed\\nnumber of parameters. We usually think of the k-nearest neighbors algorithm\\nas not having any parameters, but rather implementing a simple function of the\\ntraining data. In fact, there is not even really a training stage or learning process.\\nInstead, at test time, when we want to produce an output y for a new test input x,\\nwe find the k-nearest neighbors to x in the training data X. We then return the\\naverage of the corresponding y values in the training set. This works for essentially\\nany kind of supervised learning where we can define an average over y values. In\\nthe case of classification, we can average over one-hot code vectors c with c = 1\\ny'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='and c = 0 for all other values of i. We can then interpret the average over these\\ni\\none-hot codes as giving a probability distribution over classes. As a non-parametric\\nlearning algorithm, k-nearest neighbor can achieve very high capacity. For example,\\nsuppose we have a multiclass classification task and measure performance with 0-1\\nloss. In this setting, 1-nearest neighbor converges to double the Bayes error as the\\nnumber of training examples approaches infinity. The error in excess of the Bayes\\nerror results from choosing a single neighbor by breaking ties between equally\\ndistant neighbors randomly. When there is infinite training data, all test points x\\nwill have infinitely many training set neighbors at distance zero. If we allow the\\nalgorithm to use all of these neighbors to vote, rather than randomly choosing one\\nof them, the procedure converges to the Bayes error rate. The high capacity of\\nk-nearest neighbors allows it to obtain high accuracy given a large training set.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='However, it does so at high computational cost, and it may generalize very badly\\ngiven a small, finite training set. One weakness of k-nearest neighbors is that it\\ncannot learn that one feature is more discriminative than another. For example,\\nimagine we have a regression task with x\\nR100\\ndrawn from an isotropic Gaussian\\n∈\\n143\\nCHAPTER 5. MACHINE LEARNING BASICS\\ndistribution, but only a single variable x is relevant to the output. Suppose\\n1\\nfurther that this feature simply encodes the output directly, i.e. that y = x in all\\n1\\ncases. Nearest neighbor regression will not be able to detect this simple pattern.\\nThe nearest neighbor of most points x will be determined by the large number of\\nfeatures x through x , not by the lone feature x . Thus the output on small\\n2 100 1\\ntraining sets will essentially be random.\\n144\\nCHAPTER 5. MACHINE LEARNING BASICS\\n0 1\\n10\\n00 01 11\\n010 011 110 111\\n1110 1111\\n010\\n00 01\\n0\\n011\\n110\\n1\\n11\\n10\\n1110 111 1111'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Figure 5.7: Diagrams describing how a decision tree works. (Top)Each node of the tree\\nchooses to send the input example to the child node on the left (0) or or the child node on\\nthe right (1). Internal nodes are drawn as circles and leaf nodes as squares. Each node is\\ndisplayed with a binary string identifier corresponding to its position in the tree, obtained\\nby appending abit to its parent identifier (0=choose leftor top, 1=choose rightor bottom).\\n(Bottom)The tree divides space into regions. The 2D plane shows how a decision tree\\nmight divide R2. The nodes of the tree are plotted in this plane, with each internal node\\ndrawn along the dividing line it uses to categorize examples, and leaf nodes drawn in the\\ncenter of the region of examples they receive. The result is a piecewise-constant function,\\nwith one piece per leaf. Each leaf requires at least one training example to define, so it is\\nnot possible for the decision tree to learn a function that has more local maxima than the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='number of training examples.\\n145\\nCHAPTER 5. MACHINE LEARNING BASICS\\nAnother type of learning algorithm that also breaks the input space into regions\\nand has separate parameters for each region is the decision tree (Breiman et al.,\\n1984) and its many variants. As shown in figure 5.7, each node of the decision\\ntree is associated with a region in the input space, and internal nodes break that\\nregion into one sub-region for each child of the node (typically using an axis-aligned\\ncut). Space is thus sub-divided into non-overlapping regions, with a one-to-one\\ncorrespondence between leaf nodes and input regions. Each leaf node usually maps\\nevery point in its input region to the same output. Decision trees are usually\\ntrained with specialized algorithms that are beyond the scope of this book. The\\nlearning algorithm can be considered non-parametric if it is allowed to learn a tree\\nof arbitrary size, though decision trees are usually regularized with size constraints'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='that turn them into parametric models in practice. Decision trees as they are\\ntypically used, with axis-aligned splits and constant outputs within each node,\\nstruggle to solve some problems that are easy even for logistic regression. For\\nexample, if we have a two-class problem and the positive class occurs wherever\\nx > x , the decision boundary is not axis-aligned. The decision tree will thus\\n2 1\\nneed to approximate the decision boundary with many nodes, implementing a step\\nfunction that constantly walks back and forth across the true decision function\\nwith axis-aligned steps.\\nAs we have seen, nearest neighbor predictors and decision trees have many\\nlimitations. Nonetheless, they are useful learning algorithms when computational\\nresources are constrained. We can also build intuition for more sophisticated\\nlearning algorithms by thinking about the similarities and differences between\\nsophisticated algorithms and k-NN or decision tree baselines.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='See Murphy (2012), Bishop (2006), Hastie et al. (2001) or other machine\\nlearning textbooks for more material on traditional supervised learning algorithms.\\n5.8 Unsupervised Learning Algorithms\\nRecall from section 5.1.3 that unsupervised algorithms are those that experience\\nonly “features” but not a supervision signal. The distinction between supervised\\nand unsupervised algorithms is not formally and rigidly defined because there is no\\nobjective test for distinguishing whether a value is a feature or a target provided by\\na supervisor. Informally, unsupervised learning refers to most attempts to extract\\ninformation from a distribution that do not require human labor to annotate\\nexamples. The term is usually associated with density estimation, learning to\\ndraw samples from a distribution, learning to denoise data from some distribution,\\nfinding a manifold that the data lies near, or clustering the data into groups of\\n146\\nCHAPTER 5. MACHINE LEARNING BASICS\\nrelated examples.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='A classic unsupervised learning task is to find the “best” representation of the\\ndata. By ‘best’ we can mean different things, but generally speaking we are looking\\nfor a representation that preserves as much information about x as possible while\\nobeying some penalty or constraint aimed at keeping the representation simpler or\\nmore accessible than x itself.\\nThere are multiple ways of defining a simpler representation. Three of the\\nmost common include lower dimensional representations, sparse representations\\nand independent representations. Low-dimensional representations attempt to\\ncompress as much information about x as possible in a smaller representation.\\nSparse representations (Barlow, 1989; Olshausen and Field, 1996; Hinton and\\nGhahramani, 1997) embed the dataset into a representation whose entries are\\nmostly zeroes for most inputs. The use of sparse representations typically requires\\nincreasing the dimensionality of the representation, so that the representation'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='becoming mostly zeroes does not discard too much information. This results in an\\noverall structure of the representation that tends to distribute data along the axes\\nof the representation space. Independent representations attempt to disentangle\\nthe sources of variation underlying the data distribution such that the dimensions\\nof the representation are statistically independent.\\nOf course these three criteria are certainly not mutually exclusive. Low-\\ndimensional representations often yield elements that have fewer or weaker de-\\npendencies than the original high-dimensional data. This is because one way to\\nreduce the size of a representation is to find and remove redundancies. Identifying\\nand removing more redundancy allows the dimensionality reduction algorithm to\\nachieve more compression while discarding less information.\\nThe notion of representation is one of the central themes of deep learning and\\ntherefore one of the central themes in this book. In this section, we develop some'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='simple examples of representation learning algorithms. Together, these example\\nalgorithms show how to operationalize all three of the criteria above. Most of the\\nremaining chapters introduce additional representation learning algorithms that\\ndevelop these criteria in different ways or introduce other criteria.\\n5.8.1 Principal Components Analysis\\nIn section 2.12, we saw that the principal components analysis algorithm provides\\na means of compressing data. We can also view PCA as an unsupervised learning\\nalgorithm that learns a representation of data. This representation is based on\\ntwo of the criteria for a simple representation described above. PCA learns a\\n147\\nCHAPTER 5. MACHINE LEARNING BASICS\\n20\\n10\\n0\\n10\\n−\\n20\\n−\\n20 10 0 10 20\\n− −\\nx\\n1\\nx\\n2\\n20\\n10\\n0\\n10\\n−\\n20\\n−\\n20 10 0 10 20\\n− −\\nz\\n1\\nz\\n2\\nFigure 5.8: PCA learns a linear projection that aligns the direction of greatest variance\\nwith the axes of the new space. (Left)The original data consists of samples ofx. In this'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='space, the variance might occur along directions that are not axis-aligned. (Right)The\\ntransformed data z= x W now varies most along the axis z . The direction of second\\n\\ue03e 1\\nmost variance is now along z .\\n2\\nrepresentation that has lower dimensionality than the original input. It also learns\\na representation whose elements have no linear correlation with each other. This\\nis a first step toward the criterion of learning representations whose elements are\\nstatistically independent. To achieve full independence, a representation learning\\nalgorithm must also remove the nonlinear relationships between variables.\\nPCA learns an orthogonal, linear transformation of the data that projects an\\ninput x to a representation z as shown in figure 5.8. In section 2.12, we saw that\\nwe could learn a one-dimensional representation that best reconstructs the original\\ndata (in the sense of mean squared error) and that this representation actually'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='corresponds to the first principal component of the data. Thus we can use PCA\\nas a simple and effective dimensionality reduction method that preserves as much\\nof the information in the data as possible (again, as measured by least-squares\\nreconstruction error). In the following, we will study how the PCA representation\\ndecorrelates the original data representation X.\\nLet us consider the m n-dimensional design matrix X. We will assume that\\n×\\nE\\nthe data has a mean of zero, [x] = 0. If this is not the case, the data can easily\\nbe centered by subtracting the mean from all examples in a preprocessing step.\\nThe unbiased sample covariance matrix associated with X is given by:\\n1\\nVar[x] = X X. (5.85)\\n\\ue03e\\nm 1\\n−\\n148\\nCHAPTER 5. MACHINE LEARNING BASICS\\nPCA finds a representation (through linear transformation) z = x W where\\n\\ue03e\\nVar[z] is diagonal.\\nIn section 2.12, we saw that the principal components of a design matrix X\\nare given by the eigenvectors of X X. From this view,\\n\\ue03e\\nX X = WΛW . (5.86)\\n\\ue03e \\ue03e'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Inthissection,weexploitanalternativederivationoftheprincipalcomponents. The\\nprincipal components may also be obtained via the singular value decomposition.\\nSpecifically, they are the right singular vectors of X. To see this, let W be the\\nright singular vectors in the decomposition X = UΣW . We then recover the\\n\\ue03e\\noriginal eigenvector equation with W as the eigenvector basis:\\nX X = UΣW \\ue03e UΣW = WΣ2W . (5.87)\\n\\ue03e \\ue03e \\ue03e \\ue03e\\n\\ue010 \\ue011\\nThe SVD is helpful to show that PCA results in a diagonal Var[z]. Using the\\nSVD of X, we can express the variance of X as:\\n1\\nVar[x] = X X (5.88)\\n\\ue03e\\nm 1\\n−\\n1\\n= (UΣW ) UΣW (5.89)\\n\\ue03e \\ue03e \\ue03e\\nm 1\\n−\\n1\\n= WΣ U UΣW (5.90)\\n\\ue03e \\ue03e \\ue03e\\nm 1\\n−\\n1\\n= WΣ2W , (5.91)\\n\\ue03e\\nm 1\\n−\\nwhere we use the fact thatU U = I because the U matrix of the singular value\\n\\ue03e\\ndecomposition is defined to be orthogonal. This shows that if we take z = x W,\\n\\ue03e\\nwe can ensure that the covariance of z is diagonal as required:\\n1\\nVar[z] = Z Z (5.92)\\n\\ue03e\\nm 1\\n−\\n1\\n= W X XW (5.93)\\n\\ue03e \\ue03e\\nm 1\\n−\\n1\\n= W WΣ2W W (5.94)\\n\\ue03e \\ue03e\\nm 1\\n−\\n1\\n= Σ2, (5.95)\\nm 1\\n−'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='where this time we use the fact that W W = I, again from the definition of the\\n\\ue03e\\nSVD.\\n149\\nCHAPTER 5. MACHINE LEARNING BASICS\\nThe above analysis shows that when we project the data x to z, via the linear\\ntransformation W, the resulting representation has a diagonal covariance matrix\\n(as given by Σ2) which immediately implies that the individual elements of z are\\nmutually uncorrelated.\\nThis ability of PCA to transform data into a representation where the elements\\nare mutually uncorrelated is a very important property of PCA. It is a simple\\nexample of a representation that attempts to disentangle the unknown factors of\\nvariation underlying the data. In the case of PCA, this disentangling takes the\\nform of finding a rotation of the input space (described by W) that aligns the\\nprincipal axes of variance with the basis of the new representation space associated\\nwith z.\\nWhile correlation is an important category of dependency between elements of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the data, we are also interested in learning representations that disentangle more\\ncomplicated forms of feature dependencies. For this, we will need more than what\\ncan be done with a simple linear transformation.\\nk\\n5.8.2 -means Clustering\\nAnotherexample ofasimple representationlearning algorithmisk-meansclustering.\\nThe k-means clustering algorithm divides the training set into k different clusters\\nof examples that are near each other. We can thus think of the algorithm as\\nproviding a k-dimensional one-hot code vector h representing an input x. If x\\nbelongs to cluster i, then h = 1 and all other entries of the representation h are\\ni\\nzero.\\nThe one-hot code provided by k-means clustering is an example of a sparse\\nrepresentation, because the majority of its entries are zero for every input. Later,\\nwe will develop other algorithms that learn more flexible sparse representations,\\nwhere more than one entry can be non-zero for each input x. One-hot codes'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='are an extreme example of sparse representations that lose many of the benefits\\nof a distributed representation. The one-hot code still confers some statistical\\nadvantages (it naturally conveys the idea that all examples in the same cluster are\\nsimilar to each other) and it confers the computational advantage that the entire\\nrepresentation may be captured by a single integer.\\nThe k-meansalgorithmworksbyinitializingk differentcentroids µ(1),...,µ(k)\\n{ }\\nto different values, then alternating between two different steps until convergence.\\nIn one step, each training example is assigned to cluster i, where i is the index of\\nthe nearest centroid µ(i). In the other step, each centroid µ(i) is updated to the\\nmean of all training examples x(j) assigned to cluster i.\\n150\\nCHAPTER 5. MACHINE LEARNING BASICS\\nOnedifficultypertainingto clusteringis thattheclustering problemis inherently\\nill-posed, in the sense that there is no single criterion that measures how well a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='clustering of the data corresponds to the real world. We can measure properties of\\nthe clustering such as the average Euclidean distance from a cluster centroid to the\\nmembers of the cluster. This allows us to tell how well we are able to reconstruct\\nthe training data from the cluster assignments. We do not know how well the\\ncluster assignments correspond to properties of the real world. Moreover, there\\nmay be many different clusterings that all correspond well to some property of\\nthe real world. We may hope to find a clustering that relates to one feature but\\nobtain a different, equally valid clustering that is not relevant to our task. For\\nexample, suppose that we run two clustering algorithms on a dataset consisting of\\nimages of red trucks, images of red cars, images of gray trucks, and images of gray\\ncars. If we ask each clustering algorithm to find two clusters, one algorithm may\\nfind a cluster of cars and a cluster of trucks, while another may find a cluster of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='red vehicles and a cluster of gray vehicles. Suppose we also run a third clustering\\nalgorithm, which is allowed to determine the number of clusters. This may assign\\nthe examples to four clusters, red cars, red trucks, gray cars, and gray trucks. This\\nnew clustering now at least captures information about both attributes, but it has\\nlost information about similarity. Red cars are in a different cluster from gray\\ncars, just as they are in a different cluster from gray trucks. The output of the\\nclustering algorithm does not tell us that red cars are more similar to gray cars\\nthan they are to gray trucks. They are different from both things, and that is all\\nwe know.\\nThese issues illustrate some of the reasons that we may prefer a distributed\\nrepresentation to a one-hot representation. A distributed representation could have\\ntwo attributes for each vehicle—one representing its color and one representing\\nwhether it is a car or a truck. It is still not entirely clear what the optimal'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='distributed representation is (how can the learning algorithm know whether the\\ntwo attributes we are interested in are color and car-versus-truck rather than\\nmanufacturer and age?) but having many attributes reduces the burden on the\\nalgorithm to guess which single attribute we care about, and allows us to measure\\nsimilarity between objects in a fine-grained way by comparing many attributes\\ninstead of just testing whether one attribute matches.\\n5.9 Stochastic Gradient Descent\\nNearly all of deep learning is powered by one very importantalgorithm: stochastic\\ngradient descent or SGD. Stochastic gradient descent is an extension of the\\n151\\nCHAPTER 5. MACHINE LEARNING BASICS\\ngradient descent algorithm introduced in section 4.3.\\nA recurring problem in machine learning is that large training sets are necessary\\nfor good generalization, but large training sets are also more computationally\\nexpensive.\\nThe cost function used by a machine learning algorithm often decomposes as a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='sum over training examples of some per-example loss function. For example, the\\nnegative conditional log-likelihood of the training data can be written as\\nm\\n1\\nJ(θ) = E L(x,y,θ) = L(x(i),y(i),θ) (5.96)\\nx,y pˆ\\n∼ data m\\ni=1\\n\\ue058\\nwhere L is the per-example loss L(x,y,θ) = logp(y x;θ).\\n− |\\nFor these additive cost functions, gradient descent requires computing\\nm\\n1\\nJ(θ) = L(x(i),y(i),θ). (5.97)\\nθ θ\\n∇ m ∇\\ni=1\\n\\ue058\\nThe computational cost of this operation isO(m). As the training set size grows to\\nbillions of examples, the time to take a single gradient step becomes prohibitively\\nlong.\\nThe insight of stochastic gradient descent is that the gradient is an expectation.\\nThe expectation may be approximately estimated using a small set of samples.\\nSpecifically, on each step of the algorithm, we can sample a minibatch of examples\\nB = x(1),...,x(m \\ue030) drawn uniformly from the training set. The minibatch size\\n{ }\\nm is typically chosen to be a relatively small number of examples, ranging from\\n\\ue030'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='1 to a few hundred. Crucially, m is usually held fixed as the training set size m\\n\\ue030\\ngrows. We may fit a training set with billions of examples using updates computed\\non only a hundred examples.\\nThe estimate of the gradient is formed as\\nm\\n1 \\ue030\\ng = L(x(i),y(i),θ). (5.98)\\nθ\\nm ∇\\n\\ue030\\ni=1\\n\\ue058\\nB\\nusing examples from the minibatch . The stochastic gradient descent algorithm\\nthen follows the estimated gradient downhill:\\nθ θ \\ue00fg, (5.99)\\n← −\\nwhere \\ue00f is the learning rate.\\n152\\nCHAPTER 5. MACHINE LEARNING BASICS\\nGradient descent in general has often been regarded as slow or unreliable. In\\nthe past, the application of gradient descent to non-convex optimization problems\\nwas regarded as foolhardy or unprincipled. Today, we know that the machine\\nlearning models described in part II work very well when trained with gradient\\ndescent. The optimization algorithm may not be guaranteed to arrive at even a\\nlocal minimum in a reasonable amount of time, but it often finds a very low value'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of the cost function quickly enough to be useful.\\nStochastic gradient descent has many important uses outside the context of\\ndeep learning. It is the main way to train large linear models on very large\\ndatasets. For a fixed model size, the cost per SGD update does not depend on the\\ntraining set size m. In practice, we often use a larger model as the training set size\\nincreases, but we are not forced to do so. The number of updates required to reach\\nconvergence usually increases with training set size. However, as m approaches\\ninfinity, the model will eventually converge to its best possible test error before\\nSGD has sampled every example in the training set. Increasing m further will not\\nextend the amount of training time needed to reach the model’s best possible test\\nerror. From this point of view, one can argue that the asymptotic cost of training\\na model with SGD is O(1) as a function of m.\\nPrior to the advent of deep learning, the main way to learn nonlinear models'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='was touse the kerneltrickin combination with a linearmodel. Many kernel learning\\nalgorithms require constructing an m mmatrix G = k(x(i),x(j)). Constructing\\ni,j\\n×\\nthis matrix has computational cost O(m2), which is clearly undesirable for datasets\\nwith billions of examples. In academia, starting in 2006, deep learning was\\ninitially interesting because it was able to generalize to new examples better\\nthan competing algorithms when trained on medium-sized datasets with tens of\\nthousands of examples. Soon after, deep learning garnered additional interest in\\nindustry, because it provided a scalable way of training nonlinear models on large\\ndatasets.\\nStochastic gradient descent and many enhancements to it are described further\\nin chapter 8.\\n5.10 Building a Machine Learning Algorithm\\nNearly all deep learning algorithms can be described as particular instances of\\na fairly simple recipe: combine a specification of a dataset, a cost function, an\\noptimization procedure and a model.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='For example, the linear regression algorithm combines a dataset consisting of\\n153\\nCHAPTER 5. MACHINE LEARNING BASICS\\nX and y, the cost function\\nE\\nJ(w,b) = logp (y x), (5.100)\\nx,y pˆ model\\n− ∼ data |\\nthe model specification p (y x) = (y;x w+ b,1), and, in most cases, the\\nmodel \\ue03e\\n| N\\noptimization algorithm defined by solving for where the gradient of the cost is zero\\nusing the normal equations.\\nBy realizing that we can replace any of these components mostly independently\\nfrom the others, we can obtain a very wide variety of algorithms.\\nThe cost function typically includes at least one term that causes the learning\\nprocess to perform statistical estimation. The most common cost function is the\\nnegative log-likelihood, so that minimizing the cost function causes maximum\\nlikelihood estimation.\\nThe cost function may also include additional terms, such as regularization\\nterms. For example, we can add weight decay to the linear regression cost function\\nto obtain'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='J(w,b) = λ w 2 E logp (y x). (5.101)\\n|| ||2 − x,y ∼pˆ data model |\\nThis still allows closed-form optimization.\\nIf we change the model to be nonlinear, then most cost functions can no longer\\nbe optimized in closed form. This requires us to choose an iterative numerical\\noptimization procedure, such as gradient descent.\\nTherecipeforconstructingalearning algorithmbycombiningmodels, costs, and\\noptimization algorithms supports both supervised and unsupervised learning. The\\nlinear regression example shows how to support supervised learning. Unsupervised\\nlearning can be supported by defining a dataset that contains onlyX and providing\\nan appropriate unsupervised cost and model. For example, we can obtain the first\\nPCA vector by specifying that our loss function is\\nJ(w) = E x r(x;w) 2 (5.102)\\nx ∼pˆ data|| − ||2\\nwhile our model is defined to have w with norm one and reconstruction function\\nr(x) = w xw.\\n\\ue03e\\nIn some cases, the cost function may be a function that we cannot actually'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='evaluate, for computational reasons. In these cases, we can still approximately\\nminimize it using iterative numerical optimization so long as we have some way of\\napproximating its gradients.\\nMost machine learning algorithms make use of this recipe, though it may not\\nimmediately be obvious. If a machine learning algorithm seems especially unique or\\n154\\nCHAPTER 5. MACHINE LEARNING BASICS\\nhand-designed, it can usually be understood as using a special-case optimizer. Some\\nmodels such as decision trees or k-means require special-case optimizers because\\ntheir costfunctions have flat regions that makethem inappropriate for minimization\\nby gradient-based optimizers. Recognizing that most machine learning algorithms\\ncan be described using this recipe helps to see the different algorithms as part of a\\ntaxonomy of methods for doing related tasks that work for similar reasons, rather\\nthan as a long list of algorithms that each have separate justifications.\\n5.11 Challenges Motivating Deep Learning'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The simple machine learning algorithms described in this chapter work very well on\\na wide variety of important problems. However, they have not succeeded in solving\\nthe central problems in AI, such as recognizing speech or recognizing objects.\\nThe development of deep learning was motivated in part by the failure of\\ntraditional algorithms to generalize well on such AI tasks.\\nThis section is about how the challenge ofgeneralizing to new examples becomes\\nexponentially more difficult when working with high-dimensional data, and how\\nthe mechanisms used to achieve generalization in traditional machine learning\\nare insufficient to learn complicated functions in high-dimensional spaces. Such\\nspaces also often impose high computational costs. Deep learning was designed to\\novercome these and other obstacles.\\n5.11.1 The Curse of Dimensionality\\nMany machine learning problems become exceedingly difficult when the number\\nof dimensions in the data is high. This phenomenon is known as the curse of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='dimensionality. Of particular concern is that the number of possible distinct\\nconfigurationsof a set ofvariablesincreases exponentiallyasthe number ofvariables\\nincreases.\\n155\\nCHAPTER 5. MACHINE LEARNING BASICS\\nFigure 5.9: As the number of relevant dimensions of the data increases (from left to\\nright), the number of configurations of interest may grow exponentially. (Left)In this\\none-dimensional example, we have one variable for which we only care to distinguish 10\\nregions of interest. With enough examples falling within each of these regions (each region\\ncorresponds to a cell in the illustration), learning algorithms can easily generalize correctly.\\nA straightforward way to generalize is to estimate the value of the target function within\\neach region (and possibly interpolate between neighboring regions). (Center)With 2\\ndimensions it is more difficult to distinguish 10 different values of each variable. We need'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to keep track of up to 10 10=100 regions, and we need at least that many examples to\\n×\\ncover all those regions. (Right)With 3 dimensions this grows to 103 = 1000 regions and at\\nleast that many examples. For d dimensions and v values to be distinguished along each\\naxis, we seem to need O(vd) regions and examples. This is an instance of the curse of\\ndimensionality. Figure graciously provided by Nicolas Chapados.\\nThe curse of dimensionality arises in many places in computer science, and\\nespecially so in machine learning.\\nOne challenge posed by the curse of dimensionality is a statistical challenge.\\nAs illustrated in figure 5.9, a statistical challenge arises because the number of\\npossible configurations of x is much larger than the number of training examples.\\nTo understand the issue, let us consider that the input space is organized into a\\ngrid, like in the figure. We can describe low-dimensional space with a low number'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of grid cells that are mostly occupied by the data. When generalizing to a new data\\npoint, we can usually tell what to do simply by inspecting the training examples\\nthat lie in the same cell as the new input. For example, if estimating the probability\\ndensity at some point x, we can just return the number of training examples in\\nthe same unit volume cell as x, divided by the total number of training examples.\\nIf we wish to classify an example, we can return the most common class of training\\nexamples in the same cell. If we are doing regression we can average the target\\nvalues observed over the examples in that cell. But what about the cells for which\\nwe have seen no example? Because in high-dimensional spaces the number of\\nconfigurations is huge, much larger than our number of examples, a typical grid cell\\nhas no training example associated with it. How could we possibly say something\\n156\\nCHAPTER 5. MACHINE LEARNING BASICS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='meaningful about these new configurations? Many traditional machine learning\\nalgorithms simply assume that the output at a new point should be approximately\\nthe same as the output at the nearest training point.\\n5.11.2 Local Constancy and Smoothness Regularization\\nIn order to generalize well, machine learning algorithms need to be guided by prior\\nbeliefs about what kind of function they should learn. Previously, we have seen\\nthese priors incorporated as explicit beliefs in the form of probability distributions\\nover parameters of the model. More informally, we may also discuss prior beliefs as\\ndirectly influencing the function itself and only indirectly acting on the parameters\\nvia their effect on the function. Additionally, we informally discuss prior beliefs as\\nbeing expressed implicitly, by choosing algorithms that are biased toward choosing\\nsome class of functions over another, even though these biases may not be expressed'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='(or even possible to express) in terms of a probability distribution representing our\\ndegree of belief in various functions.\\nAmong the most widely used of these implicit “priors” is the smoothness\\nprior or local constancy prior. This prior states that the function we learn\\nshould not change very much within a small region.\\nMany simpler algorithms rely exclusively on this prior to generalize well, and\\nas a result they fail to scale to the statistical challenges involved in solving AI-\\nlevel tasks. Throughout this book, we will describe how deep learning introduces\\nadditional (explicit and implicit) priors in order to reduce the generalization\\nerror on sophisticated tasks. Here, we explain why the smoothness prior alone is\\ninsufficient for these tasks.\\nThere are many different ways to implicitly or explicitly express a prior belief\\nthat the learnedfunction should be smooth or locally constant. All of these different'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='methods are designed to encourage the learning process to learn a function f that\\n∗\\nsatisfies the condition\\nf (x) f (x +\\ue00f) (5.103)\\n∗ ∗\\n≈\\nfor most configurations x and small change \\ue00f. In other words, if we know a good\\nanswer for an input x (for example, if x is a labeled training example) then that\\nanswer is probably good in the neighborhood of x. If we have several good answers\\nin some neighborhood we would combine them (by some form of averaging or\\ninterpolation) to produce an answer that agrees with as many of them as much as\\npossible.\\nAn extreme example of the local constancy approach is the k-nearest neighbors\\nfamily of learning algorithms. These predictors are literally constant over each\\n157\\nCHAPTER 5. MACHINE LEARNING BASICS\\nregion containing all the points x that have the same set of k nearest neighbors in\\nthe training set. For k = 1, the number of distinguishable regions cannot be more\\nthan the number of training examples.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='While the k-nearest neighbors algorithm copies the output from nearby training\\nexamples, most kernel machines interpolate between training set outputs associated\\nwith nearby training examples. An important class of kernels is the family of local\\nkernels where k(u,v) is large when u =v and decreases as u and v grow farther\\napart from each other. A local kernel can be thought of as a similarity function\\nthat performs template matching, by measuring how closely a test example x\\nresembles each training example x(i). Much of the modern motivation for deep\\nlearning is derived from studying the limitations of local template matching and\\nhow deep models are able to succeed in cases where local template matching fails\\n(Bengio et al., 2006b).\\nDecision trees also suffer from the limitations of exclusively smoothness-based\\nlearning because they break the input space into as many regions as there are\\nleaves and use a separate parameter (or sometimes many parameters for extensions'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of decision trees) in each region. If the target function requires a tree with at\\nleast n leaves to be represented accurately, then at least n training examples are\\nrequired to fit the tree. A multiple of n is needed to achieve some level of statistical\\nconfidence in the predicted output.\\nIn general, to distinguish O(k) regions in input space, all of these methods\\nrequire O(k) examples. Typically there are O(k) parameters, with O(1) parameters\\nassociated with each of the O(k) regions. The case of a nearest neighbor scenario,\\nwhere each training example can be used to define at most one region, is illustrated\\nin figure 5.10.\\nIs there a way to represent a complex function that has many more regions\\nto be distinguished than the number of training examples? Clearly, assuming\\nonly smoothness of the underlying function will not allow a learner to do that.\\nFor example, imagine that the target function is a kind of checkerboard. A'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='checkerboard contains many variations but there is a simple structure to them.\\nImagine what happens when the number of training examples is substantially\\nsmaller than the number of black and white squares on the checkerboard. Based\\non only local generalization and the smoothness or local constancy prior, we would\\nbe guaranteed to correctly guess the color of a new point if it lies within the same\\ncheckerboard square as a training example. There is no guarantee that the learner\\ncould correctly extend the checkerboard pattern to points lying in squares that do\\nnot contain training examples. With this prior alone, the only information that an\\nexample tells us is the color of its square, and the only way to get the colors of the\\n158\\nCHAPTER 5. MACHINE LEARNING BASICS\\nFigure 5.10: Illustration of how the nearest neighbor algorithm breaks up the input space\\ninto regions. An example (represented here by a circle) within each region defines the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='region boundary (representedhere by the lines). They value associated with each example\\ndefines what the output should be for all points within the corresponding region. The\\nregions defined by nearest neighbor matching form a geometric pattern called a Voronoi\\ndiagram. The number of these contiguous regions cannot grow faster than the number\\nof training examples. While this figure illustrates the behavior of the nearest neighbor\\nalgorithm specifically, other machine learning algorithms that rely exclusively on the\\nlocal smoothness prior for generalization exhibit similar behaviors: each training example\\nonly informs the learner about how to generalize in some neighborhood immediately\\nsurrounding that example.\\n159\\nCHAPTER 5. MACHINE LEARNING BASICS\\nentire checkerboard right is to cover each of its cells with at least one example.\\nThe smoothness assumption and the associated non-parametric learning algo-\\nrithms work extremely well so long as there are enough examples for the learning'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='algorithm to observe high points on most peaks and low points on most valleys\\nof the true underlying function to be learned. This is generally true when the\\nfunction to be learned is smooth enough and varies in few enough dimensions.\\nIn high dimensions, even a very smooth function can change smoothly but in a\\ndifferent way along each dimension. If the function additionally behaves differently\\nin different regions, it can become extremely complicated to describe with a set of\\ntraining examples. If the function is complicated (we want to distinguish a huge\\nnumber of regions compared to the number of examples), is there any hope to\\ngeneralize well?\\nThe answer to both of these questions—whether it is possible to represent\\na complicated function efficiently, and whether it is possible for the estimated\\nfunction to generalize well to new inputs—is yes. The key insight is that a very\\nlarge number of regions, e.g., O(2k), can be defined with O(k) examples, so long'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='as we introduce some dependencies between the regions via additional assumptions\\nabout the underlying data generating distribution. In this way, we can actually\\ngeneralize non-locally (Bengio and Monperrus, 2005; Bengio et al., 2006c). Many\\ndifferent deep learning algorithms provide implicit or explicit assumptions that are\\nreasonable for a broad range of AI tasks in order to capture these advantages.\\nOther approaches to machine learning often make stronger, task-specific as-\\nsumptions. For example, we could easily solve the checkerboard task by providing\\nthe assumption that the target function is periodic. Usually we do not include such\\nstrong, task-specific assumptions into neural networks so that they can generalize\\nto a much wider variety of structures. AI tasks have structure that is much too\\ncomplex to be limited to simple, manually specified properties such as periodicity,\\nso we want learning algorithms that embody more general-purpose assumptions.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The core idea in deep learning is that we assume that the data was generated by\\nthe composition of factors or features, potentially at multiple levels in a hierarchy.\\nMany other similarly generic assumptions can further improve deep learning al-\\ngorithms. These apparently mild assumptions allow an exponential gain in the\\nrelationship between the number of examples and the number of regions that can\\nbe distinguished. These exponential gains are described more precisely in sections\\n6.4.1, 15.4 and 15.5. The exponential advantages conferred by the use of deep,\\ndistributed representations counter the exponential challenges posed by the curse\\nof dimensionality.\\n160\\nCHAPTER 5. MACHINE LEARNING BASICS\\n5.11.3 Manifold Learning\\nAn important concept underlying many ideas in machine learning is that of a\\nmanifold.\\nA manifold is a connected region. Mathematically, it is a set of points,\\nassociated with a neighborhood around each point. From any given point, the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='manifold locally appears to be a Euclidean space. In everyday life, we experience\\nthe surface of the world as a 2-D plane, but it is in fact a spherical manifold in\\n3-D space.\\nThe definition of a neighborhood surrounding each point implies the existence\\nof transformations that can be applied to move on the manifold from one position\\nto a neighboring one. In the example of the world’s surface as a manifold, one can\\nwalk north, south, east, or west.\\nAlthough there is a formal mathematical meaning to the term “manifold,” in\\nmachine learning it tends to be used more loosely to designate a connected set\\nof points that can be approximated well by considering only a small number of\\ndegrees of freedom, or dimensions, embedded in a higher-dimensional space. Each\\ndimension corresponds to a local direction of variation. See figure 5.11 for an\\nexample of training data lying near a one-dimensional manifold embedded in two-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='dimensional space. In the context of machine learning, we allow the dimensionality\\nof the manifold to vary from one point to another. This often happens when a\\nmanifold intersects itself. For example, a figure eight is a manifold that has a single\\ndimension in most places but two dimensions at the intersection at the center.\\n2.5\\n2.0\\n1.5\\n1.0\\n0.5\\n0.0\\n0.5\\n−\\n1.0\\n− 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\\nFigure 5.11: Data sampled from a distribution in a two-dimensional space that is actually\\nconcentratednearaone-dimensional manifold,likeatwistedstring. The solidlineindicates\\nthe underlying manifold that the learner should infer.\\n161\\nCHAPTER 5. MACHINE LEARNING BASICS\\nMany machine learning problems seem hopeless if we expect the machine\\nlearning algorithm to learn functions with interesting variations across all of\\nRn.\\nManifold learning algorithms surmount this obstacle by assuming that most\\nof\\nRn\\nconsists of invalid inputs, and that interesting inputs occur only along'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a collection of manifolds containing a small subset of points, with interesting\\nvariations in the output of the learned function occurring only along directions\\nthat lie on the manifold, or with interesting variations happening only when we\\nmove from one manifold to another. Manifold learning was introduced in the case\\nof continuous-valued data and the unsupervised learning setting, although this\\nprobability concentration idea can be generalized to both discrete data and the\\nsupervised learning setting: the key assumption remains that probability mass is\\nhighly concentrated.\\nThe assumption that the data lies along a low-dimensional manifold may not\\nalways be correct or useful. We argue that in the context of AI tasks, such as\\nthose that involve processing images, sounds, or text, the manifold assumption is\\nat least approximately correct. The evidence in favor of this assumption consists\\nof two categories of observations.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The first observation in favor of the manifold hypothesis is that the proba-\\nbility distribution over images, text strings, and sounds that occur in real life is\\nhighly concentrated. Uniform noise essentially never resembles structured inputs\\nfrom these domains. Figure 5.12 shows how, instead, uniformly sampled points\\nlook like the patterns of static that appear on analog television sets when no signal\\nis available. Similarly, if you generate a document by picking letters uniformly at\\nrandom, what is the probability that you will get a meaningful English-language\\ntext? Almost zero, again, because most of the long sequences of letters do not\\ncorrespond to a natural language sequence: the distribution of natural language\\nsequences occupies a very small volume in the total space of sequences of letters.\\n162\\nCHAPTER 5. MACHINE LEARNING BASICS\\nFigure 5.12: Sampling images uniformly at random (by randomly picking each pixel'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='according to a uniform distribution) gives rise to noisy images. Although there is a non-\\nzero probability to generate an image of a face or any other object frequently encountered\\nin AI applications, we never actually observe this happening in practice. This suggests\\nthat the images encountered in AI applications occupy a negligible proportion of the\\nvolume of image space.\\nOf course, concentrated probability distributions are not sufficient to show\\nthat the data lies on a reasonably small number of manifolds. We must also\\nestablish that the examples we encounter are connected to each other by other\\n163\\nCHAPTER 5. MACHINE LEARNING BASICS\\nexamples, with each example surrounded by other highly similar examples that\\nmay be reached by applying transformations to traverse the manifold. The second\\nargument in favor of the manifold hypothesis is that we can also imagine such\\nneighborhoods and transformations, at least informally. In the case of images, we'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='can certainly think of many possible transformations that allow us to trace out a\\nmanifold in image space: we can gradually dim or brighten the lights, gradually\\nmove or rotate objects in the image, gradually alter the colors on the surfaces of\\nobjects, etc. It remains likely that there are multiple manifolds involved in most\\napplications. For example, the manifold of images of human faces may not be\\nconnected to the manifold of images of cat faces.\\nThese thought experiments supporting the manifold hypotheses convey some in-\\ntuitivereasons supporting it. More rigorousexperiments (Cayton,2005; Narayanan\\nand Mitter, 2010; Schölkopf et al., 1998; Roweis and Saul, 2000; Tenenbaum et al.,\\n2000; Brand, 2003; Belkin and Niyogi, 2003; Donoho and Grimes, 2003; Weinberger\\nand Saul, 2004) clearly support the hypothesis for a large class of datasets of\\ninterest in AI.\\nWhen the data lies on a low-dimensional manifold, it can be most natural'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='for machine learning algorithms to represent the data in terms of coordinates on\\nthe manifold, rather than in terms of coordinates in Rn. In everyday life, we can\\nthink of roads as 1-D manifolds embedded in 3-D space. We give directions to\\nspecific addresses in terms of address numbers along these 1-D roads, not in terms\\nof coordinates in 3-D space. Extracting these manifold coordinates is challenging,\\nbut holds the promise to improve many machine learning algorithms. This general\\nprinciple is applied in many contexts. Figure 5.13 shows the manifold structure of\\na dataset consisting of faces. By the end of this book, we will have developed the\\nmethods necessary to learn such a manifold structure. In figure 20.6, we will see\\nhow a machine learning algorithm can successfully accomplish this goal.\\nThis concludes part I, which has provided the basic concepts in mathematics\\nand machine learning which are employed throughout the remaining parts of the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='book. You are now prepared to embark upon your study of deep learning.\\n164\\nCHAPTER 5. MACHINE LEARNING BASICS\\nFigure5.13: TrainingexamplesfromtheQMULMultiviewFaceDataset(Gonget al., 2000)\\nfor which the subjects were asked to move in such a way as to cover the two-dimensional\\nmanifold corresponding to two angles of rotation. We would like learning algorithms to be\\nable to discover and disentangle such manifold coordinates. Figure 20.6 illustrates such a\\nfeat.\\n165\\nPart II\\nDeep Networks: Modern\\nPractices\\n166\\nThis part of the book summarizes the state of modern deep learning as it is\\nused to solve practical applications.\\nDeep learning has a long history and many aspirations. Several approaches\\nhave been proposed that have yet to entirely bear fruit. Several ambitious goals\\nhave yet to be realized. These less-developed branches of deep learning appear in\\nthe final part of the book.\\nThis part focuses only on those approaches that are essentially working tech-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='nologies that are already used heavily in industry.\\nModern deep learning provides a very powerful framework for supervised\\nlearning. By adding more layers and more units within a layer, a deep network can\\nrepresent functions of increasing complexity. Most tasks that consist of mapping an\\ninput vector to an output vector, and that are easy for a person to do rapidly, can\\nbe accomplished via deep learning, given sufficiently large models and sufficiently\\nlarge datasets of labeled training examples. Other tasks, that can not be described\\nas associating one vector to another, or that are difficult enough that a person\\nwould require time to think and reflect in order to accomplish the task, remain\\nbeyond the scope of deep learning for now.\\nThis part of the book describes the core parametric function approximation\\ntechnology that is behind nearly all modern practical applications of deep learning.\\nWe begin by describing the feedforward deep network model that is used to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='represent these functions. Next, we present advanced techniques for regularization\\nand optimization of such models. Scaling these models to large inputs such as high\\nresolution images or long temporal sequences requires specialization. We introduce\\nthe convolutional network for scaling to large images and the recurrent neural\\nnetwork for processing temporal sequences. Finally, we present general guidelines\\nfor the practical methodology involved in designing, building, and configuring an\\napplication involving deep learning, and review some of the applications of deep\\nlearning.\\nThese chapters are the most important for a practitioner—someone who wants\\nto begin implementing and using deep learning algorithms to solve real-world\\nproblems today.\\n167\\nChapter 6\\nDeep Feedforward Networks\\nDeep feedforward networks, also often called feedforward neural networks,\\nor multilayer perceptrons (MLPs), are the quintessential deep learning models.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The goal of a feedforward network is to approximate some function f . For example,\\n∗\\nfor a classifier, y = f (x) maps an input x to a category y. A feedforward network\\n∗\\ndefines a mapping y = f(x;θ) and learns the value of the parameters θ that result\\nin the best function approximation.\\nThese models are called feedforward because information flows through the\\nfunction being evaluated from x, through the intermediate computations used to\\ndefine f, and finally to the output y. There are no feedback connections in which\\noutputs of the model are fed back into itself. When feedforward neural networks\\nare extended to include feedback connections, they are called recurrent neural\\nnetworks, presented in chapter 10.\\nFeedforward networks are of extreme importance to machine learning practi-\\ntioners. They form the basis of many important commercial applications. For\\nexample, the convolutional networks used for object recognition from photos are a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='specialized kind of feedforward network. Feedforward networks are a conceptual\\nstepping stone on the path to recurrent networks, which power many natural\\nlanguage applications.\\nFeedforward neural networks are called networks because they are typically\\nrepresented by composing together many different functions. The model is asso-\\nciated with a directed acyclic graph describing how the functions are composed\\ntogether. For example, we might have three functions f(1), f(2), and f(3) connected\\nin a chain, to form f(x) = f(3)(f(2)(f(1)(x))). These chain structures are the most\\ncommonly used structures of neural networks. In this case, f(1) is called the first\\nlayer of the network, f(2) is called the second layer, and so on. The overall\\n168\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nlength of the chain gives the depth of the model. It is from this terminology that\\nthe name “deep learning” arises. The final layer of a feedforward network is called'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the output layer. During neural network training, we drive f(x) to match f (x).\\n∗\\nThe training data provides us with noisy, approximate examples of f (x) evaluated\\n∗\\nat different training points. Each example x is accompanied by a label y f (x).\\n∗\\n≈\\nThe training examples specify directly what the output layer must do at each point\\nx; it must produce a value that is close to y. The behavior of the other layers is\\nnot directly specified by the training data. The learning algorithm must decide\\nhow to use those layers to produce the desired output, but the training data does\\nnot say what each individual layer should do. Instead, the learning algorithm must\\ndecide how to use these layers to best implement an approximation of f . Because\\n∗\\nthe training data does not show the desired output for each of these layers, these\\nlayers are called hidden layers.\\nFinally, these networks are called neural because they are loosely inspired by'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='neuroscience. Each hidden layer of the network is typically vector-valued. The\\ndimensionality of these hidden layers determines the width of the model. Each\\nelement of the vector may be interpreted as playing a role analogous to a neuron.\\nRather than thinking of the layer as representing a single vector-to-vector function,\\nwe can also think of the layer as consisting of many units that act in parallel,\\neach representing a vector-to-scalar function. Each unit resembles a neuron in\\nthe sense that it receives input from many other units and computes its own\\nactivation value. The idea of using many layers of vector-valued representation\\nis drawn from neuroscience. The choice of the functions f(i)(x) used to compute\\nthese representations is also loosely guided by neuroscientific observations about\\nthe functions that biological neurons compute. However, modern neural network\\nresearch is guided by many mathematical and engineering disciplines, and the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='goal of neural networks is not to perfectly model the brain. It is best to think of\\nfeedforward networks as function approximation machines that are designed to\\nachieve statistical generalization, occasionally drawing some insights from what we\\nknow about the brain, rather than as models of brain function.\\nOne way to understand feedforward networks is to begin with linear models\\nand consider how to overcome their limitations. Linear models, such as logistic\\nregression and linear regression, are appealing because they may be fit efficiently\\nand reliably, either in closed form or with convex optimization. Linear models also\\nhave the obvious defect that the model capacity is limited to linear functions, so\\nthe model cannot understand the interaction between any two input variables.\\nTo extend linear models to represent nonlinear functions of x, we can apply\\nthe linear model not to x itself but to a transformed input φ(x), where φ is a\\n169\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='nonlinear transformation. Equivalently, we can apply the kernel trick described in\\nsection 5.7.2, to obtain a nonlinear learning algorithm based on implicitly applying\\nthe φ mapping. We can think of φ as providing a set of features describing x, or\\nas providing a new representation for x.\\nThe question is then how to choose the mapping φ.\\n1. One option is to use a very generic φ, such as the infinite-dimensional φ that\\nis implicitly used by kernel machines based on the RBF kernel. If φ(x) is\\nof high enough dimension, we can always have enough capacity to fit the\\ntraining set, but generalization to the test set often remains poor. Very\\ngeneric feature mappings are usually based only on the principle of local\\nsmoothness and do not encode enough prior information to solve advanced\\nproblems.\\n2. Another option is to manually engineer φ. Until the advent of deep learning,\\nthis was the dominant approach. This approach requires decades of human'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='effort for each separate task, with practitioners specializing in different\\ndomains such as speech recognition or computer vision, and with little\\ntransfer between domains.\\n3. The strategy of deep learning is to learn φ. In this approach, we have a model\\ny = f(x;θ,w) = φ(x;θ) w. We now have parameters θthat we use to learn\\n\\ue03e\\nφ from a broad class of functions, and parameters w that map from φ(x) to\\nthe desired output. This is an example of a deep feedforward network, with\\nφ defining a hidden layer. This approach is the only one of the three that\\ngives up on the convexity of the training problem, but the benefits outweigh\\nthe harms. In this approach, we parametrize the representation as φ(x;θ)\\nand use the optimization algorithm to find the θ that corresponds to a good\\nrepresentation. If we wish, this approach can capture the benefit of the first\\napproach by being highly generic—we do so by using a very broad family'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='φ(x;θ). This approach can also capture the benefit of the second approach.\\nHuman practitioners can encode their knowledge to help generalization by\\ndesigning families φ(x; θ) that they expect will perform well. The advantage\\nis that the human designer only needs to find the right general function\\nfamily rather than finding precisely the right function.\\nThis general principle of improving models by learning features extends beyond\\nthe feedforward networks described in this chapter. It is a recurring theme of deep\\nlearning that applies to all of the kinds of models described throughout this book.\\nFeedforward networks are the application of this principle to learning deterministic\\n170\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nmappings from x to y that lack feedback connections. Other models presented\\nlater will apply these principles to learning stochastic mappings, learning functions\\nwith feedback, and learning probability distributions over a single vector.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='We begin this chapter with a simple example of a feedforward network. Next,\\nwe address each of the design decisions needed to deploy a feedforward network.\\nFirst, training a feedforward network requires making many of the same design\\ndecisions as are necessary for a linear model: choosing the optimizer, the cost\\nfunction, and the form of the output units. We review these basics of gradient-based\\nlearning, then proceed to confront some of the design decisions that are unique\\nto feedforward networks. Feedforward networks have introduced the concept of a\\nhidden layer, and this requires us to choose the activation functions that will\\nbe used to compute the hidden layer values. We must also design the architecture\\nof the network, including how many layers the network should contain, how these\\nlayers should be connected to each other, and how many units should be in\\neach layer. Learning in deep neural networks requires computing the gradients'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of complicated functions. We present the back-propagation algorithm and its\\nmodern generalizations, which can be used to efficiently compute these gradients.\\nFinally, we close with some historical perspective.\\n6.1 Example: Learning XOR\\nTo make the idea of a feedforward network more concrete, we begin with an\\nexample of a fully functioning feedforward network on a very simple task: learning\\nthe XOR function.\\nThe XOR function (“exclusive or”) is an operation on two binary values, x\\n1\\nand x . When exactly one of these binary values is equal to 1, the XOR function\\n2\\nreturns 1. Otherwise, it returns 0. The XOR function provides the target function\\ny = f (x) that we want to learn. Our model provides a function y =f(x;θ) and\\n∗\\nour learning algorithm will adapt the parameters θ to make f as similar as possible\\nto f .\\n∗\\nIn this simple example, we will not be concerned with statistical generalization.\\nX\\nWe want our network to perform correctly on the four points = [0,0] , [0,1] ,\\n\\ue03e \\ue03e\\n{'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='[1,0] , and [1,1] . We will train the network on all four of these points. The\\n\\ue03e \\ue03e\\n}\\nonly challenge is to fit the training set.\\nWe can treat this problem as a regression problem and use a mean squared\\nerror loss function. We choose this loss function to simplify the math for this\\nexample as much as possible. In practical applications, MSE is usually not an\\n171\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nappropriate cost function for modeling binary data. More appropriate approaches\\nare described in section 6.2.2.2.\\nEvaluated on our whole training set, the MSE loss function is\\n1\\n2\\nJ(θ) = (f (x) f(x;θ)) . (6.1)\\n∗\\n4 −\\nx X\\n\\ue058∈\\nNow we must choose the form of our model, f(x;θ). Suppose that we choose\\na linear model, with θ consisting of w and b. Our model is defined to be\\nf(x;w,b) = x w +b. (6.2)\\n\\ue03e\\nWe can minimize J(θ) in closed form with respect to w and b using the normal\\nequations.\\nAfter solving the normal equations, we obtain w = 0 and b = 1. The linear\\n2'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='model simply outputs 0.5 everywhere. Why does this happen? Figure 6.1 shows\\nhow a linear model is not able to represent the XOR function. One way to solve\\nthis problem is to use a model that learns a different feature space in which a\\nlinear model is able to represent the solution.\\nSpecifically, we will introduce a very simple feedforward network with one\\nhidden layer containing two hidden units. See figure 6.2 for an illustration of\\nthis model. This feedforward network has a vector of hidden units h that are\\ncomputed by a function f(1)(x; W,c). The values of these hidden units are then\\nused as the input for a second layer. The second layer is the output layer of the\\nnetwork. The output layer is still just a linear regression model, but now it is\\napplied to h rather than to x. The network now contains two functions chained\\ntogether: h = f(1)(x;W,c) and y = f(2)(h;w,b), with the complete model being\\nf(x;W,c,w,b) = f(2)(f(1)(x)).'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='What function should f(1) compute? Linear models have served us well so far,\\nand it may be tempting to make f(1) be linear as well. Unfortunately, if f(1) were\\nlinear, then the feedforward network as a whole would remain a linear function of\\nits input. Ignoring the intercept terms for the moment, suppose f(1)(x) = W x\\n\\ue03e\\nand f(2)(h) = h w. Then f(x) = w W x. We could represent this function as\\n\\ue03e \\ue03e \\ue03e\\nf(x) = x w where w = Ww.\\n\\ue03e \\ue030 \\ue030\\nClearly, we must use a nonlinear function to describe the features. Most neural\\nnetworks do so using an affine transformation controlled by learned parameters,\\nfollowed by a fixed, nonlinear function called an activation function. We use that\\nstrategy here, by defining h = g(W x+c), where W provides the weights of a\\n\\ue03e\\nlinear transformation and c the biases. Previously, to describe a linear regression\\n172\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\n1\\n0\\n0 1\\nx\\n1\\nx\\n2\\nOriginal x space\\n1\\n0\\n0 1 2\\nh\\n1\\n2h\\nLearned h space'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Figure 6.1: Solving the XOR problem by learning a representation. The bold numbers\\nprinted on the plot indicate the value that the learned function must output at each point.\\n(Left)A linear model applied directly to the original input cannot implement the XOR\\nfunction. When x = 0, the model’s output must increase as x increases. When x = 1,\\n1 2 1\\nthe model’s output must decrease as x increases. A linear model must apply a fixed\\n2\\ncoefficient w to x . The linear model therefore cannot use the value of x to change\\n2 2 1\\nthe coefficient on x and cannot solve this problem. (Right)In the transformed space\\n2\\nrepresented by the features extracted by a neural network, a linear model can now solve\\nthe problem. In our example solution, the two points that must have output 1 have been\\ncollapsed into a single point in feature space. In other words, the nonlinear features have\\nmapped both x= [1,0] and x = [0,1] to a single point in feature space, h = [1,0] .\\n\\ue03e \\ue03e \\ue03e'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The linear model can now describe the function as increasing in h and decreasing in h .\\n1 2\\nIn this example, the motivation for learning the feature space is only to make the model\\ncapacity greater so that it can fit the training set. In more realistic applications, learned\\nrepresentations can also help the model to generalize.\\n173\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nyy yy\\nw\\nhh hh hh\\n11 22\\nW\\nxx xx xx\\n11 22\\nFigure6.2: Anexample ofafeedforwardnetwork, drawnintwodifferentstyles. Specifically,\\nthis is the feedforward network we use to solve the XOR example. It has a single hidden\\nlayer containing two units. (Left)In this style, we draw every unit as a node in the graph.\\nThis style is very explicit and unambiguous but for networks larger than this example\\nit can consume too much space. (Right)In this style, we draw a node in the graph for\\neach entire vector representing a layer’s activations. This style is much more compact.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Sometimes we annotate the edges in this graph with the name of the parameters that\\ndescribe the relationship between two layers. Here, we indicate that a matrix W describes\\nthe mapping from x to h, and a vector w describes the mapping from h to y. We\\ntypically omit the intercept parameters associated with each layer when labeling this kind\\nof drawing.\\nmodel, we used a vector of weights and a scalar bias parameter to describe an\\naffine transformation from an input vector to an output scalar. Now, we describe\\nan affine transformation from a vector x to a vector h, so an entire vector of bias\\nparameters is needed. The activation function g is typically chosen to be a function\\nthat is applied element-wise, with h = g(x W +c ). In modern neural networks,\\ni \\ue03e :,i i\\nthe default recommendation is to use the rectified linear unit or ReLU (Jarrett\\net al., 2009; Nair and Hinton, 2010; Glorot et al., 2011a) defined by the activation\\nfunction g(z) = max 0,z depicted in figure 6.3.\\n{ }'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='We can now specify our complete network as\\nf(x;W,c,w,b) = w max 0,W x+c +b. (6.3)\\n\\ue03e \\ue03e\\n{ }\\nWe can now specify a solution to the XOR problem. Let\\n1 1\\nW = , (6.4)\\n1 1\\n\\ue014 \\ue015\\n0\\nc = , (6.5)\\n1\\n−\\n174\\n\\ue014 \\ue015\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\n0\\n0\\nz\\nz,0\\nxam=)z(g\\n}\\n{\\nFigure 6.3: The rectified linear activation function. This activation function is the default\\nactivation function recommended for use with most feedforward neural networks. Applying\\nthis function to the output of a linear transformation yields a nonlinear transformation.\\nHowever, the function remains very close to linear, in the sense that is a piecewise linear\\nfunction with two linear pieces. Because rectified linear units are nearly linear, they\\npreserve many of the properties that make linear models easy to optimize with gradient-\\nbased methods. They also preserve many of the properties that make linear models\\ngeneralize well. A common principle throughout computer science is that we can build'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='complicated systems from minimal components. Much as a Turing machine’s memory\\nneeds only to be able to store 0 or 1 states, we can build a universal function approximator\\nfrom rectified linear functions.\\n175\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\n1\\nw = , (6.6)\\n2\\n\\ue014 − \\ue015\\nand b = 0.\\nWe can now walk through the way that the model processes a batch of inputs.\\nLet X be the design matrix containing all four points in the binary input space,\\nwith one example per row:\\n0 0\\n0 1\\nX = \\uf8ee \\uf8f9. (6.7)\\n1 0\\n\\uf8ef 1 1 \\uf8fa\\n\\uf8ef \\uf8fa\\n\\uf8f0 \\uf8fb\\nThe first step in the neural network is to multiply the input matrix by the first\\nlayer’s weight matrix:\\n0 0\\n1 1\\nXW = \\uf8ee \\uf8f9. (6.8)\\n1 1\\n\\uf8ef 2 2 \\uf8fa\\n\\uf8ef \\uf8fa\\n\\uf8f0 \\uf8fb\\nNext, we add the bias vector c, to obtain\\n0 1\\n−\\n1 0\\n\\uf8ee \\uf8f9. (6.9)\\n1 0\\n\\uf8ef 2 1 \\uf8fa\\n\\uf8ef \\uf8fa\\n\\uf8f0 \\uf8fb\\nIn this space, all of the examples lie along a line with slope 1. As we move along\\nthis line, the output needs to begin at 0, then rise to 1, then drop back down to 0.\\nA linear model cannot implement such a function. To finish computing the value'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of h for each example, we apply the rectified linear transformation:\\n0 0\\n1 0\\n\\uf8ee \\uf8f9 . (6.10)\\n1 0\\n\\uf8ef 2 1 \\uf8fa\\n\\uf8ef \\uf8fa\\n\\uf8f0 \\uf8fb\\nThis transformation has changed the relationship between the examples. They no\\nlonger lie on a single line. As shown in figure 6.1, they now lie in a space where a\\nlinear model can solve the problem.\\nWe finish by multiplying by the weight vector w:\\n0\\n1\\n. (6.11)\\n1\\n0\\n176\\n\\uf8ee \\uf8f9\\n\\uf8ef \\uf8fa\\n\\uf8ef \\uf8fa\\n\\uf8f0 \\uf8fb\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nThe neural network has obtained the correct answer for every example in the batch.\\nIn this example, we simply specified the solution, then showed that it obtained\\nzero error. In a real situation, there might be billions of model parameters and\\nbillions of training examples, so one cannot simply guess the solution as we did\\nhere. Instead, a gradient-based optimization algorithm can find parameters that\\nproduce very little error. The solution we described to the XOR problem is at a\\nglobal minimum of the loss function, so gradient descent could converge to this'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='point. There are other equivalent solutions to the XOR problem that gradient\\ndescent could also find. The convergence point of gradient descent depends on the\\ninitial values of the parameters. In practice, gradient descent would usually not\\nfind clean, easily understood, integer-valued solutions like the one we presented\\nhere.\\n6.2 Gradient-Based Learning\\nDesigning and training a neural network is not much different from training any\\nother machine learning model with gradient descent. In section 5.10, we described\\nhow to build a machine learning algorithm by specifying an optimization procedure,\\na cost function, and a model family.\\nThe largest difference between the linear models we have seen so far and neural\\nnetworks is that the nonlinearity of a neural network causes most interesting loss\\nfunctions to become non-convex. This means that neural networks are usually\\ntrained by using iterative, gradient-based optimizers that merely drive the cost'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='function to a very low value, rather than the linear equation solvers used to train\\nlinear regression models or the convex optimization algorithms with global conver-\\ngence guarantees used to train logistic regression or SVMs. Convex optimization\\nconverges starting from any initial parameters (in theory—in practice it is very\\nrobust but can encounter numerical problems). Stochastic gradient descent applied\\nto non-convex loss functions has no such convergence guarantee, and is sensitive\\nto the values of the initial parameters. For feedforward neural networks, it is\\nimportant to initialize all weights to small random values. The biases may be\\ninitialized to zero or to small positive values. The iterative gradient-based opti-\\nmization algorithms used to train feedforward networks and almost all other deep\\nmodels will be described in detail in chapter 8, with parameter initialization in\\nparticular discussed in section 8.4. For the moment, it suffices to understand that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the training algorithm is almost always based on using the gradient to descend the\\ncost function in one way or another. The specific algorithms are improvements\\nand refinements on the ideas of gradient descent, introduced in section 4.3, and,\\n177\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nmore specifically, are most often improvements of the stochastic gradient descent\\nalgorithm, introduced in section 5.9.\\nWe can of course, train models such as linear regression and support vector\\nmachines with gradient descent too, and in fact this is common when the training\\nset is extremely large. From this point of view, training a neural network is not\\nmuch different from training any other model. Computing the gradient is slightly\\nmore complicated for a neural network, but can still be done efficiently and exactly.\\nSection 6.5 will describe how to obtain the gradient using the back-propagation\\nalgorithm and modern generalizations of the back-propagation algorithm.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='As with other machine learning models, to apply gradient-based learning we\\nmust choose a cost function, and we must choose how to represent the output of\\nthe model. We now revisit these design considerations with special emphasis on\\nthe neural networks scenario.\\n6.2.1 Cost Functions\\nAn important aspect of the design of a deep neural network is the choice of the\\ncost function. Fortunately, the cost functions for neural networks are more or less\\nthe same as those for other parametric models, such as linear models.\\nIn most cases, our parametric model defines a distribution p(y x;θ) and\\n|\\nwe simply use the principle of maximum likelihood. This means we use the\\ncross-entropy between the training data and the model’s predictions as the cost\\nfunction.\\nSometimes, wetake asimpler approach,where rather than predictinga complete\\nprobability distribution over y, we merely predict some statistic of y conditioned\\non x. Specialized loss functions allow us to train a predictor of these estimates.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The total cost function used to train a neural network will often combine one\\nof the primary cost functions described here with a regularization term. We have\\nalready seen some simple examples of regularization applied to linear models in\\nsection 5.2.2. The weight decay approach used for linear models is also directly\\napplicable to deep neural networks and is among the most popular regularization\\nstrategies. More advanced regularization strategies for neural networks will be\\ndescribed in chapter 7.\\n6.2.1.1 Learning Conditional Distributions with Maximum Likelihood\\nMost modern neural networks are trained using maximum likelihood. This means\\nthat the cost function is simply the negative log-likelihood, equivalently described\\n178\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nas the cross-entropy between the training data and the model distribution. This\\ncost function is given by\\nE\\nJ(θ) = logp (y x). (6.12)\\nx,y pˆ model\\n− ∼ data |'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The specific form of the cost function changes from model to model, depending\\non the specific form of logp . The expansion of the above equation typically\\nmodel\\nyields some terms that do not depend on the model parameters and may be dis-\\ncarded. For example, as we saw in section 5.5.1, if p (y x) = (y;f(x;θ),I),\\nmodel\\n| N\\nthen we recover the mean squared error cost,\\n1\\nJ(θ) = E y f(x;θ) 2 +const, (6.13)\\nx,y pˆ\\n2 ∼ data|| − ||\\nup to a scaling factor of 1 and a term that does not depend on θ. The discarded\\n2\\nconstant is based on the variance of the Gaussian distribution, which in this case\\nwe chose not to parametrize. Previously, we saw that the equivalence between\\nmaximum likelihood estimation with an output distribution and minimization of\\nmean squared error holds for a linear model, but in fact, the equivalence holds\\nregardless of the f(x;θ) used to predict the mean of the Gaussian.\\nAn advantage of this approach of deriving the cost function from maximum'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='likelihood is that it removes the burden of designing cost functions for each model.\\nSpecifying a model p(y x) automatically determines a cost function logp(y x).\\n| |\\nOne recurring theme throughout neural network design is that the gradient of\\nthe cost function must be large and predictable enough to serve as a good guide\\nfor the learning algorithm. Functions that saturate (become very flat) undermine\\nthis objective because they make the gradient become very small. In many cases\\nthis happens because the activation functions used to produce the output of the\\nhidden units or the output units saturate. The negative log-likelihood helps to\\navoid this problem for many models. Many output units involve an exp function\\nthat can saturate when its argument is very negative. The log function in the\\nnegative log-likelihood cost function undoes the exp of some output units. We will\\ndiscuss the interaction between the cost function and the choice of output unit in\\nsection 6.2.2.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='One unusual property of the cross-entropy cost used to perform maximum\\nlikelihood estimation is that it usually does not have a minimum value when applied\\nto the models commonly used in practice. For discrete output variables, most\\nmodels are parametrized in such a way that they cannot represent a probability\\nof zero or one, but can come arbitrarily close to doing so. Logistic regression\\nis an example of such a model. For real-valued output variables, if the model\\n179\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\ncan control the density of the output distribution (for example, by learning the\\nvariance parameter of a Gaussian output distribution) then it becomes possible\\nto assign extremely high density to the correct training set outputs, resulting in\\ncross-entropy approaching negative infinity. Regularization techniques described\\nin chapter 7 provide several different ways of modifying the learning problem so\\nthat the model cannot reap unlimited reward in this way.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='6.2.1.2 Learning Conditional Statistics\\nInstead of learning a full probability distribution p(y x; θ) we often want to learn\\n|\\njust one conditional statistic of y given x.\\nFor example, we may have a predictor f(x;θ) that we wish to predict the mean\\nof y.\\nIf we use a sufficiently powerful neural network, we can think of the neural\\nnetwork as being able to represent any function f from a wide class of functions,\\nwith this class being limited only by features such as continuity and boundedness\\nrather than by having a specific parametric form. From this point of view, we\\ncan view the cost function as being a functional rather than just a function. A\\nfunctional is a mapping from functions to real numbers. We can thus think of\\nlearning as choosing a function rather than merely choosing a set of parameters.\\nWe can design our cost functional to have its minimum occur at some specific\\nfunction we desire. For example, we can design the cost functional to have its'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='minimum lie on the function that maps x to the expected value of y given x.\\nSolving an optimization problem with respect to a function requires a mathematical\\ntool called calculus of variations, described in section 19.4.2. It is not necessary\\nto understand calculus of variations to understand the content of this chapter. At\\nthe moment, it is only necessary to understand that calculus of variations may be\\nused to derive the following two results.\\nOur first result derived using calculus of variations is that solving the optimiza-\\ntion problem\\nf = argminE y f(x) 2 (6.14)\\n∗ x,y p\\nf ∼ data || − ||\\nyields\\nE\\nf (x) = [y], (6.15)\\n∗ y p (y x)\\ndata\\n∼ |\\nso long as this function lies within the class we optimize over. In other words, if we\\ncould train on infinitely many samples from the true data generating distribution,\\nminimizing the mean squared error cost function gives a function that predicts the\\nmean of y for each value of x.\\n180\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Different cost functions give different statistics. A second result derived using\\ncalculus of variations is that\\nf = argminE y f(x) (6.16)\\n∗ x,y p 1\\nf ∼ data || − ||\\nyields a function that predicts the median value of y for each x, so long as such a\\nfunction may be described by the family of functions we optimize over. This cost\\nfunction is commonly called mean absolute error.\\nUnfortunately, mean squared error and mean absolute error often lead to poor\\nresults when used with gradient-based optimization. Some output units that\\nsaturate produce very small gradients when combined with these cost functions.\\nThis is one reason that the cross-entropy cost function is more popular than mean\\nsquared error or mean absolute error, even when it is not necessary to estimate an\\nentire distribution p(y x).\\n|\\n6.2.2 Output Units\\nThe choice of cost function is tightly coupled with the choice of output unit. Most\\nof the time, we simply use the cross-entropy between the data distribution and the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='model distribution. The choice of how to represent the output then determines\\nthe form of the cross-entropy function.\\nAny kind of neural network unit that may be used as an output can also be\\nused as a hidden unit. Here, we focus on the use of these units as outputs of the\\nmodel, but in principle they can be used internally as well. We revisit these units\\nwith additional detail about their use as hidden units in section 6.3.\\nThroughout this section, we suppose that the feedforward network provides a\\nset of hidden features defined by h = f(x;θ). The role of the output layer is then\\nto provide some additional transformation from the features to complete the task\\nthat the network must perform.\\n6.2.2.1 Linear Units for Gaussian Output Distributions\\nOne simple kind of output unit is an output unit based on an affine transformation\\nwith no nonlinearity. These are often just called linear units.\\nGiven features h, a layer of linear output units produces a vector yˆ = W h+b.\\n\\ue03e'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Linear output layers are often used to produce the mean of a conditional\\nGaussian distribution:\\np(y x) = (y;yˆ,I). (6.17)\\n| N\\n181\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nMaximizing the log-likelihood is then equivalent to minimizing the mean squared\\nerror.\\nThe maximum likelihood framework makes it straightforward to learn the\\ncovariance of the Gaussian too, or to make the covariance of the Gaussian be a\\nfunction of the input. However, the covariance must be constrained to be a positive\\ndefinite matrix for all inputs. It is difficult to satisfy such constraints with a linear\\noutput layer, so typically other output units are used to parametrize the covariance.\\nApproaches to modeling the covariance are described shortly, in section 6.2.2.4.\\nBecause linear units do not saturate, they pose little difficulty for gradient-\\nbased optimization algorithms and may be used with a wide variety of optimization\\nalgorithms.\\n6.2.2.2 Sigmoid Units for Bernoulli Output Distributions'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Many tasks require predicting the value of a binary variable y. Classification\\nproblems with two classes can be cast in this form.\\nThe maximum-likelihood approach is to define a Bernoulli distribution over y\\nconditioned on x.\\nA Bernoulli distribution is defined by just a single number. The neural net\\nneeds to predict only P(y = 1 x). For this number to be a valid probability, it\\n|\\nmust lie in the interval [0, 1].\\nSatisfying this constraint requires some careful design effort. Suppose we were\\nto use a linear unit, and threshold its value to obtain a valid probability:\\nP(y = 1 x) = max 0,min 1,w h+b . (6.18)\\n\\ue03e\\n|\\n\\ue06e \\ue06e \\ue06f\\ue06f\\nThis would indeed define a valid conditional distribution, but we would not be able\\nto train it very effectively with gradient descent. Any time that w h +b strayed\\n\\ue03e\\noutside the unit interval, the gradient of the output of the model with respect to\\nits parameters would be 0. A gradient of 0 is typically problematic because the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='learning algorithm no longer has a guide for how to improve the corresponding\\nparameters.\\nInstead, it is better to use a different approach that ensures there is always a\\nstrong gradient whenever the model has the wrong answer. This approach is based\\non using sigmoid output units combined with maximum likelihood.\\nA sigmoid output unit is defined by\\nyˆ= σ w h+b (6.19)\\n\\ue03e\\n182\\n\\ue010 \\ue011\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nwhere σ is the logistic sigmoid function described in section 3.10.\\nWe can think of the sigmoid output unit as having two components. First, it\\nuses a linear layer to compute z = w h +b. Next, it uses the sigmoid activation\\n\\ue03e\\nfunction to convert z into a probability.\\nWe omit the dependence on x for the moment to discuss how to define a\\nprobability distribution over y using the value z. The sigmoid can be motivated\\n˜\\nby constructing an unnormalized probability distribution P(y), which does not\\nsum to 1. We can then divide by an appropriate constant to obtain a valid'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='probability distribution. If we begin with the assumption that the unnormalized log\\nprobabilities are linear in y and z, we can exponentiate to obtain the unnormalized\\nprobabilities. We then normalize to see that this yields a Bernoulli distribution\\ncontrolled by a sigmoidal transformation of z:\\n˜\\nlogP(y) = yz (6.20)\\n˜\\nP(y) = exp(yz) (6.21)\\nexp(yz)\\nP(y) = (6.22)\\n1\\nexp(y z)\\ny =0 \\ue030\\n\\ue030\\nP(y) = σ((2y 1)z). (6.23)\\n\\ue050\\n−\\nProbability distributions based on exponentiation and normalization are common\\nthroughout the statistical modeling literature. The z variable defining such a\\ndistribution over binary variables is called a logit.\\nThis approach to predicting the probabilities in log-space is natural to use\\nwith maximum likelihood learning. Because the cost function used with maximum\\nlikelihood is logP(y x), the log in the cost function undoes the exp of the\\n− |\\nsigmoid. Without this effect, the saturation of the sigmoid could prevent gradient-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='based learning from making good progress. The loss function for maximum\\nlikelihood learning of a Bernoulli parametrized by a sigmoid is\\nJ(θ) = logP(y x) (6.24)\\n− |\\n= logσ((2y 1)z) (6.25)\\n− −\\n= ζ((1 2y)z). (6.26)\\n−\\nThis derivation makes use of some properties from section 3.10. By rewriting\\nthe loss in terms of the softplus function, we can see that it saturates only when\\n(1 2y)z is very negative. Saturation thus occurs only when the model already\\n−\\nhas the right answer—when y = 1 and z is very positive, or y = 0 and z is very\\nnegative. When z has the wrong sign, the argument to the softplus function,\\n183\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\n(1 2y)z, may be simplified to z . As z becomes large while z has the wrong sign,\\n− | | | |\\nthe softplus function asymptotes toward simply returning its argument z . The\\n| |\\nderivative with respect to z asymptotes to sign(z), so, in the limit of extremely\\nincorrect z, the softplus function does not shrink the gradient at all. This property'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='is very useful because it means that gradient-based learning can act to quickly\\ncorrect a mistaken z.\\nWhen we use other loss functions, such as mean squared error, the loss can\\nsaturate anytime σ(z) saturates. The sigmoid activation function saturates to 0\\nwhen z becomes very negative and saturates to 1 when z becomes very positive.\\nThe gradient can shrink too small to be useful for learning whenever this happens,\\nwhether the model has the correct answer or the incorrect answer. For this reason,\\nmaximum likelihood is almost always the preferred approach to training sigmoid\\noutput units.\\nAnalytically, the logarithm of the sigmoid is always defined and finite, because\\nthe sigmoid returns values restricted to the open interval (0,1), rather than using\\nthe entire closed interval of valid probabilities [0,1]. In software implementations,\\nto avoid numerical problems, it is best to write the negative log-likelihood as a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='function of z, rather than as a function of yˆ = σ(z). If the sigmoid function\\nunderflows to zero, then taking the logarithm of yˆ yields negative infinity.\\n6.2.2.3 Softmax Units for Multinoulli Output Distributions\\nAny time we wish to represent a probability distribution over a discrete variable\\nwith n possible values, we may use the softmax function. This can be seen as a\\ngeneralization of the sigmoid function which was used to represent a probability\\ndistribution over a binary variable.\\nSoftmax functions are most often used as the output of a classifier, to represent\\nthe probability distribution over n different classes. More rarely, softmax functions\\ncan be used inside the model itself, if we wish the model to choose between one of\\nn different options for some internal variable.\\nIn the case of binary variables, we wished to produce a single number\\nyˆ= P(y = 1 x). (6.27)\\n|\\nBecause this number needed to lie between 0 and 1, and because we wanted the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='logarithm of the number to be well-behaved for gradient-based optimization of\\n˜\\nthe log-likelihood, we chose to instead predict a number z = logP(y = 1 x).\\n|\\nExponentiating and normalizing gave us a Bernoulli distribution controlled by the\\nsigmoid function.\\n184\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nTo generalize to the case of a discrete variable with n values, we now need\\nto produce a vector yˆ, with yˆ = P(y = i x). We require not only that each\\ni\\n|\\nelement of yˆ be between 0 and 1, but also that the entire vector sums to 1 so that\\ni\\nit represents a valid probability distribution. The same approach that worked for\\nthe Bernoulli distribution generalizes to the multinoulli distribution. First, a linear\\nlayer predicts unnormalized log probabilities:\\nz = W h+b, (6.28)\\n\\ue03e\\n˜\\nwhere z = logP(y = i x). The softmax function can then exponentiate and\\ni\\n|\\nnormalize z to obtain the desired yˆ. Formally, the softmax function is given by\\nexp(z )\\ni\\nsoftmax(z) = . (6.29)\\ni\\nexp(z )\\nj j\\n\\ue050'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='As with the logistic sigmoid, the use of the exp function works very well when\\ntraining the softmax to output a target value y using maximum log-likelihood. In\\nthis case, we wish to maximize logP(y = i;z) = logsoftmax(z). Defining the\\ni\\nsoftmax in terms of exp is natural because the log in the log-likelihood can undo\\nthe exp of the softmax:\\nlogsoftmax(z) = z log exp(z ). (6.30)\\ni i j\\n−\\nj\\n\\ue058\\nThe first term of equation 6.30 shows that the input z always has a direct\\ni\\ncontribution to the cost function. Because this term cannot saturate, we know\\nthat learning can proceed, even if the contribution of z to the second term of\\ni\\nequation 6.30 becomes very small. When maximizing the log-likelihood, the first\\nterm encourages z to be pushed up, while the second term encourages all of z to be\\ni\\npushed down. To gain some intuition for the second term, log exp(z ), observe\\nj j\\nthat this term can be roughly approximated by max z . This approximation is\\nj j\\n\\ue050'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='based on the idea that exp(z ) is insignificant for any z that is noticeably less than\\nk k\\nmax z . The intuition we can gain from this approximation is that the negative\\nj j\\nlog-likelihood cost function always strongly penalizes the most active incorrect\\nprediction. If the correct answer already has the largest input to the softmax, then\\nthe z term and the log exp(z ) max z = z terms will roughly cancel.\\n− i j j ≈ j j i\\nThis example will then contribute little to the overall training cost, which will be\\n\\ue050\\ndominated by other examples that are not yet correctly classified.\\nSofar wehavediscussed only asingle example. Overall, unregularizedmaximum\\nlikelihood will drive the model to learn parameters that drive the softmax to predict\\n185\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nthe fraction of counts of each outcome observed in the training set:\\nm\\n1\\nj=1 y(j)=i,x(j)=x\\nsoftmax(z(x;θ)) . (6.31)\\ni m\\n≈ 1\\n\\ue050 j=1 x(j)=x\\nBecause maximum likelihood is a consistent e\\ue050stimator, this is guaranteed to happen'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='so long as the model family is capable of representing the training distribution. In\\npractice, limited model capacity and imperfect optimization will mean that the\\nmodel is only able to approximate these fractions.\\nMany objective functions other than the log-likelihood do not work as well\\nwith the softmax function. Specifically, objective functions that do not use a log to\\nundo the exp of the softmax fail to learn when the argument to theexp becomes\\nvery negative, causing the gradient to vanish. In particular, squared error is a\\npoor loss function for softmax units, and can fail to train the model to change its\\noutput, even when the model makes highly confident incorrect predictions (Bridle,\\n1990). To understand why these other loss functions can fail, we need to examine\\nthe softmax function itself.\\nLike the sigmoid, the softmax activation can saturate. The sigmoid function has\\na single output that saturates when its input is extremely negative or extremely'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='positive. In the case of the softmax, there are multiple output values. These\\noutput values can saturate when the differences between input values become\\nextreme. When the softmax saturates, many cost functions based on the softmax\\nalso saturate, unless they are able to invert the saturating activating function.\\nTo see that the softmax function responds to the difference between its inputs,\\nobserve that the softmax output is invariant to adding the same scalar to all of its\\ninputs:\\nsoftmax(z) = softmax(z +c). (6.32)\\nUsing this property, we can derive a numerically stable variant of the softmax:\\nsoftmax(z) = softmax(z maxz ). (6.33)\\ni\\n− i\\nThe reformulated version allows us to evaluate softmax with only small numerical\\nerrors even when z contains extremely large or extremely negative numbers. Ex-\\namining the numerically stable variant, we see that the softmax function is driven\\nby the amount that its arguments deviate from max z .\\ni i'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='An output softmax(z) saturates to 1 when the corresponding input is maximal\\ni\\n(z = max z ) and z is much greater than all of the other inputs. The output\\ni i i i\\nsoftmax(z) can also saturate to 0 when z is not maximal and the maximum is\\ni i\\nmuch greater. This is a generalization of the way that sigmoid units saturate, and\\n186\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\ncan cause similar difficulties for learning if the loss function is not designed to\\ncompensate for it.\\nThe argument z to the softmax function can be produced in two different ways.\\nThe most common is simply to have an earlier layer of the neural network output\\nevery element of z, as described above using the linear layer z = W h+b. While\\n\\ue03e\\nstraightforward, this approach actually overparametrizes the distribution. The\\nconstraint that the n outputs must sum to 1 means that only n 1 parameters are\\n−\\nnecessary; the probability of the n-th value may be obtained by subtracting the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='first n 1 probabilities from 1. We can thus impose a requirement that one element\\n−\\nof z be fixed. For example, we can require that z = 0. Indeed, this is exactly\\nn\\nwhat the sigmoid unit does. Defining P(y= 1 x) = σ(z) is equivalent to defining\\n|\\nP(y = 1 x) = softmax(z) with a two-dimensional z and z = 0. Both the n 1\\n1 1\\n| −\\nargument and the n argument approaches to the softmax can describe the same\\nset of probability distributions, but have different learning dynamics. In practice,\\nthere is rarely much difference between using the overparametrized version or the\\nrestricted version, and it is simpler to implement the overparametrized version.\\nFrom a neuroscientific point of view, it is interesting to think of the softmax as\\na way to create a form of competition between the units that participate in it: the\\nsoftmax outputs always sum to 1 so an increase in the value of one unit necessarily\\ncorresponds to a decrease in the value of others. This is analogous to the lateral'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='inhibition that is believed to exist between nearby neurons in the cortex. At the\\nextreme (when the difference between the maximal a and the others is large in\\ni\\nmagnitude) it becomes a form of winner-take-all (one of the outputs is nearly 1\\nand the others are nearly 0).\\nThe name “softmax” can be somewhat confusing. The function is more closely\\nrelated to the argmax function than the max function. The term “soft” derives\\nfrom the fact that the softmax function is continuous and differentiable. The\\nargmax function, with its result represented as a one-hot vector, is not continuous\\nor differentiable. The softmax function thus provides a “softened” version of the\\nargmax. The corresponding soft version of the maximum function is softmax(z) z.\\n\\ue03e\\nIt would perhaps be better to call the softmax function “softargmax,” but the\\ncurrent name is an entrenched convention.\\n6.2.2.4 Other Output Types\\nThe linear, sigmoid, and softmax output units described above are the most'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='common. Neural networks can generalize to almost any kind of output layer that\\nwe wish. The principle of maximum likelihood provides a guide for how to design\\n187\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\na good cost function for nearly any kind of output layer.\\nIn general, if we define a conditional distribution p(y x; θ), the principle of\\n|\\nmaximum likelihood suggests we use logp(y x;θ) as our cost function.\\n− |\\nIn general, wecan think of the neural network as representinga function f(x;θ).\\nThe outputs of this function are not direct predictions of the value y. Instead,\\nf(x;θ) = ω provides the parameters for a distribution over y. Our loss function\\ncan then be interpreted as logp(y;ω(x)).\\n−\\nFor example, we may wish to learn the variance of a conditional Gaussian for y,\\ngiven x. In the simple case, where the variance σ2 is a constant, there is a closed\\nform expression because the maximum likelihood estimator of variance is simply the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='empirical mean of the squared difference between observations y and their expected\\nvalue. A computationally more expensive approach that does not require writing\\nspecial-case code is to simply include the variance as one of the properties of the\\ndistribution p(y x) that is controlled by ω = f(x; θ). The negative log-likelihood\\n|\\nlogp(y;ω(x)) will then provide a cost function with the appropriate terms\\n−\\nnecessary to make our optimization procedure incrementally learn the variance. In\\nthe simple case where the standard deviation does not depend on the input, we\\ncan make a new parameter in the network that is copied directly into ω. This new\\nparameter might be σ itself or could be a parameter v representing σ2 or it could\\nbe a parameter β representing 1 , depending on how we choose to parametrize\\nσ2\\nthe distribution. We may wish our model to predict a different amount of variance\\nin y for different values of x. This is called a heteroscedastic model. In the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='heteroscedastic case, we simply make the specification of the variance be one of\\nthe values output by f(x;θ). A typical way to do this is to formulate the Gaussian\\ndistribution using precision, rather than variance, as described in equation 3.22.\\nIn the multivariate case it is most common to use a diagonal precision matrix\\ndiag(β). (6.34)\\nThis formulation works well with gradient descent because the formula for the\\nlog-likelihood of the Gaussian distribution parametrized by β involves only mul-\\ntiplication by β and addition of logβ . The gradient of multiplication, addition,\\ni i\\nand logarithm operations is well-behaved. By comparison, if we parametrized the\\noutput in terms of variance, we would need to use division. The division function\\nbecomes arbitrarily steep near zero. While large gradients can help learning,\\narbitrarily large gradients usually result in instability. If we parametrized the\\noutput in terms of standard deviation, the log-likelihood would still involve division,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='and would also involve squaring. The gradient through the squaring operation\\ncan vanish near zero, making it difficult to learn parameters that are squared.\\n188\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nRegardless of whether we use standard deviation, variance, or precision, we must\\nensure that the covariance matrix of the Gaussian is positive definite. Because\\nthe eigenvalues of the precision matrix are the reciprocals of the eigenvalues of\\nthe covariance matrix, this is equivalent to ensuring that the precision matrix is\\npositive definite. If we use a diagonal matrix, or a scalar times the diagonal matrix,\\nthen the only condition we need to enforce on the output of the model is positivity.\\nIf we suppose that a is the raw activation of the model used to determine the\\ndiagonal precision, we can use the softplus function to obtain a positive precision\\nvector: β = ζ(a). This same strategy applies equally if using variance or standard'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='deviation rather than precision or if using a scalar times identity rather than\\ndiagonal matrix.\\nIt is rare to learn a covariance or precision matrix with richer structure than\\ndiagonal. If the covariance is full and conditional, then a parametrization must\\nbe chosen that guarantees positive-definiteness of the predicted covariance matrix.\\nThis can be achieved by writing Σ(x) = B(x)B (x), whereB is an unconstrained\\n\\ue03e\\nsquare matrix. One practical issue if the matrix is full rank is that computing the\\nlikelihood is expensive, with a d d matrix requiring O(d3) computation for the\\n×\\ndeterminant and inverse of Σ(x) (or equivalently, and more commonly done, its\\neigendecomposition or that of B(x)).\\nWe often want to perform multimodal regression, that is, to predict real values\\nthat come from a conditional distribution p(y x) that can have several different\\n|\\npeaks in y space for the same value of x. In this case, a Gaussian mixture is'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a natural representation for the output (Jacobs et al., 1991; Bishop, 1994).\\nNeural networks with Gaussian mixtures as their output are often called mixture\\ndensity networks. A Gaussian mixture output with n components is defined by\\nthe conditional probability distribution\\nn\\np(y x) = p(c = i x) (y;µ(i)(x),Σ(i)(x)). (6.35)\\n| | N\\ni=1\\n\\ue058\\nThe neural network must have three outputs: a vector defining p(c = i x), a\\n|\\nmatrix providing µ(i)(x) for all i, and a tensor providing Σ(i)(x) for all i. These\\noutputs must satisfy different constraints:\\n1. Mixture components p(c = i x): these form a multinoulli distribution\\n|\\nover the n different components associated with latent variable1 c, and can\\n1We consider c to be latent because we do not observe it in the data: given input x and target\\ny, it is not possible to know with certainty which Gaussian component was responsible for y, but\\nwe can imagine that y was generated by picking one of them, and make that unobserved choice a\\nrandom variable.\\n189'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 6. DEEP FEEDFORWARD NETWORKS\\ntypically be obtained by a softmax over an n-dimensional vector, to guarantee\\nthat these outputs are positive and sum to 1.\\n2. Means µ(i)(x): these indicate the center or mean associated with the i-th\\nGaussian component, and are unconstrained (typically with no nonlinearity\\nat all for these output units). If yis a d-vector, then the network must output\\nan n d matrix containing all n of these d-dimensional vectors. Learning\\n×\\nthese means with maximum likelihood is slightly more complicated than\\nlearning the means of a distribution with only one output mode. We only\\nwant to update the mean for the component that actually produced the\\nobservation. In practice, we do not know which component produced each\\nobservation. The expression for the negative log-likelihood naturally weights\\neach example’s contribution to the loss for each component by the probability\\nthat the component produced the example.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='3. Covariances Σ(i)(x): these specify the covariance matrix for each component\\ni. As when learning a single Gaussian component, we typically use a diagonal\\nmatrixto avoid needing to computedeterminants. As withlearning the means\\nof the mixture, maximum likelihood is complicated by needing to assign\\npartial responsibility for each point to each mixture component. Gradient\\ndescent will automatically follow the correct process if given the correct\\nspecification of the negative log-likelihood under the mixture model.\\nIt has been reported that gradient-based optimization of conditional Gaussian\\nmixtures (on the output of neural networks) can be unreliable, in part because one\\ngets divisions (by the variance) which can be numerically unstable (when some\\nvariance gets to be small for a particular example, yielding very large gradients).\\nOne solution is to clip gradients (see section 10.11.1) while another is to scale\\nthe gradients heuristically (Murray and Larochelle, 2014).'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Gaussian mixture outputs are particularly effective in generative models of\\nspeech (Schuster, 1999) or movements of physical objects (Graves, 2013). The\\nmixture density strategy gives a way for the network to represent multiple output\\nmodes and to control the variance of its output, which is crucial for obtaining\\na high degree of quality in these real-valued domains. An example of a mixture\\ndensity network is shown in figure 6.4.\\nIn general, we may wish to continue to model larger vectors y containing more\\nvariables, and to impose richer and richer structures on these output variables. For\\nexample, we may wish for our neural network to output a sequence of characters\\nthat forms a sentence. In these cases, we may continue to use the principle\\nof maximum likelihood applied to our model p(y;ω(x)), but the model we use\\n190\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nx\\ny\\nFigure 6.4: Samples drawn from a neural network with a mixture density output layer.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The input x is sampled from a uniform distribution and the output y is sampled from\\np (y x). The neural network is able to learn nonlinear mappings from the input to\\nmodel\\n|\\nthe parameters of the output distribution. These parameters include the probabilities\\ngoverning which of three mixture components will generate the output as well as the\\nparameters for each mixture component. Each mixture component is Gaussian with\\npredicted mean and variance. All of these aspects of the output distribution are able to\\nvary with respect to the input x, and to do so in nonlinear ways.\\nto describe y becomes complex enough to be beyond the scope of this chapter.\\nChapter 10 describes how to use recurrent neural networks to define such models\\nover sequences, and part III describes advanced techniques for modeling arbitrary\\nprobability distributions.\\n6.3 Hidden Units\\nSo far we have focused our discussion on design choices for neural networks that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='are common to most parametric machine learning models trained with gradient-\\nbased optimization. Now we turn to an issue that is unique to feedforward neural\\nnetworks: how to choose the type of hidden unit to use in the hidden layers of the\\nmodel.\\nThe design of hidden units is an extremely active area of research and does not\\nyet have many definitive guiding theoretical principles.\\nRectified linear units are an excellent default choice of hidden unit. Many other\\ntypes of hidden units are available. It can be difficult to determine when to use\\nwhich kind (though rectified linear units are usually an acceptable choice). We\\n191\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\ndescribe here some of the basic intuitions motivating each type of hidden units.\\nThese intuitions can help decide when to try out each of these units. It is usually\\nimpossible to predict in advance which will work best. The design process consists\\nof trial and error, intuiting that a kind of hidden unit may work well, and then'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='training a network with that kind of hidden unit and evaluating its performance\\non a validation set.\\nSome of the hidden units included in this list are not actually differentiable at\\nall input points. For example, the rectified linear function g(z) = max 0,z is not\\n{ }\\ndifferentiable at z = 0. This may seem like it invalidates g for use with a gradient-\\nbased learning algorithm. In practice, gradient descent still performs well enough\\nfor these models to be used for machine learning tasks. This is in part because\\nneural network training algorithms do not usually arrive at a local minimum of\\nthe cost function, but instead merely reduce its value significantly, as shown in\\nfigure 4.3. These ideas will be described further in chapter 8. Because we do not\\nexpect training to actually reach a point where the gradient is 0, it is acceptable\\nfor the minima of the cost function to correspond to points with undefined gradient.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Hidden units that are not differentiable are usually non-differentiable at only a\\nsmall number of points. In general, a function g(z) has a left derivative defined\\nby the slope of the function immediately to the left of z and a right derivative\\ndefined by the slope of the function immediately to the right of z. A function\\nis differentiable at z only if both the left derivative and the right derivative are\\ndefined and equal to each other. The functions used in the context of neural\\nnetworks usually have defined left derivatives and defined right derivatives. In the\\ncase of g(z) = max 0,z , the left derivative at z = 0 is 0 and the right derivative\\n{ }\\nis 1. Software implementations of neural network training usually return one of\\nthe one-sided derivatives rather than reporting that the derivative is undefined or\\nraising an error. This may be heuristically justified by observing that gradient-\\nbased optimization on a digital computer is subject to numerical error anyway.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='When a function is asked to evaluate g(0), it is very unlikely that the underlying\\nvalue truly was 0. Instead, it was likely to be some small value \\ue00f that was rounded\\nto 0. In some contexts, more theoretically pleasing justifications are available, but\\nthese usually do not apply to neural network training. The important point is that\\nin practice one can safely disregard the non-differentiability of the hidden unit\\nactivation functions described below.\\nUnless indicated otherwise, most hidden units can be described as accepting\\na vector of inputs x, computing an affine transformation z = W x+ b, and\\n\\ue03e\\nthen applying an element-wise nonlinear function g(z). Most hidden units are\\ndistinguished from each other only by the choice of the form of the activation\\nfunction g(z).\\n192\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\n6.3.1 Rectified Linear Units and Their Generalizations\\nRectified linear units use the activation function g(z) = max 0,z .\\n{ }'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Rectified linear units are easy to optimize because they are so similar to linear\\nunits. The only difference between a linear unit and a rectified linear unit is\\nthat a rectified linear unit outputs zero across half its domain. This makes the\\nderivatives through a rectified linear unit remain large whenever the unit is active.\\nThe gradients are not only large but also consistent. The second derivative of the\\nrectifying operation is 0 almost everywhere, and the derivative of the rectifying\\noperation is 1 everywhere that the unit is active. This means that the gradient\\ndirection is far more useful for learning than it would be with activation functions\\nthat introduce second-order effects.\\nRectified linear units are typically used on top of an affine transformation:\\nh = g(W x+b). (6.36)\\n\\ue03e\\nWhen initializing the parameters of the affine transformation, it can be a good\\npractice to set all elements of b to a small, positive value, such as 0.1. This makes'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='it very likely that the rectified linear units will be initially active for most inputs\\nin the training set and allow the derivatives to pass through.\\nSeveral generalizations of rectified linear units exist. Most of these general-\\nizations perform comparably to rectified linear units and occasionally perform\\nbetter.\\nOne drawback to rectified linear units is that they cannot learn via gradient-\\nbased methods on examples for which their activation is zero. A variety of\\ngeneralizations of rectified linear units guarantee that they receive gradient every-\\nwhere.\\nThree generalizations of rectified linear units are based on using a non-zero\\nslope α when z < 0: h = g(z,α) =max(0,z )+ α min(0,z ). Absolute value\\ni i i i i i i\\nrectification fixes α = 1 to obtain g(z) = z . It is used for object recognition\\ni\\n− | |\\nfrom images (Jarrett et al., 2009), where it makes sense to seek features that are\\ninvariant under a polarity reversal of the input illumination. Other generalizations'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of rectified linear units are more broadly applicable. A leaky ReLU (Maas et al.,\\n2013) fixes α to a small value like 0.01 while a parametric ReLU or PReLU\\ni\\ntreats α as a learnable parameter (He et al., 2015).\\ni\\nMaxout units (Goodfellow et al., 2013a) generalize rectified linear units\\nfurther. Instead of applying an element-wise function g(z), maxout units dividez\\ninto groups of k values. Each maxout unit then outputs the maximum element of\\n193\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\none of these groups:\\ng(z) = max z (6.37)\\ni j\\nj G(i)\\n∈\\nwhere G(i) is the set of indices into the inputs for group i, (i 1)k+1,...,ik .\\n{ − }\\nThis provides a way of learning a piecewise linear function that responds to multiple\\ndirections in the input x space.\\nA maxout unit can learn a piecewise linear, convex function with up to k pieces.\\nMaxout units can thus be seen as learning the activation function itself rather\\nthan just the relationship between units. With large enough k, a maxout unit can'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='learn to approximate any convex function with arbitrary fidelity. In particular,\\na maxout layer with two pieces can learn to implement the same function of the\\ninput x as a traditional layer using the rectified linear activation function, absolute\\nvalue rectification function, or the leaky or parametric ReLU, or can learn to\\nimplement a totally different function altogether. The maxout layer will of course\\nbe parametrized differently from any of these other layer types, so the learning\\ndynamics will be different even in the cases where maxout learns to implement the\\nsame function of x as one of the other layer types.\\nEach maxout unit is now parametrized by k weight vectors instead of just one,\\nso maxout units typically need more regularization than rectified linear units. They\\ncan work well without regularization if the training set is large and the number of\\npieces per unit is kept low (Cai et al., 2013).\\nMaxout units have a few other benefits. In some cases, one can gain some sta-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='tistical and computational advantages by requiring fewer parameters. Specifically,\\nif the features captured by n different linear filters can be summarized without\\nlosing information by taking the max over each group of k features, then the next\\nlayer can get by with k times fewer weights.\\nBecause each unit is driven by multiple filters, maxout units have some redun-\\ndancy that helps them to resist a phenomenon called catastrophic forgetting\\nin which neural networks forget how to perform tasks that they were trained on in\\nthe past (Goodfellow et al., 2014a).\\nRectified linear units and all of these generalizations of them are based on the\\nprinciple that models are easier to optimize if their behavior is closer to linear.\\nThis same general principle of using linear behavior to obtain easier optimization\\nalso applies in other contexts besides deep linear networks. Recurrent networks can\\nlearn from sequences and produce a sequence of states and outputs. When training'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='them, one needs to propagate information through several time steps, whichis much\\neasier when some linear computations (with some directional derivatives being of\\nmagnitude near 1) are involved. One of the best-performing recurrent network\\n194\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\narchitectures, the LSTM, propagates information through time via summation—a\\nparticular straightforward kind of such linear activation. This is discussed further\\nin section 10.10.\\n6.3.2 Logistic Sigmoid and Hyperbolic Tangent\\nPrior to the introduction of rectified linear units, most neural networks used the\\nlogistic sigmoid activation function\\ng(z) = σ(z) (6.38)\\nor the hyperbolic tangent activation function\\ng(z) = tanh(z). (6.39)\\nThese activation functions are closely related because tanh(z) = 2σ(2z) 1.\\n−\\nWe have already seen sigmoid units as output units, used to predict the\\nprobability that a binary variable is 1. Unlike piecewise linear units, sigmoidal'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='units saturate across most of their domain—they saturate to a high value when\\nz is very positive, saturate to a low value when z is very negative, and are only\\nstrongly sensitive to their input when z is near 0. The widespread saturation of\\nsigmoidal units can make gradient-based learning very difficult. For this reason,\\ntheir use as hidden units in feedforward networks is now discouraged. Their use\\nas output units is compatible with the use of gradient-based learning when an\\nappropriate cost function can undo the saturation of the sigmoid in the output\\nlayer.\\nWhen a sigmoidal activation function must be used, the hyperbolic tangent\\nactivation function typically performs better than the logistic sigmoid. It resembles\\nthe identity function more closely, in the sense that tanh(0) = 0 while σ(0) = 1.\\n2\\nBecause tanh is similar to the identity function near 0, training a deep neural\\nnetwork yˆ = w tanh(U tanh(V x)) resembles training a linear model yˆ =\\n\\ue03e \\ue03e \\ue03e'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='w U V x so long as the activations of the network can be kept small. This\\n\\ue03e \\ue03e \\ue03e\\nmakes training the tanh network easier.\\nSigmoidal activation functions are more common in settings other than feed-\\nforward networks. Recurrent networks, many probabilistic models, and some\\nautoencoders have additional requirements that rule out the use of piecewise\\nlinear activation functions and make sigmoidal units more appealing despite the\\ndrawbacks of saturation.\\n195\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\n6.3.3 Other Hidden Units\\nMany other types of hidden units are possible, but are used less frequently.\\nIn general, a wide variety of differentiable functions perform perfectly well.\\nMany unpublished activation functions perform just as well as the popular ones.\\nTo provide a concrete example, the authors tested a feedforward network using\\nh = cos(Wx+ b) on the MNIST dataset and obtained an error rate of less than\\n1%, which is competitive with results obtained using more conventional activation'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='functions. During research and development of new techniques, it is common\\nto test many different activation functions and find that several variations on\\nstandard practice perform comparably. This means that usually new hidden unit\\ntypes are published only if they are clearly demonstrated to provide a significant\\nimprovement. New hidden unit types that perform roughly comparably to known\\ntypes are so common as to be uninteresting.\\nIt would be impractical to list all of the hidden unit types that have appeared\\nin the literature. We highlight a few especially useful and distinctive ones.\\nOne possibility is to not have an activation g(z) at all. One can also think of\\nthis as using the identity function as the activation function. We have already\\nseen that a linear unit can be useful as the output of a neural network. It may\\nalso be used as a hidden unit. If every layer of the neural network consists of only\\nlinear transformations, then the network as a whole will be linear. However, it'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='is acceptable for some layers of the neural network to be purely linear. Consider\\na neural network layer with n inputs and p outputs, h = g(W x+ b). We may\\n\\ue03e\\nreplace this with two layers, with one layer using weight matrix U and the other\\nusing weight matrix V . If the first layer has no activation function, then we have\\nessentially factored the weight matrix of the original layer based on W . The\\nfactored approach is to compute h =g(V U x+b). If U produces q outputs,\\n\\ue03e \\ue03e\\nthen U and V together contain only (n + p)q parameters, while W contains np\\nparameters. For small q, this can be a considerable saving in parameters. It\\ncomes at the cost of constraining the linear transformation to be low-rank, but\\nthese low-rank relationships are often sufficient. Linear hidden units thus offer an\\neffective way of reducing the number of parameters in a network.\\nSoftmax units are another kind of unit that is usually used as an output (as'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='described in section 6.2.2.3) but may sometimes be used as a hidden unit. Softmax\\nunits naturally represent a probability distribution over a discrete variable with k\\npossible values, so they may be used as a kind of switch. These kinds of hidden\\nunits are usually only used in more advanced architectures that explicitly learn to\\nmanipulate memory, described in section 10.12.\\n196\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nA few other reasonably common hidden unit types include:\\nRadial basis function or RBF unit: h = exp 1 W x 2 . This\\n• i −σ2|| :,i − ||\\ni\\nfunction becomes more active as x approaches a \\ue010template W :,i. Be\\ue011cause it\\nsaturates to 0 for most x, it can be difficult to optimize.\\nSoftplus: g(a) = ζ(a) = log(1+ea). This is a smooth version of the rectifier,\\n•\\nintroduced by Dugas et al. (2001) for function approximation and by Nair\\nand Hinton (2010) for the conditional distributions of undirected probabilistic\\nmodels. Glorot et al. (2011a) compared the softplus and rectifier and found'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='better results with the latter. The use of the softplus is generally discouraged.\\nThe softplus demonstrates that the performance of hidden unit types can\\nbe very counterintuitive—one might expect it to have an advantage over\\nthe rectifier due to being differentiable everywhere or due to saturating less\\ncompletely, but empirically it does not.\\nHard tanh: this is shaped similarly to the tanh and the rectifier but unlike\\n•\\nthe latter, it is bounded, g(a) = max( 1,min(1,a)). It was introduced\\n−\\nby Collobert (2004).\\nHidden unit design remains an active area of research and many useful hidden\\nunit types remain to be discovered.\\n6.4 Architecture Design\\nAnotherkeydesignconsiderationforneuralnetworks isdeterminingthearchitecture.\\nThe word architecture refers to the overall structure of the network: how many\\nunits it should have and how these units should be connected to each other.\\nMost neural networks are organized into groups of units called layers. Most'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='neural network architectures arrange these layers in a chain structure, with each\\nlayer being a function of the layer that preceded it. In this structure, the first layer\\nis given by\\nh(1) = g(1) W(1) x+b(1) , (6.40)\\n\\ue03e\\n\\ue010 \\ue011\\nthe second layer is given by\\nh(2) = g(2) W(2) h(1) +b(2) , (6.41)\\n\\ue03e\\nand so on.\\n197\\n\\ue010 \\ue011\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nIn these chain-based architectures, the main architectural considerations are\\nto choose the depth of the network and the width of each layer. As we will see,\\na network with even one hidden layer is sufficient to fit the training set. Deeper\\nnetworks often are able to use far fewer units per layer and far fewer parameters\\nand often generalize to the test set, but are also often harder to optimize. The\\nideal network architecture for a task must be found via experimentation guided by\\nmonitoring the validation set error.\\n6.4.1 Universal Approximation Properties and Depth\\nA linear model, mapping from features to outputs via matrix multiplication, can'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='by definition represent only linear functions. It has the advantage of being easy to\\ntrain because many loss functions result in convex optimization problems when\\napplied to linear models. Unfortunately, we often want to learn nonlinear functions.\\nAt first glance, we might presume that learning a nonlinear function requires\\ndesigning a specialized model family for the kind of nonlinearity we want to learn.\\nFortunately, feedforward networks with hidden layers provide a universal approxi-\\nmation framework. Specifically, the universal approximation theorem (Hornik\\net al., 1989; Cybenko, 1989) states that a feedforward network with a linear output\\nlayer and at least one hidden layer with any “squashing” activation function (such\\nas the logistic sigmoid activation function) can approximate any Borel measurable\\nfunction from one finite-dimensional space to another with any desired non-zero\\namount of error, provided that the network is given enough hidden units. The'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='derivatives of the feedforward network can also approximate the derivatives of the\\nfunction arbitrarily well (Hornik et al., 1990). The concept of Borel measurability\\nis beyond the scope of this book; for our purposes it suffices to say that any\\ncontinuous function on a closed and bounded subset of\\nRn\\nis Borel measurable\\nand therefore may be approximated by a neural network. A neural network may\\nalso approximate any function mapping from any finite dimensional discrete space\\nto another. While the original theorems were first stated in terms of units with\\nactivation functions that saturate both for very negative and for very positive\\narguments, universal approximation theorems have also been proved for a wider\\nclass of activation functions, which includes the now commonly used rectified linear\\nunit (Leshno et al., 1993).\\nThe universal approximation theorem means that regardless of what function\\nwe are trying to learn, we know that a large MLP will be able to represent this'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='function. However, we are not guaranteed that the training algorithm will be able\\nto learn that function. Even if the MLP is able to represent the function, learning\\ncan fail for two different reasons. First, the optimization algorithm used for training\\n198\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nmay not be able to find the value of the parameters that corresponds to the desired\\nfunction. Second, the training algorithm might choose the wrong function due to\\noverfitting. Recall from section 5.2.1 that the “no free lunch” theorem shows that\\nthere is no universally superior machine learning algorithm. Feedforward networks\\nprovide a universal system for representing functions, in the sense that, given a\\nfunction, there exists a feedforward network that approximates the function. There\\nis no universal procedure for examining a training set of specific examples and\\nchoosing a function that will generalize to points not in the training set.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The universal approximation theorem says that there exists a network large\\nenough to achieve any degree of accuracy we desire, but the theorem does not\\nsay how large this network will be. Barron (1993) provides some bounds on the\\nsize of a single-layer network needed to approximate a broad class of functions.\\nUnfortunately, in the worse case, an exponential number of hidden units (possibly\\nwith one hidden unit corresponding to each input configuration that needs to be\\ndistinguished) may be required. This is easiest to see in the binary case: the\\nnumber of possible binary functions on vectors v 0, 1 n is 22n and selecting\\n∈ { }\\none such function requires 2n bits, which will in general require O(2n) degrees of\\nfreedom.\\nIn summary, a feedforward network with a single layer is sufficient to represent\\nany function, but the layer may be infeasibly large and may fail to learn and\\ngeneralize correctly. In many circumstances, using deeper models can reduce the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='number of units required to represent the desired function and can reduce the\\namount of generalization error.\\nThere exist families of functions which can be approximated efficiently by an\\narchitecture with depth greater than some valued, but which require a much larger\\nmodel if depth is restricted to be less than or equal to d. In many cases, the number\\nof hidden units required by the shallow model is exponential in n. Such results\\nwere first proved for models that do not resemble the continuous, differentiable\\nneural networks used for machine learning, but have since been extended to these\\nmodels. The first results were for circuits of logic gates (Håstad, 1986). Later\\nwork extended these results to linear threshold units with non-negative weights\\n(Håstad and Goldmann, 1991; Hajnal et al., 1993), and then to networks with\\ncontinuous-valued activations (Maass, 1992; Maass et al., 1994). Many modern\\nneural networks use rectified linear units. Leshno et al. (1993) demonstrated'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='that shallow networks with a broad family of non-polynomial activation functions,\\nincluding rectified linear units, have universal approximation properties, but these\\nresults do not address the questions of depth or efficiency—they specify only that\\na sufficiently wide rectifier network could represent any function. Montufar et al.\\n199\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\n(2014) showed that functions representable with a deep rectifier net can require\\nan exponential number of hidden units with a shallow (one hidden layer) network.\\nMore precisely, they showed that piecewise linear networks (which can be obtained\\nfrom rectifier nonlinearities or maxout units) can represent functions with a number\\nof regions that is exponential in the depth of the network. Figure 6.5 illustrates how\\na network with absolute value rectification creates mirror images of the function\\ncomputed on top of some hidden unit, with respect to the input of that hidden'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='unit. Each hidden unit specifies where to fold the input space in order to create\\nmirror responses (on both sides of the absolute value nonlinearity). By composing\\nthese folding operations, we obtain an exponentially large number of piecewise\\nlinear regions which can capture all kinds of regular (e.g., repeating) patterns.\\nFigure 6.5: An intuitive, geometric explanation of the exponential advantage of deeper\\nrectifier networks formally by Montufar et al. (2014). (Left)An absolute value rectification\\nunit has the same output for every pair of mirror points in its input. The mirror axis\\nof symmetry is given by the hyperplane defined by the weights and bias of the unit. A\\nfunction computed on top of that unit (the green decision surface) will be a mirror image\\nof a simpler pattern across that axis of symmetry. (Center)The function can be obtained\\nby folding the space around the axis of symmetry. (Right)Another repeating pattern can'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='be folded on top of the first (by another downstream unit) to obtain another symmetry\\n(which is now repeated four times, with two hidden layers). Figure reproduced with\\npermission from Montufar et al. (2014).\\nMore precisely, the main theorem in Montufar et al. (2014) states that the\\nnumber of linear regions carved out by a deep rectifier network with d inputs,\\ndepth l, and n units per hidden layer, is\\nd(l 1)\\nn −\\nO nd , (6.42)\\nd\\n\\ue020 \\ue021\\n\\ue012 \\ue013\\ni.e., exponential in the depth l. In the case of maxout networks with k filters per\\nunit, the number of linear regions is\\nO k(l 1)+d . (6.43)\\n−\\n200\\n\\ue010 \\ue011\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nOf course, there is no guarantee that the kinds of functions we want to learn in\\napplications of machine learning (and in particular for AI) share such a property.\\nWe may also want to choose a deep model for statistical reasons. Any time\\nwe choose a specific machine learning algorithm, we are implicitly stating some'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='set of prior beliefs we have about what kind of function the algorithm should\\nlearn. Choosing a deep model encodes a very general belief that the function we\\nwant to learn should involve composition of several simpler functions. This can be\\ninterpreted from a representation learning point of view as saying that we believe\\nthe learning problem consists of discovering a set of underlying factors of variation\\nthat can in turn be described in terms of other, simpler underlying factors of\\nvariation. Alternately, we can interpret the use of a deep architecture as expressing\\na belief that the function we want to learn is a computer program consisting of\\nmultiple steps, where each step makes use of the previous step’s output. These\\nintermediate outputs are not necessarily factors of variation, but can instead be\\nanalogous to counters or pointers that the network uses to organize its internal\\nprocessing. Empirically, greater depth does seem to result in better generalization'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='for a wide variety of tasks (Bengio et al., 2007; Erhan et al., 2009; Bengio, 2009;\\nMesnil et al., 2011; Ciresan et al., 2012; Krizhevsky et al., 2012; Sermanet et al.,\\n2013; Farabet et al., 2013; Couprie et al., 2013; Kahou et al., 2013; Goodfellow\\net al., 2014d; Szegedy et al., 2014a). See figure 6.6 and figure 6.7 for examples of\\nsome of these empirical results. This suggests that using deep architectures does\\nindeed express a useful prior over the space of functions the model learns.\\n6.4.2 Other Architectural Considerations\\nSo far we have described neural networks as being simple chains of layers, with the\\nmain considerations being the depth of the network and the width of each layer.\\nIn practice, neural networks show considerably more diversity.\\nMany neural network architectures have been developed for specific tasks.\\nSpecialized architectures for computer vision called convolutional networks are\\ndescribed in chapter 9. Feedforward networks may also be generalized to the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='recurrent neural networks for sequence processing, described in chapter 10, which\\nhave their own architectural considerations.\\nIn general, the layers need not be connected in a chain, even though this is the\\nmost common practice. Many architectures build a main chain but then add extra\\narchitectural features to it, such as skip connections going from layer i to layer\\ni+2 or higher. These skip connections make it easier for the gradient to flow from\\noutput layers to layers nearer the input.\\n201\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\n96.5\\n96.0\\n95.5\\n95.0\\n94.5\\n94.0\\n93.5\\n93.0\\n92.5\\n92.0\\n3 4 5 6 7 8 9 10 11\\nNumber of hidden layers\\n)tnecrep(\\nycarucca\\ntseT\\nFigure 6.6: Empirical results showing that deeper networks generalize better when used\\nto transcribe multi-digit numbers from photographs of addresses. Data from Goodfellow\\net al. (2014d). The test set accuracy consistently increases with increasing depth. See'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='figure 6.7 for a control experiment demonstrating that other increases to the model size\\ndo not yield the same effect.\\nAnother key consideration of architecture design is exactly how to connect a\\npair of layers to each other. In the default neural network layerdescribed by a linear\\ntransformation via a matrix W, every input unit is connected to every output\\nunit. Many specialized networks in the chapters ahead have fewer connections, so\\nthat each unit in the input layer is connected to only a small subset of units in\\nthe output layer. These strategies for reducing the number of connections reduce\\nthe number of parameters and the amount of computation required to evaluate\\nthe network, but are often highly problem-dependent. For example, convolutional\\nnetworks, described in chapter 9, use specialized patterns of sparse connections\\nthat are very effective for computer vision problems. In this chapter, it is difficult'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to give much more specific advice concerning the architecture of a generic neural\\nnetwork. Subsequent chapters develop the particular architectural strategies that\\nhave been found to work well for different application domains.\\n202\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\n97\\n96\\n95\\n94\\n93\\n92\\n91\\n0.0 0.2 0.4 0.6 0.8 1.0\\nNumber of parameters\\n108\\n×\\n)tnecrep(\\nycarucca\\ntseT\\n3, convolutional\\n3, fully connected\\n11, convolutional\\nFigure 6.7: Deeper models tend to perform better. This is not merely because the model is\\nlarger. This experiment from Goodfellow et al. (2014d) shows that increasing the number\\nof parameters in layers of convolutional networks without increasing their depth is not\\nnearly as effective at increasing test set performance. The legend indicates the depth of\\nnetwork used to make each curve and whether the curve represents variation in the size of\\nthe convolutional or the fully connected layers. We observe that shallow models in this'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='context overfit at around 20 million parameters while deep ones can benefit from having\\nover 60 million. This suggests that using a deep model expresses a useful preference over\\nthe space of functions the model can learn. Specifically, it expresses a belief that the\\nfunction should consist of many simpler functions composed together. This could result\\neither in learning a representation that is composed in turn of simpler representations(e.g.,\\ncorners defined in terms of edges) or in learning a program with sequentially dependent\\nsteps (e.g., first locate a set of objects, then segment them from each other, then recognize\\nthem).\\n203\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\n6.5 Back-Propagation and Other Differentiation Algo-\\nrithms\\nWhen we use a feedforward neural network to accept an input x and produce an\\noutput yˆ, information flows forward through the network. The inputs x provide\\nthe initial information that then propagates up to the hidden units at each layer'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='and finally produces yˆ. This is called forward propagation. During training,\\nforward propagation can continue onward until it produces a scalar cost J(θ).\\nThe back-propagation algorithm (Rumelhart et al., 1986a), often simply called\\nbackprop, allows the information from the cost to then flow backwards through\\nthe network, in order to compute the gradient.\\nComputing an analytical expression for the gradient is straightforward, but\\nnumerically evaluating such an expression can be computationally expensive. The\\nback-propagation algorithm does so using a simple and inexpensive procedure.\\nThe term back-propagation is often misunderstood as meaning the whole\\nlearning algorithm for multi-layer neural networks. Actually, back-propagation\\nrefers only to the method for computing the gradient, while another algorithm,\\nsuch as stochastic gradient descent, is used to perform learning using this gradient.\\nFurthermore, back-propagation is often misunderstood as being specific to multi-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='layer neural networks, but in principle it can compute derivatives of any function\\n(for some functions, the correct response is to report that the derivative of the\\nfunction is undefined). Specifically, we will describe how to compute the gradient\\nf(x,y)for an arbitraryfunction f, wherex isa set ofvariableswhose derivatives\\nx\\n∇\\nare desired, and y is an additional set of variables that are inputs to the function\\nbut whose derivativesare not required. In learning algorithms, the gradient we most\\noften require is the gradient of the cost function with respect to the parameters,\\nJ(θ). Many machine learning tasks involve computing other derivatives, either\\nθ\\n∇\\nas part of the learning process, or to analyze the learned model. The back-\\npropagation algorithm can be applied to these tasks as well, and is not restricted\\nto computing the gradient of the cost function with respect to the parameters. The\\nidea of computing derivatives by propagating information through a network is'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='very general, and can be used to compute values such as the Jacobian of a function\\nf with multiple outputs. We restrict our description here to the most commonly\\nused case where f has a single output.\\n204\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\n6.5.1 Computational Graphs\\nSo far we have discussed neural networks with a relatively informal graph language.\\nTo describe the back-propagation algorithm more precisely, it is helpful to have a\\nmore precise computational graph language.\\nMany ways of formalizing computation as graphs are possible.\\nHere, we use each node in the graph to indicate a variable. The variable may\\nbe a scalar, vector, matrix, tensor, or even a variable of another type.\\nTo formalize our graphs, we also need to introduce the idea of an operation.\\nAn operation is a simple function of one or more variables. Our graph language\\nis accompanied by a set of allowable operations. Functions more complicated\\nthan the operations in this set may be described by composing many operations'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='together.\\nWithout loss of generality, we define an operation to return only a single\\noutput variable. This does not lose generality because the output variable can have\\nmultiple entries, such as a vector. Software implementations of back-propagation\\nusually support operations with multiple outputs, but we avoid this case in our\\ndescription because it introduces many extra details that are not important to\\nconceptual understanding.\\nIf a variable y is computed by applying an operation to a variable x, then\\nwe draw a directed edge from x to y. We sometimes annotate the output node\\nwith the name of the operation applied, and other times omit this label when the\\noperation is clear from context.\\nExamples of computational graphs are shown in figure 6.8.\\n6.5.2 Chain Rule of Calculus\\nThe chain rule of calculus (not to be confused with the chain rule of probability) is\\nused to compute the derivatives of functions formed by composing other functions'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='whose derivatives are known. Back-propagation is an algorithm that computes the\\nchain rule, with a specific order of operations that is highly efficient.\\nLet x be a real number, and let f and g both be functions mapping from a real\\nnumber to a real number. Suppose that y = g(x) and z = f(g(x)) = f(y). Then\\nthe chain rule states that\\ndz dz dy\\n= . (6.44)\\ndx dydx\\nWe can generalize this beyond the scalar case. Suppose that x\\nRm,\\ny\\nRn,\\n∈ ∈\\n205\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nyyˆˆ\\nσ\\nzz uu((11)) uu((22))\\n+\\ndot\\n×\\nxx yy xx ww bb\\n(a) (b)\\nHH uu((22)) uu((33))\\nrelu\\nsum ×\\nUU((11)) UU((22)) yyˆˆ uu((11))\\n+\\nsqr\\ndot\\nmatmul\\nXX WW bb xx ww λλ\\n(c) (d)\\nFigure 6.8: Examples of computational graphs. (a)The graph using the operation to\\n×\\ncompute z = xy. (b)The graph for the logistic regression prediction yˆ = σ x w+b .\\n\\ue03e\\nSome of the intermediate expressions do not have names in the algebraic expression\\n\\ue000 \\ue001\\nbut need names in the graph. We simply name the i-th such variable u(i). (c)The'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='computational graph for the expression H = max 0,XW + b , which computes a design\\n{ }\\nmatrix of rectified linear unit activations H given a design matrix containing a minibatch\\nof inputs X. (d)Examples a–c applied at most one operation to each variable, but it\\nis possible to apply more than one operation. Here we show a computation graph that\\napplies more than one operation to the weights w of a linear regression model. The\\nweights are used to make both the prediction yˆ and the weight decay penalty λ w2.\\ni i\\n\\ue050\\n206\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\ng maps from\\nRm\\nto\\nRn,\\nand f maps from\\nRn\\nto\\nR\\n. Ify = g(x) and z = f(y), then\\n∂z ∂z ∂y\\nj\\n= . (6.45)\\n∂x ∂y ∂x\\ni j i\\nj\\n\\ue058\\nIn vector notation, this may be equivalently written as\\n∂y\\n\\ue03e\\nz = z, (6.46)\\nx y\\n∇ ∂x ∇\\n\\ue012 \\ue013\\n∂y\\nwhere is the n m Jacobian matrix of g.\\n∂x\\n×\\nFrom this wesee that the gradient ofa variable x canbe obtained by multiplying\\n∂y\\na Jacobian matrix by a gradient z. The back-propagation algorithm consists\\n∂x ∇y'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of performing such a Jacobian-gradient product for each operation in the graph.\\nUsually we do not apply the back-propagation algorithm merely to vectors,\\nbut rather to tensors of arbitrary dimensionality. Conceptually, this is exactly the\\nsame as back-propagation with vectors. The only difference is how the numbers\\nare arranged in a grid to form a tensor. We could imagine flattening each tensor\\ninto a vector before we run back-propagation, computing a vector-valued gradient,\\nand then reshaping the gradient back into a tensor. In this rearranged view,\\nback-propagation is still just multiplying Jacobians by gradients.\\nX\\nTo denote the gradient of a value z with respect to a tensor , we write Xz,\\n∇\\nX X\\njust as if were a vector. The indices into now have multiple coordinates—for\\nexample, a 3-D tensor is indexed by three coordinates. We can abstract this away\\nby using a single variable i to represent the complete tuple of indices. For all\\npossible index tuples i, ( ∇Xz)\\ni'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='gives ∂∂ Xz i. This is exactly the same as how for all\\npossible integer indices i into a vector, ( z) gives ∂z . Using this notation, we\\n∇x i Y∂x\\ni X Y\\ncan write the chain rule as it applies to tensors. If = g( ) and z = f( ), then\\n∂z\\nXz = ( XY j)\\nY\\n. (6.47)\\n∇ ∇ ∂\\nj\\nj\\n\\ue058\\n6.5.3 Recursively Applying the Chain Rule to Obtain Backprop\\nUsing the chain rule, it is straightforward to write down an algebraic expression for\\nthe gradient of a scalar with respect to any node in the computational graph that\\nproduced that scalar. However, actually evaluating that expression in a computer\\nintroduces some extra considerations.\\nSpecifically, many subexpressions may be repeated several times within the\\noverall expression for the gradient. Any procedure that computes the gradient\\n207\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nwill need to choose whether to store these subexpressions or to recompute them\\nseveral times. An example of how these repeated subexpressions arise is given in'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='figure 6.9. In some cases, computing the same subexpression twice would simply\\nbe wasteful. For complicated graphs, there can be exponentially many of these\\nwasted computations, making a naive implementation of the chain rule infeasible.\\nIn other cases, computing the same subexpression twice could be a valid way to\\nreduce memory consumption at the cost of higher runtime.\\nWe first begin by a version of the back-propagation algorithm that specifies the\\nactual gradientcomputation directly (algorithm 6.2 along with algorithm 6.1 for the\\nassociated forwardcomputation), in the order it will actually be done and according\\nto the recursive application of chain rule. One could either directly perform these\\ncomputations or view the description of the algorithm as a symbolic specification\\nof the computational graph for computing the back-propagation. However, this\\nformulation does not make explicit the manipulation and the construction of the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='symbolic graph that performs the gradient computation. Such a formulation is\\npresented below in section 6.5.6, with algorithm 6.5, where we also generalize to\\nnodes that contain arbitrary tensors.\\nFirst consider a computational graph describing how to compute a single scalar\\nu(n) (say the loss on a training example). This scalar is the quantity whose\\ngradient we want to obtain, with respect to the n input nodes u(1) to u(n i). In\\ni\\nother words we wish to compute\\n∂u(n)\\nfor all i 1,2,...,n . In the application\\n∂u(i) ∈ { i }\\nof back-propagation to computing gradients for gradient descent over parameters,\\nu(n) will be the cost associated with an example or a minibatch, while u(1) to u(n i)\\ncorrespond to the parameters of the model.\\nWe will assume that the nodes of the graph have been ordered in such a way\\nthat we can compute their output one after the other, starting at u(n i+1) and\\ngoing up to u(n). As defined in algorithm 6.1, each node u(i) is associated with an'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='operation f(i) and is computed by evaluating the function\\nu(i)= f(A(i)) (6.48)\\nwhere A(i) is the set of all nodes that are parents of u(i).\\nThat algorithm specifies the forward propagation computation, which we could\\nput in a graph . In order to perform back-propagation, we can construct a\\nG\\ncomputational graph that depends on and adds to it an extra set of nodes. These\\nG\\nform a subgraph with one node per node of . Computation in proceeds in\\nB G B\\nexactly the reverse of the order of computation in , and each node of computes\\nG B\\nthe derivative ∂u(n) associated with the forward graph node u(i). This is done\\n∂u(i)\\n208\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nAlgorithm 6.1 A procedure that performs the computations mapping n inputs\\ni\\nu(1) to u(n i) to an output u(n). This defines a computational graph where each node\\ncomputes numerical value u(i) by applying a function f(i) to the set of arguments\\nA(i) that comprises the values of previous nodes u(j), j < i, withj Pa(u(i)). The\\n∈'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='input to the computational graph is the vector x, and is set into the first n nodes\\ni\\nu(1) to u(n i). The output of the computational graph is read off the last (output)\\nnode u(n).\\nfor i = 1,...,n do\\ni\\nu(i) x\\ni\\n←\\nend for\\nfor i = n +1,...,n do\\ni\\nA(i) u(j) j Pa(u(i))\\n← { | ∈ }\\nu(i) f(i)(A(i))\\n←\\nend for\\nreturn u(n)\\nusing the chain rule with respect to scalar output u(n):\\n∂u(n) ∂u(n) ∂u(i)\\n= (6.49)\\n∂u(j) ∂u(i) ∂u(j)\\ni:j P\\ue058a(u(i))\\n∈\\nas specified by algorithm 6.2. The subgraph contains exactly one edge for each\\nB\\nedge from node u(j) to node u(i) of . The edge from u(j) to u(i) is associated with\\nG\\nthe computation of\\n∂u(i)\\n. In addition, a dot product is performed for each node,\\n∂u(j)\\nbetween the gradient already computed with respect to nodes u(i) that are children\\nof u(j) and the vector containing the partial derivatives ∂u(i) for the same children\\n∂u(j)\\nnodes u(i). To summarize, the amount of computation required for performing'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the back-propagation scales linearly with the number of edges in , where the\\nG\\ncomputation for each edge corresponds to computing a partial derivative (of one\\nnode with respect to one of its parents) as well as performing one multiplication\\nand one addition. Below, we generalize this analysis to tensor-valued nodes, which\\nis just a way to group multiple scalar values in the same node and enable more\\nefficient implementations.\\nThe back-propagation algorithm is designed to reduce the number of common\\nsubexpressions without regard to memory. Specifically, it performs on the order\\nof one Jacobian product per node in the graph. This can be seen from the fact\\nthat backprop (algorithm 6.2) visits each edge from node u(j) to node u(i) of\\nthe graph exactly once in order to obtain the associated partial derivative\\n∂u(i)\\n.\\n∂u(j)\\n209\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nAlgorithm 6.2 Simplifiedversionof theback-propagationalgorithm for computing'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the derivatives of u(n) with respect to the variables in the graph. This example is\\nintended to further understanding by showing a simplified case where all variables\\nare scalars, and we wish to compute the derivatives with respect to u(1),...,u(n i).\\nThis simplified version computes the derivatives of all nodes in the graph. The\\ncomputational cost of this algorithm is proportional to the number of edges in\\nthe graph, assuming that the partial derivative associated with each edge requires\\na constant time. This is of the same order as the number of computations for\\nthe forward propagation. Each ∂u(i) is a function of the parents u(j) of u(i), thus\\n∂u(j)\\nlinking the nodes of the forward graph to those added for the back-propagation\\ngraph.\\nRun forward propagation (algorithm 6.1 for this example) to obtain the activa-\\ntions of the network\\nInitialize grad_table, a data structure that will store the derivatives that have'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='been computed. The entry grad_table[u(i)] will store the computed value of\\n∂u(n)\\n.\\n∂u(i)\\ngrad_table[u(n)] 1\\n←\\nfor j = n 1 down to 1 do\\n−\\nThe next line computes\\n∂u(n)\\n=\\n∂u(n) ∂u(i)\\nusing stored values:\\n∂u(j) i:j Pa(u(i)) ∂u(i) ∂u(j)\\n∈\\ngrad_table[u(j)] grad_table[u(i)]∂u(i)\\n← i:j Pa(u(i)\\ue050) ∂u(j)\\nend for ∈\\n\\ue050\\nreturn grad_table[u(i)] i = 1,...,n\\ni\\n{ | }\\nBack-propagation thus avoids the exponential explosion in repeated subexpressions.\\nHowever, other algorithms may be able to avoid more subexpressions by performing\\nsimplifications on the computational graph, or may be able to conserve memory by\\nrecomputing rather than storing some subexpressions. We will revisit these ideas\\nafter describing the back-propagation algorithm itself.\\n6.5.4 Back-Propagation Computation in Fully-Connected MLP\\nTo clarify the above definition of the back-propagation computation, let us consider\\nthe specific graph associated with a fully-connected multi-layer MLP.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Algorithm 6.3 first shows the forward propagation, which maps parameters to\\nthe supervised loss L(yˆ,y) associated with a single (input,target) training example\\n(x,y), with ˆy the output of the neural network when x is provided in input.\\nAlgorithm 6.4 then shows the corresponding computation to be done for\\n210\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nzz\\nf\\nyy\\nf\\nxx\\nf\\nww\\nFigure6.9: Acomputationalgraphthat resultsinrepeatedsubexpressionswhencomputing\\nthe gradient. Let w R be the input to the graph. We use the same function f :R R\\n∈ →\\nas the operation that we apply at every step of a chain: x = f(w), y = f(x), z = f(y).\\nTo compute ∂z , we apply equation 6.44 and obtain:\\n∂w\\n∂z\\n(6.50)\\n∂w\\n∂z∂y ∂x\\n= (6.51)\\n∂y∂x ∂w\\n=f (y)f (x)f (w) (6.52)\\n\\ue030 \\ue030 \\ue030\\n=f (f(f(w)))f (f(w))f (w) (6.53)\\n\\ue030 \\ue030 \\ue030\\nEquation 6.52 suggests an implementation in which we compute the value of f(w) only\\nonce and store it in the variable x. This is the approach taken by the back-propagation'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='algorithm. An alternative approachis suggested by equation 6.53, where the subexpression\\nf(w) appears more than once. In the alternative approach, f(w) is recomputed each time\\nit is needed. When the memory required to store the value of these expressions is low, the\\nback-propagation approach of equation 6.52 is clearly preferable because of its reduced\\nruntime. However, equation 6.53 is also a valid implementation of the chain rule, and is\\nuseful when memory is limited.\\n211\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\napplying the back-propagation algorithm to this graph.\\nAlgorithms 6.3 and 6.4 are demonstrations that are chosen to be simple and\\nstraightforward to understand. However, they are specialized to one specific\\nproblem.\\nModern software implementations are based on the generalized form of back-\\npropagation described in section 6.5.6 below, which can accommodate any compu-\\ntational graph by explicitly manipulating a data structure for representing symbolic\\ncomputation.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Algorithm 6.3 Forward propagation through a typical deep neural network and\\nthe computation of the cost function. The loss L(yˆ,y) depends on the output\\nyˆ and on the target y (see section 6.2.1.1 for examples of loss functions). To\\nobtain the total cost J, the loss may be added to a regularizer Ω(θ), where θ\\ncontains all the parameters (weights and biases). Algorithm 6.4 shows how to\\ncompute gradients of J with respect to parameters W and b. For simplicity, this\\ndemonstration uses only a single input example x. Practical applications should\\nuse a minibatch. See section 6.5.7 for a more realistic demonstration.\\nRequire: Network depth, l\\nRequire: W(i),i 1,...,l , the weight matrices of the model\\n∈ { }\\nRequire: b(i),i 1,...,l , the bias parameters of the model\\n∈ { }\\nRequire: x, the input to process\\nRequire: y, the target output\\nh(0) = x\\nfor k = 1,...,l do\\na(k) = b(k) +W(k)h(k 1)\\n−\\nh(k) = f(a(k))\\nend for\\nyˆ = h(l)\\nJ = L(yˆ,y)+λΩ(θ)\\n6.5.5 Symbol-to-Symbol Derivatives'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Algebraic expressions and computational graphs both operate on symbols, or\\nvariables that do not have specific values. These algebraic and graph-based\\nrepresentations are called symbolic representations. When we actually use or\\ntrain a neural network, we must assign specific values to these symbols. We\\nreplace a symbolic input to the network x with a specific numeric value, such as\\n[1.2,3.765, 1.8] .\\n\\ue03e\\n−\\n212\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nAlgorithm 6.4 Backward computation for the deep neural network of algo-\\nrithm 6.3, which uses in addition to the input x a target y. This computation\\nyields the gradients on the activations a(k) for each layer k, starting from the\\noutput layer and going backwards to the first hidden layer. From these gradients,\\nwhich can be interpreted as an indication of how each layer’s output should change\\nto reduce error, one can obtain the gradient on the parameters of each layer. The'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='gradients on weights and biases can be immediately used as part of a stochas-\\ntic gradient update (performing the update right after the gradients have been\\ncomputed) or used with other gradient-based optimization methods.\\nAfter the forward computation, compute the gradient on the output layer:\\ng J = L(yˆ,y)\\nyˆ yˆ\\n← ∇ ∇\\nfor k = l,l 1,...,1 do\\n−\\nConvert the gradient on the layer’s output into a gradient into the pre-\\nnonlinearity activation (element-wise multiplication if f is element-wise):\\ng J = g f (a(k))\\na(k) \\ue030\\n← ∇ \\ue00c\\nCompute gradients on weights and biases (including the regularization term,\\nwhere needed):\\nJ = g +λ Ω(θ)\\nb(k) b(k)\\n∇ ∇\\nJ = g h(k 1) +λ Ω(θ)\\nW (k) − \\ue03e W(k)\\n∇ ∇\\nPropagate the gradients w.r.t. the next lower-level hidden layer’s activations:\\ng J = W(k) g\\nh(k 1) \\ue03e\\n← ∇ −\\nend for\\n213\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nzz zz\\nf f\\nf\\ue021\\nddzz\\nyy yy\\nddyy\\nf f\\nf\\ue021\\nddyy ddzz\\nxx xx ×\\nddxx ddxx\\nf f\\nf\\ue021\\nddxx × ddzz\\nww ww\\nddww ddww'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Figure 6.10: An example of the symbol-to-symbol approach to computing derivatives. In\\nthis approach, the back-propagation algorithm does not need to ever access any actual\\nspecific numeric values. Instead, it adds nodes to a computational graph describing how\\nto compute these derivatives. A generic graph evaluation engine can later compute the\\nderivatives for any specific numeric values. (Left)In this example, we begin with a graph\\nrepresenting z = f(f(f(w))). (Right)We run the back-propagation algorithm, instructing\\nit to construct the graph for the expression corresponding to dz. In this example, we do\\ndw\\nnot explain how the back-propagation algorithm works. The purpose is only to illustrate\\nwhat the desired result is: a computational graph with a symbolic description of the\\nderivative.\\nSome approaches to back-propagation take a computational graph and a set\\nof numerical values for the inputs to the graph, then return a set of numerical'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='values describing the gradient at those input values. We call this approach “symbol-\\nto-number” differentiation. This is the approach used by libraries such as Torch\\n(Collobert et al., 2011b) and Caffe (Jia, 2013).\\nAnother approach is to take a computational graph and add additional nodes\\nto the graph that provide a symbolic description of the desired derivatives. This\\nis the approach taken by Theano (Bergstra et al., 2010; Bastien et al., 2012)\\nand TensorFlow (Abadi et al., 2015). An example of how this approach works\\nis illustrated in figure 6.10. The primary advantage of this approach is that\\nthe derivatives are described in the same language as the original expression.\\nBecause the derivatives are just another computational graph, it is possible to run\\nback-propagation again, differentiating the derivatives in order to obtain higher\\nderivatives. Computation of higher-order derivatives is described in section 6.5.10.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='We will use the latter approach and describe the back-propagation algorithm in\\n214\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nterms of constructing a computational graph for the derivatives. Any subset of the\\ngraph may then be evaluated using specific numerical values at a later time. This\\nallows us to avoid specifying exactly when each operation should be computed.\\nInstead, a generic graph evaluation engine can evaluate every node as soon as its\\nparents’ values are available.\\nThe description of the symbol-to-symbol based approach subsumes the symbol-\\nto-number approach. The symbol-to-number approach can be understood as\\nperforming exactly the same computations as are done in the graph built by the\\nsymbol-to-symbol approach. The key difference is that the symbol-to-number\\napproach does not expose the graph.\\n6.5.6 General Back-Propagation\\nThe back-propagation algorithm is very simple. To compute the gradient of some'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='scalar z with respect to one of its ancestors x in the graph, we begin by observing\\nthat the gradient with respect to z is given by dz = 1. We can then compute\\ndz\\nthe gradient with respect to each parent of z in the graph by multiplying the\\ncurrent gradient by the Jacobian of the operation that produced z. We continue\\nmultiplying by Jacobians traveling backwards through the graph in this way until\\nwe reach x. For any node that may be reached by going backwards from z through\\ntwo or more paths, we simply sum the gradients arriving from different paths at\\nthat node.\\nMore formally, each node in the graph corresponds to a variable. To achieve\\nG\\nV\\nmaximum generality, we describe this variable as being a tensor . Tensor can\\nin general have any number of dimensions. They subsume scalars, vectors, and\\nmatrices.\\nV\\nWe assume that each variable is associated with the following subroutines:\\nget_operation(V\\n): This returns the operation that computes\\nV\\n, repre-\\n•\\nV'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='sented by the edges coming into in the computational graph. For example,\\nthere may be a Python or C++ class representing the matrix multiplication\\noperation, and theget_operation function. Suppose we have a variable that\\nis created by matrix multiplication, C = AB. Then\\nget_operation(V\\n)\\nreturns a pointer to an instance of the corresponding C++ class.\\nget_consumers(V\\n, ): This returns the list of variables that are children of\\n• G\\nV\\nin the computational graph .\\nG\\nget_inputs(V , ): This returns the list of variables that are parents of V\\n• G\\nin the computational graph .\\nG\\n215\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nEach operation op is also associated with a bprop operation. This bprop\\noperation can compute a Jacobian-vector product as described by equation 6.47.\\nThis is how the back-propagation algorithm is able to achieve great generality.\\nEach operation is responsible for knowing how to back-propagate through the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='edges in the graph that it participates in. For example, we might use a matrix\\nmultiplication operation to create a variable C = AB. Suppose that the gradient\\nof a scalar z with respect to C is given by G. The matrix multiplication operation\\nis responsible for defining two back-propagation rules, one for each of its input\\narguments. If we call the bprop method to request the gradient with respect to\\nA given that the gradient on the output is G, then the bprop method of the\\nmatrix multiplication operation must state that the gradient with respect to A\\nis given by GB . Likewise, if we call the bprop method to request the gradient\\n\\ue03e\\nwith respect to B, then the matrix operation is responsible for implementing the\\nbprop method and specifying that the desired gradient is given by A G. The\\n\\ue03e\\nback-propagation algorithm itself does not need to know anydifferentiation rules. It\\nonly needs to call each operation’s bprop rules with the right arguments. Formally,\\nop.bprop(inputs,X ,G\\n) must return'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='( Xop.f(inputs) i)G i, (6.54)\\n∇\\ni\\n\\ue058\\nwhich is just an implementation of the chain rule as expressed in equation 6.47.\\nHere, inputs is a list of inputs that are supplied to the operation, op.f is the\\nX\\nmathematicalfunction that the operation implements, isthe input whose gradient\\nG\\nwe wish to compute, and is the gradient on the output of the operation.\\nThe op.bprop method should always pretend that all of its inputs are distinct\\nfrom each other, even if they are not. For example, if the mul operator is passed\\ntwo copies of x to compute x2, the op.bprop method should still return x as the\\nderivative with respect to both inputs. The back-propagation algorithm will later\\nadd both of these arguments together to obtain 2x, which is the correct total\\nderivative on x.\\nSoftware implementations of back-propagation usually provide both the opera-\\ntions and their bprop methods, so that users of deep learning software libraries are'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='able to back-propagate through graphs built using common operations like matrix\\nmultiplication, exponents, logarithms, and so on. Software engineers who build a\\nnew implementation of back-propagation or advanced users who need to add their\\nown operation to an existing library must usually derive the op.bprop method for\\nany new operations manually.\\nThe back-propagation algorithm is formally described in algorithm 6.5.\\n216\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nAlgorithm 6.5 The outermost skeleton of the back-propagation algorithm. This\\nportion does simple setup and cleanup work. Most of the important work happens\\nin the build_grad subroutine of algorithm 6.6\\n.\\nT\\nRequire: , the target set of variables whose gradients must be computed.\\nRequire: , the computational graph\\nG\\nRequire: z, the variable to be differentiated\\nLet be pruned to contain only nodes that are ancestors of z and descendents\\n\\ue030\\nG G\\nT\\nof nodes in .'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Initialize grad_table, a data structure associating tensors to their gradients\\ngrad_table[z] 1\\n←\\nV T\\nfor in do\\nbuild_grad(V , , ,grad_table)\\n\\ue030\\nG G\\nend for\\nReturn grad_table restricted to T\\nIn section 6.5.2, we explained that back-propagation was developed in order to\\navoid computing the same subexpression in the chain rule multiple times. The naive\\nalgorithm could have exponential runtime due to these repeated subexpressions.\\nNow that we have specified the back-propagation algorithm, we can understand its\\ncomputational cost. If we assume that each operation evaluation has roughly the\\nsame cost, then we may analyze the computational cost in terms of the number\\nof operations executed. Keep in mind here that we refer to an operation as the\\nfundamental unit of our computational graph, which might actually consist of very\\nmany arithmetic operations (for example, we might have a graph that treats matrix\\nmultiplication as a single operation). Computing a gradientin a graph with n nodes'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='will never execute more than O(n2) operations or store the output of more than\\nO(n2) operations. Here we are counting operations in the computational graph, not\\nindividual operations executed by the underlying hardware, so it is important to\\nremember that the runtime of each operation may be highly variable. For example,\\nmultiplying two matrices that each contain millions of entries might correspond to\\na single operation in the graph. We can see that computing the gradient requires as\\nmost O(n2) operations because the forward propagation stage will at worst execute\\nall n nodes in the original graph (depending on which values we want to compute,\\nwe may not need to execute the entire graph). The back-propagation algorithm\\nadds one Jacobian-vector product, which should be expressed with O(1) nodes, per\\nedge in the original graph. Because the computational graph is a directed acyclic\\ngraph it has at mostO(n2) edges. For the kinds of graphs that are commonly used'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='in practice, the situation is even better. Most neural network cost functions are\\n217\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nAlgorithm 6.6 The inner loop subroutine build_grad(V , , ,grad_table) of\\n\\ue030\\nG G\\nthe back-propagation algorithm, called by the back-propagation algorithm defined\\nin algorithm 6.5.\\nRequire: V , the variable whose gradient should be added to and grad_table.\\nG\\nRequire: , the graph to modify.\\nG\\nRequire: , the restriction of to nodes that participate in the gradient.\\n\\ue030\\nG G\\nRequire: grad_table, a data structure mapping nodes to their gradients\\nif V is in grad_table then\\nReturn\\ngrad_table[V\\n]\\nend if\\ni 1\\n←\\nfor\\nC\\nin\\nget_consumers(V\\n, ) do\\n\\ue030\\nG\\nop get_operation(C )\\n←\\nD build_grad(C , , ,grad_table)\\n\\ue030\\n← G G\\nG(i) op.bprop(get_inputs(C\\n,\\n),V ,D\\n)\\n\\ue030\\n← G\\ni i+1\\n←\\nend for\\nG G(i)\\n← i\\ngrad_table[V\\n] =\\nG\\n\\ue050\\nG\\nInsert and the operations creating it into\\nG\\nG\\nReturn\\nroughly chain-structured, causing back-propagation to have O(n) cost. This is far'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='better than the naive approach, which might need to execute exponentially many\\nnodes. This potentially exponential cost can be seen by expanding and rewriting\\nthe recursive chain rule (equation 6.49) non-recursively:\\nt\\n∂u(n) ∂u(π k)\\n= . (6.55)\\n∂u(j) ∂u(π\\nk\\n1)\\npath(u(π1) \\ue058,u(π2),...,u(πt)), \\ue059k=2 −\\nfromπ =jtoπ =n\\n1 t\\nSince the number of paths from node j to node n can grow exponentially in the\\nlength of these paths, the number of terms in the above sum, which is the number\\nof such paths, can grow exponentially with the depth of the forward propagation\\ngraph. This large cost would be incurred because the same computation for\\n∂u(i)\\nwould be redone many times. To avoid such recomputation, we can think\\n∂u(j)\\nof back-propagation as a table-filling algorithm that takes advantage of storing\\nintermediate results\\n∂u(n)\\n. Each node in the graph has a corresponding slot in a\\n∂u(i)\\ntable to store the gradient for that node. By filling in these table entries in order,\\n218'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nback-propagation avoids repeating many common subexpressions. This table-filling\\nstrategy is sometimes called dynamic programming.\\n6.5.7 Example: Back-Propagation for MLP Training\\nAs an example, we walk through the back-propagation algorithm as it is used to\\ntrain a multilayer perceptron.\\nHere we develop a very simple multilayer perception with a single hidden\\nlayer. To train this model, we will use minibatch stochastic gradient descent.\\nThe back-propagation algorithm is used to compute the gradient of the cost on a\\nsingle minibatch. Specifically, we use a minibatch of examples from the training\\nset formatted as a design matrix X and a vector of associated class labels y.\\nThe network computes a layer of hidden features H = max 0,XW(1) . To\\n{ }\\nsimplify the presentation we do not use biases in this model. We assume that our\\ngraph language includes a relu operation that can compute max 0,Z element-\\n{ }'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='wise. The predictions of the unnormalized log probabilities over classes are then\\ngiven by HW(2). We assume that our graph language includes a cross_entropy\\noperationthat computesthe cross-entropybetween thetargets yandthe probability\\ndistribution defined by these unnormalized log probabilities. The resulting cross-\\nentropy defines the cost J . Minimizing this cross-entropy performs maximum\\nMLE\\nlikelihood estimation of the classifier. However, to make this example more realistic,\\nwe also include a regularization term. The total cost\\n2 2\\n(1) (2)\\nJ = J +λ W + W (6.56)\\nMLE \\uf8eb i,j i,j \\uf8f6\\n\\ue058i,j \\ue010 \\ue011 \\ue058i,j \\ue010 \\ue011\\n\\uf8ed \\uf8f8\\nconsists of the cross-entropy and a weight decay term with coefficient λ. The\\ncomputational graph is illustrated in figure 6.11.\\nThe computational graph for the gradient of this example is large enough that\\nit would be tedious to draw or to read. This demonstrates one of the benefits\\nof the back-propagation algorithm, which is that it can automatically generate'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='gradients that would be straightforward but tedious for a software engineer to\\nderive manually.\\nWe can roughly trace out the behavior of the back-propagation algorithm\\nby looking at the forward propagation graph in figure 6.11. To train, we wish\\nto compute both J and J. There are two different paths leading\\nW(1) W(2)\\n∇ ∇\\nbackward from J to the weights: one through the cross-entropy cost, and one\\nthrough the weight decay cost. The weight decay cost is relatively simple; it will\\nalways contribute 2λW(i) to the gradient on W(i).\\n219\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nJJ MMLLEE JJ\\ncross_entropy\\n+\\nUU((22)) yy uu((88))\\nmatmul\\n×\\nHH WW((22)) UU((55)) uu((66)) uu((77)) λλ\\nsqr sum\\n+\\nrelu\\nUU((11))\\nmatmul\\nXX WW((11)) UU((33)) uu((44))\\nsqr sum\\nFigure 6.11: The computational graph used to compute the cost used to train our example\\nof a single-layer MLP using the cross-entropy loss and weight decay.\\nThe other path through the cross-entropy cost is slightly more complicated.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Let G be the gradient on the unnormalized log probabilities U(2) provided by\\nthe cross_entropy operation. The back-propagation algorithm now needs to\\nexplore two different branches. On the shorter branch, it adds H G to the\\n\\ue03e\\ngradient on W(2), using the back-propagation rule for the second argument to\\nthe matrix multiplication operation. The other branch corresponds to the longer\\nchain descending further along the network. First, the back-propagation algorithm\\ncomputes J =GW(2) using the back-propagation rule for the first argument\\nH \\ue03e\\n∇\\nto the matrix multiplication operation. Next, the relu operation uses its back-\\npropagation rule to zero out components of the gradient corresponding to entries\\nof U(1) that were less than 0. Let the result be called G . The last step of the\\n\\ue030\\nback-propagation algorithm is to use the back-propagation rule for the second\\nargument of the matmul operation to add X G to the gradient on W(1).\\n\\ue03e \\ue030'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='After these gradients have been computed, it is the responsibility of the gradient\\ndescent algorithm, or another optimization algorithm, to use these gradients to\\nupdate the parameters.\\nFor the MLP, the computational cost is dominated by the cost of matrix\\nmultiplication. During the forward propagation stage, we multiply by each weight\\n220\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nmatrix, resulting in O(w) multiply-adds, where w is the number of weights. During\\nthe backward propagation stage, we multiply by the transpose of each weight\\nmatrix, which has the same computational cost. The main memory cost of the\\nalgorithm is that we need to store the input to the nonlinearity of the hidden layer.\\nThis value is stored from the time it is computed until the backward pass has\\nreturned to the same point. The memory cost is thus O(mn ), where m is the\\nh\\nnumber of examples in the minibatch and n is the number of hidden units.\\nh\\n6.5.8 Complications'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Our description of the back-propagation algorithm here is simpler than the imple-\\nmentations actually used in practice.\\nAs noted above, we have restricted the definition of an operation to be a\\nfunction that returns a single tensor. Most software implementations need to\\nsupport operations that can return more than one tensor. For example, if we wish\\nto compute both the maximum value in a tensor and the index of that value, it is\\nbest to compute both in a single pass through memory, so it is most efficient to\\nimplement this procedure as a single operation with two outputs.\\nWe have not described how to control the memory consumption of back-\\npropagation. Back-propagation often involves summation of many tensors together.\\nIn the naive approach, each of these tensors would be computed separately, then\\nall of them would be added in a second step. The naive approach has an overly\\nhigh memory bottleneck that can be avoided by maintaining a single buffer and'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='adding each value to that buffer as it is computed.\\nReal-world implementations of back-propagation also need to handle various\\ndata types, such as 32-bit floating point, 64-bit floating point, and integer values.\\nThe policy for handling each of these types takes special care to design.\\nSome operations have undefined gradients, and it is important to track these\\ncases and determine whether the gradient requested by the user is undefined.\\nVarious other technicalities make real-world differentiation more complicated.\\nThese technicalities are not insurmountable, and this chapter has described the key\\nintellectual tools needed to compute derivatives, but it is important to be aware\\nthat many more subtleties exist.\\n6.5.9 Differentiation outside the Deep Learning Community\\nThe deep learning community has been somewhat isolated from the broader\\ncomputer science community and has largely developed its own cultural attitudes\\n221\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='concerning how to perform differentiation. More generally, the field of automatic\\ndifferentiation is concerned with how to compute derivatives algorithmically.\\nThe back-propagation algorithm described here is only one approach to automatic\\ndifferentiation. It is a special case of a broader class of techniques called reverse\\nmode accumulation. Other approaches evaluate the subexpressions of the chain\\nrule in different orders. In general, determining the order of evaluation that\\nresults in the lowest computational cost is a difficult problem. Finding the optimal\\nsequence of operations to compute the gradient is NP-complete (Naumann, 2008),\\nin the sense that it may require simplifying algebraic expressions into their least\\nexpensive form.\\nFor example, suppose we have variables p ,p ,...,p representing probabilities\\n1 2 n\\nand variables z ,z ,...,z representing unnormalized log probabilities. Suppose\\n1 2 n\\nwe define\\nexp(z )\\ni\\nq = , (6.57)\\ni\\nexp(z )\\ni i'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='where we build the softmax function out of exponentiation, summation and division\\n\\ue050\\noperations, and construct a cross-entropy loss J = p logq . A human\\n− i i i\\nmathematician can observe that the derivative of J with respect to z takes a very\\ni\\n\\ue050\\nsimple form: q p . The back-propagation algorithm is not capable of simplifying\\ni i\\n−\\nthe gradient this way, and will instead explicitly propagate gradients through all of\\nthe logarithm and exponentiation operations in the original graph. Some software\\nlibraries such as Theano (Bergstra et al., 2010; Bastien et al., 2012) are able to\\nperform some kinds of algebraic substitution to improve over the graph proposed\\nby the pure back-propagation algorithm.\\nWhen the forward graph has a single output node and each partial derivative\\nG\\n∂u(i)\\ncan be computed with a constant amount of computation, back-propagation\\n∂u(j)\\nguarantees that the number of computations for the gradient computation is of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the same order as the number of computations for the forward computation: this\\ncan be seen in algorithm 6.2 because each local partial derivative\\n∂u(i)\\nneeds to\\n∂u(j)\\nbe computed only once along with an associated multiplication and addition for\\nthe recursive chain-rule formulation (equation 6.49). The overall computation is\\ntherefore O(# edges). However, it can potentially be reduced by simplifying the\\ncomputational graph constructed by back-propagation, and this is an NP-complete\\ntask. Implementations such as Theano and TensorFlow use heuristics based on\\nmatching known simplification patterns in order to iteratively attempt to simplify\\nthe graph. We defined back-propagation only for the computation of a gradient of a\\nscalar output but back-propagation can be extended to compute a Jacobian (either\\nof k different scalar nodes in the graph, or of a tensor-valued node containing k\\nvalues). A naive implementation may then need k times more computation: for\\n222'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 6. DEEP FEEDFORWARD NETWORKS\\neach scalar internal node in the original forward graph, the naive implementation\\ncomputes k gradients instead of a single gradient. When the number of outputs of\\nthe graph is larger than the number of inputs, it is sometimes preferable to use\\nanother form of automatic differentiation called forward mode accumulation.\\nForward mode computation has been proposed for obtaining real-time computation\\nof gradients in recurrent networks, for example (Williams and Zipser, 1989). This\\nalso avoids the need to store the values and gradients for the whole graph, trading\\noff computational efficiency for memory. The relationship between forward mode\\nand backward mode is analogous to the relationship between left-multiplying versus\\nright-multiplying a sequence of matrices, such as\\nABCD, (6.58)\\nwhere the matrices can be thought of as Jacobian matrices. For example, if D\\nis a column vector while A has many rows, this corresponds to a graph with a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='single output and many inputs, and starting the multiplications from the end\\nand going backwards only requires matrix-vector products. This corresponds to\\nthe backward mode. Instead, starting to multiply from the left would involve a\\nseries of matrix-matrix products, which makes the whole computation much more\\nexpensive. However, if A has fewer rows than D has columns, it is cheaper to run\\nthe multiplications left-to-right, corresponding to the forward mode.\\nIn many communities outside of machine learning, it is more common to im-\\nplement differentiation software that acts directly on traditional programming\\nlanguage code, such as Python or C code, and automatically generates programs\\nthat differentiate functions written in these languages. In the deep learning com-\\nmunity, computational graphs are usually represented by explicit data structures\\ncreated by specialized libraries. The specialized approach has the drawback of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='requiring the library developer to define the bprop methods for every operation\\nand limiting the user of the library to only those operations that have been defined.\\nHowever, the specialized approach also has the benefit of allowing customized\\nback-propagation rules to be developed for each operation, allowing the developer\\nto improve speed or stability in non-obvious ways that an automatic procedure\\nwould presumably be unable to replicate.\\nBack-propagation is therefore not the only way or the optimal way of computing\\nthe gradient, but it is a very practical method that continues to serve the deep\\nlearning community very well. In the future, differentiation technology for deep\\nnetworksmayimproveasdeeplearningpractitionersbecomemoreawareofadvances\\nin the broader field of automatic differentiation.\\n223\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\n6.5.10 Higher-Order Derivatives\\nSome software frameworks support the use of higher-order derivatives. Among the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='deep learning software frameworks, this includes at least Theano and TensorFlow.\\nThese libraries use the same kind of data structure to describe the expressions for\\nderivatives as they use to describe the original function being differentiated. This\\nmeans that the symbolic differentiation machinery can be applied to derivatives.\\nIn the context of deep learning, it is rare to compute a single second derivative\\nof a scalar function. Instead, we are usually interested in properties of the Hessian\\nmatrix. If we have a function f :\\nRn R\\n, then the Hessian matrix is of size n n.\\n→ ×\\nIn typical deep learning applications, n will be the number of parameters in the\\nmodel, which could easily number in the billions. The entire Hessian matrix is\\nthus infeasible to even represent.\\nInstead of explicitly computing the Hessian, the typical deep learning approach\\nis to use Krylov methods. Krylov methods are a set of iterative techniques for'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='performing various operations like approximately inverting a matrix or finding\\napproximations to its eigenvectors or eigenvalues, without using any operation\\nother than matrix-vector products.\\nIn order to use Krylov methods on the Hessian, we only need to be able to\\ncompute the product between the Hessian matrix H and an arbitrary vector v. A\\nstraightforward technique (Christianson, 1992) for doing so is to compute\\nHv = ( f(x)) v . (6.59)\\nx x \\ue03e\\n∇ ∇\\n\\ue068 \\ue069\\nBoth of the gradient computations in this expression may be computed automati-\\ncally by the appropriate software library. Note that the outer gradient expression\\ntakes the gradient of a function of the inner gradient expression.\\nIf v is itself a vector produced by a computational graph, it is important to\\nspecify that the automatic differentiation software should not differentiate through\\nthe graph that produced v.\\nWhile computing the Hessian is usually not advisable, it is possible to do with'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Hessian vector products. One simply computes He(i) for all i = 1,...,n, where\\ne(i) is the one-hot vector with e(i) = 1 and all other entries equal to 0.\\ni\\n6.6 Historical Notes\\nFeedforward networks can be seen as efficient nonlinear function approximators\\nbased on using gradient descent to minimize the error in a function approximation.\\n224\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nFrom this point of view, the modern feedforward network is the culmination of\\ncenturies of progress on the general function approximation task.\\nThe chain rule that underlies the back-propagation algorithm was invented\\nin the 17th century (Leibniz, 1676; L’Hôpital, 1696). Calculus and algebra have\\nlong been used to solve optimization problems in closed form, but gradient descent\\nwas not introduced as a technique for iteratively approximating the solution to\\noptimization problems until the 19th century (Cauchy, 1847).\\nBeginning in the 1940s, these function approximation techniques were used to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='motivate machine learning models such as the perceptron. However, the earliest\\nmodels were based on linear models. Critics including Marvin Minsky pointed out\\nseveral of the flaws of the linear model family, such as its inability to learn the\\nXOR function, which led to a backlash against the entire neural network approach.\\nLearning nonlinear functions required the development of a multilayer per-\\nceptron and a means of computing the gradient through such a model. Efficient\\napplications of the chain rule based on dynamic programming began to appear\\nin the 1960s and 1970s, mostly for control applications (Kelley, 1960; Bryson and\\nDenham, 1961; Dreyfus, 1962; Bryson and Ho, 1969; Dreyfus, 1973) but also for\\nsensitivity analysis (Linnainmaa, 1976). Werbos (1981) proposed applying these\\ntechniques to training artificial neural networks. The idea was finally developed\\nin practice after being independently rediscovered in different ways (LeCun, 1985;'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Parker, 1985; Rumelhart et al., 1986a). The book Parallel Distributed Pro-\\ncessing presented the results of some of the first successful experiments with\\nback-propagation in a chapter (Rumelhart et al., 1986b) that contributed greatly\\nto the popularization of back-propagation and initiated a very active period of\\nresearch in multi-layer neural networks. However, the ideas put forward by the\\nauthors of that book and in particular by Rumelhart and Hinton go much beyond\\nback-propagation. They include crucial ideas about the possible computational\\nimplementation of several central aspects of cognition and learning, which came\\nunder the name of “connectionism” because of the importance this school of thought\\nplaces on the connections between neurons as the locus of learning and memory.\\nIn particular, these ideas include the notion of distributed representation (Hinton\\net al., 1986).\\nFollowing the success of back-propagation, neural network research gained pop-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='ularity and reached a peak in the early 1990s. Afterwards, other machine learning\\ntechniques became more popular until the modern deep learning renaissance that\\nbegan in 2006.\\nThe core ideas behind modern feedforward networks have not changed sub-\\nstantially since the 1980s. The same back-propagation algorithm and the same\\n225\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\napproaches to gradient descent are still in use. Most of the improvement in neural\\nnetwork performance from 1986 to 2015 can be attributed to two factors. First,\\nlarger datasets have reduced the degree to which statistical generalization is a\\nchallenge for neural networks. Second, neural networks have become much larger,\\ndue to more powerful computers, and better software infrastructure. However, a\\nsmall number of algorithmic changes have improved the performance of neural\\nnetworks noticeably.\\nOne of these algorithmic changes was the replacement of mean squared error'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='with the cross-entropy family of loss functions. Mean squared error was popular in\\nthe 1980s and 1990s, but was gradually replaced by cross-entropy losses and the\\nprinciple of maximum likelihood as ideas spread between the statistics community\\nand the machine learning community. The use of cross-entropy losses greatly\\nimproved the performance of models with sigmoid and softmax outputs, which\\nhad previously suffered from saturation and slow learning when using the mean\\nsquared error loss.\\nThe other major algorithmic change that has greatly improved the performance\\nof feedforward networks was the replacement of sigmoid hidden units with piecewise\\nlinear hidden units, such as rectified linear units. Rectification using the max 0,z\\n{ }\\nfunction was introduced in early neural network models and dates back at least\\nas far as the Cognitron and Neocognitron (Fukushima, 1975, 1980). These early\\nmodels did not use rectified linear units, but instead applied rectification to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='nonlinear functions. Despite the early popularity of rectification, rectification was\\nlargely replaced by sigmoids in the 1980s, perhaps because sigmoids perform better\\nwhen neural networks are very small. As of the early 2000s, rectified linear units\\nwere avoided due to a somewhat superstitious belief that activation functions with\\nnon-differentiable points must be avoided. This began to change in about 2009.\\nJarrett et al. (2009) observed that “using a rectifying nonlinearity is the single most\\nimportant factor in improving the performance of a recognition system” among\\nseveral different factors of neural network architecture design.\\nFor small datasets, Jarrett et al. (2009) observed that using rectifying non-\\nlinearities is even more important than learning the weights of the hidden layers.\\nRandom weights are sufficient to propagate useful information through a rectified\\nlinear network, allowing the classifier layer at the top to learn how to map different'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='feature vectors to class identities.\\nWhen more data isavailable, learning begins to extract enough useful knowledge\\nto exceed the performance of randomly chosen parameters. Glorot et al. (2011a)\\nshowed that learning is far easier in deep rectified linear networks than in deep\\nnetworks that have curvature or two-sided saturation in their activation functions.\\n226\\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\\nRectified linear units are also of historical interest because they show that\\nneuroscience has continued to have an influence on the development of deep\\nlearning algorithms. Glorot et al. (2011a) motivate rectified linear units from\\nbiological considerations. The half-rectifying nonlinearity was intended to capture\\nthese properties of biological neurons: 1) For some inputs, biological neurons are\\ncompletely inactive. 2) For some inputs, a biological neuron’s output is proportional\\nto its input. 3) Most of the time, biological neurons operate in the regime where'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='they are inactive (i.e., they should have sparse activations).\\nWhen the modern resurgence of deep learning began in 2006, feedforward\\nnetworks continued to have a bad reputation. From about 2006-2012, it was widely\\nbelieved that feedforwardnetworks would not perform wellunless they were assisted\\nby other models, such as probabilistic models. Today, it is now known that with the\\nright resources and engineering practices, feedforward networks perform very well.\\nToday, gradient-based learning in feedforward networks is used as a tool to develop\\nprobabilistic models, such as the variational autoencoder and generative adversarial\\nnetworks, described in chapter 20. Rather than being viewed as an unreliable\\ntechnology that must be supported by other techniques, gradient-based learning in\\nfeedforward networks has been viewed since 2012 as a powerful technology that\\nmay be applied to many other machine learning tasks. In 2006, the community'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='used unsupervised learning to support supervised learning, and now, ironically, it\\nis more common to use supervised learning to support unsupervised learning.\\nFeedforward networks continue to have unfulfilled potential. In the future, we\\nexpect they will be applied to many more tasks, and that advances in optimization\\nalgorithms and model design will improve their performance even further. This\\nchapter has primarily described the neural network family of models. In the\\nsubsequent chapters, we turn to how to use these models—how to regularize and\\ntrain them.\\n227\\nChapter 7\\nRegularization for Deep Learning\\nA central problem in machine learning is how to make an algorithm that will\\nperform well not just on the training data, but also on new inputs. Many strategies\\nused in machine learning are explicitly designed to reduce the test error, possibly\\nat the expense of increased training error. These strategies are known collectively'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='as regularization. As we will see there are a great many forms of regularization\\navailable to the deep learning practitioner. In fact, developing more effective\\nregularization strategies has been one of the major research efforts in the field.\\nChapter 5 introduced the basic concepts of generalization, underfitting, overfit-\\nting, bias, variance and regularization. If you are not already familiar with these\\nnotions, please refer to that chapter before continuing with this one.\\nIn this chapter, we describe regularization in more detail, focusing on regular-\\nization strategies for deep models or models that may be used as building blocks\\nto form deep models.\\nSome sections of this chapter deal with standard concepts in machine learning.\\nIf you are already familiar with these concepts, feel free to skip the relevant\\nsections. However, most of this chapter is concerned with the extension of these\\nbasic concepts to the particular case of neural networks.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In section 5.2.2, we defined regularization as “any modification we make to\\na learning algorithm that is intended to reduce its generalization error but not\\nits training error.” There are many regularization strategies. Some put extra\\nconstraints on a machine learning model, such as adding restrictions on the\\nparameter values. Some add extra terms in the objective function that can be\\nthought of as corresponding to a soft constraint on the parameter values. If chosen\\ncarefully, these extra constraints and penalties can lead to improved performance\\n228\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\non the test set. Sometimes these constraints and penalties are designed to encode\\nspecific kinds of prior knowledge. Other times, these constraints and penalties\\nare designed to express a generic preference for a simpler model class in order to\\npromote generalization. Sometimes penalties and constraints are necessary to make'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='an underdetermined problem determined. Other forms of regularization, known as\\nensemble methods, combine multiple hypotheses that explain the training data.\\nIn the context of deep learning, most regularization strategies are based on\\nregularizing estimators. Regularization of an estimator works by trading increased\\nbias for reduced variance. An effective regularizer is one that makes a profitable\\ntrade, reducing variance significantly while not overly increasing the bias. When we\\ndiscussed generalization and overfitting in chapter 5, we focused on three situations,\\nwhere the model family being trained either (1) excluded the true data generating\\nprocess—corresponding to underfitting and inducing bias, or (2) matched the true\\ndata generating process, or (3) included the generating process but also many\\nother possible generating processes—the overfitting regime where variance rather\\nthan bias dominates the estimation error. The goal of regularization is to take a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='model from the third regime into the second regime.\\nIn practice, an overly complex model family does not necessarily include the\\ntarget function or the true data generating process, or even a close approximation\\nof either. We almost never have access to the true data generating process so\\nwe can never know for sure if the model family being estimated includes the\\ngenerating process or not. However, most applications of deep learning algorithms\\nare to domains where the true data generating process is almost certainly outside\\nthe model family. Deep learning algorithms are typically applied to extremely\\ncomplicated domains such as images, audio sequences and text, for which the true\\ngeneration process essentially involves simulating the entire universe. To some\\nextent, we are always trying to fit a square peg (the data generating process) into\\na round hole (our model family).\\nWhat this means is that controlling the complexity of the model is not a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='simple matter of finding the model of the right size, with the right number of\\nparameters. Instead, wemightfind—and indeedinpractical deeplearning scenarios,\\nwe almost always do find—that the best fitting model (in the sense of minimizing\\ngeneralization error) is a large model that has been regularized appropriately.\\nWenowreviewseveralstrategiesfor howto createsuchalarge, deep, regularized\\nmodel.\\n229\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\n7.1 Parameter Norm Penalties\\nRegularizationhasbeenusedfordecadespriortotheadventofdeeplearning. Linear\\nmodelssuchaslinear regressionand logisticregressionallowsimple, straightforward,\\nand effective regularization strategies.\\nMany regularization approaches are based on limiting the capacity of models,\\nsuch as neural networks, linear regression, or logistic regression, by adding a pa-\\nrameter norm penalty Ω(θ) to the objective function J. We denote the regularized\\n˜\\nobjective function by J:\\n˜\\nJ(θ;X,y) = J(θ;X,y) + αΩ(θ) (7.1)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='where α [0, ) is a hyperparameter that weights the relative contribution of the\\n∈ ∞\\nnorm penalty term, Ω, relative to the standard objective function J. Setting α to 0\\nresults in no regularization. Larger values of α correspond to more regularization.\\n˜\\nWhen our training algorithm minimizes the regularized objective function J it\\nwill decrease both the original objective J on the training data and some measure\\nof the size of the parameters θ (or some subset of the parameters). Different\\nchoices for the parameter norm Ω can result in different solutions being preferred.\\nIn this section, we discuss the effects of the various norms when used as penalties\\non the model parameters.\\nBefore delving into the regularization behavior of different norms, we note that\\nfor neural networks, we typically choose to use a parameter norm penalty Ω that\\npenalizes only the weights of the affine transformation at each layer and leaves'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the biases unregularized. The biases typically require less data to fit accurately\\nthan the weights. Each weight specifies how two variables interact. Fitting the\\nweight well requires observing both variables in a variety of conditions. Each\\nbias controls only a single variable. This means that we do not induce too much\\nvariance by leaving the biases unregularized. Also, regularizing the bias parameters\\ncan introduce a significant amount of underfitting. We therefore use the vector w\\nto indicate all of the weights that should be affected by a norm penalty, while the\\nvector θ denotes all of the parameters, including both w and the unregularized\\nparameters.\\nIn the context of neural networks, it is sometimes desirable to use a separate\\npenalty with a different α coefficient for each layer of the network. Because it can\\nbe expensive to search for the correct value of multiple hyperparameters, it is still\\nreasonable to use the same weight decay at all layers just to reduce the size of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='search space.\\n230\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\n7.1.1\\nL2\\nParameter Regularization\\nWe have already seen, in section 5.2.2, one of the simplest and most common kinds\\nof parameter norm penalty: the L2 parameter norm penalty commonly known as\\nweight decay. This regularization strategy drives the weights closer to the origin1\\nby adding a regularization term Ω(θ) = 1 w 2 to the objective function. In other\\n2\\ue06b \\ue06b2\\nacademic communities, L2 regularization is also known as ridge regression or\\nTikhonov regularization.\\nWe can gain some insight into the behavior of weight decay regularization\\nby studying the gradient of the regularized objective function. To simplify the\\npresentation, we assume no bias parameter, so θ is just w. Such a model has the\\nfollowing total objective function:\\nα\\n˜\\nJ(w;X,y) = w w + J(w;X,y), (7.2)\\n\\ue03e\\n2\\nwith the corresponding parameter gradient\\n˜\\nJ(w;X,y) = αw + J(w;X,y). (7.3)\\nw w\\n∇ ∇\\nTo take a single gradient step to update the weights, we perform this update:'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='w w \\ue00f(αw + J(w;X,y)). (7.4)\\nw\\n← − ∇\\nWritten another way, the update is:\\nw (1 \\ue00fα)w \\ue00f J(w;X,y). (7.5)\\nw\\n← − − ∇\\nWe can see that the addition of the weight decay term has modified the learning\\nrule to multiplicatively shrink the weight vector by a constant factor on each step,\\njust before performing the usual gradient update. This describes what happens in\\na single step. But what happens over the entire course of training?\\nWe will further simplify the analysis by making a quadratic approximation\\nto the objective function in the neighborhood of the value of the weights that\\nobtains minimal unregularized training cost, w = argmin J(w). If the objective\\n∗ w\\nfunction is truly quadratic, as in the case of fitting a linear regression model with\\n1More generally, we could regularize the parameters to be near any specific point in space\\nand, surprisingly, still get a regularization effect, but better results will be obtained for a value'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='closer to the true one, with zero being a default value that makes sense when we do not know if\\nthe correct value should be positive or negative. Since it is far more common to regularize the\\nmodel parameters towards zero, we will focus on this special case in our exposition.\\n231\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nˆ\\nmean squared error, then the approximation is perfect. The approximation J is\\ngiven by\\n1\\nˆ\\nJ(θ) = J(w ) + (w w ) H(w w ), (7.6)\\n∗ ∗ \\ue03e ∗\\n2 − −\\nwhere H is the Hessian matrix of J with respect to w evaluated at w . There is\\n∗\\nno first-order term in this quadratic approximation, because w is defined to be a\\n∗\\nminimum, where the gradient vanishes. Likewise, because w is the location of a\\n∗\\nminimum of J, we can conclude that H is positive semidefinite.\\nˆ\\nThe minimum of J occurs where its gradient\\nˆ\\nJ(w) = H(w w ) (7.7)\\nw ∗\\n∇ −\\nis equal to 0.\\nTo study the effect of weight decay, we modify equation 7.7 by adding the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='weight decay gradient. We can now solve for the minimum of the regularized\\nˆ\\nversion of J. We use the variable w˜ to represent the location of the minimum.\\nαw˜ + H(w˜ w ) = 0 (7.8)\\n∗\\n−\\n(H + αI)w˜ = Hw (7.9)\\n∗\\nw˜ = (H +αI) 1Hw . (7.10)\\n− ∗\\nAs α approaches 0, the regularized solution w˜ approaches w . But what\\n∗\\nhappens as α grows? Because H is real and symmetric, we can decompose it\\ninto a diagonal matrix Λ and an orthonormal basis of eigenvectors, Q, such that\\nH = QΛQ . Applying the decomposition to equation 7.10, we obtain:\\n\\ue03e\\nw˜ = (QΛQ +αI) 1QΛQ w (7.11)\\n\\ue03e − \\ue03e ∗\\n1\\n= Q(Λ+ αI)Q − QΛQ w (7.12)\\n\\ue03e \\ue03e ∗\\n=\\nQ\\ue068\\n(Λ+ αI)\\n1ΛQ\\ue069\\nw . (7.13)\\n− \\ue03e ∗\\nWe see that the effect of weight decay is to rescale w along the axes defined by\\n∗\\nthe eigenvectors of H. Specifically, the component of w that is aligned with the\\n∗\\ni-th eigenvector of H is rescaled by a factor of λ i . (You may wish to review\\nλ +α\\ni\\nhow this kind of scaling works, first explained in figure 2.3).'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Alongthe directions wherethe eigenvalues of H arerelativelylarge, for example,\\nwhere λ α, the effect of regularization is relatively small. However, components\\ni\\n\\ue01d\\nwith λ α will be shrunk to have nearly zero magnitude. This effect is illustrated\\ni\\n\\ue01c\\nin figure 7.1.\\n232\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nw\\n1\\n2w\\nw\\n∗\\nw˜\\nFigure 7.1: An illustration of the effect of L2 (or weight decay) regularization on the value\\nof the optimal w. The solid ellipses represent contours of equal value of the unregularized\\nobjective. The dotted circles represent contours of equal value of the L2 regularizer. At\\nthe point w˜, these competing objectives reach an equilibrium. In the first dimension, the\\neigenvalue of the Hessian of J is small. The objective function does not increase much\\nwhen moving horizontally away from w . Because the objective function does not express\\n∗\\na strong preference along this direction, the regularizer has a strong effect on this axis.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The regularizer pulls w close to zero. In the second dimension, the objective function\\n1\\nis very sensitive to movements away from w . The corresponding eigenvalue is large,\\n∗\\nindicating high curvature. As a result, weight decay affects the position of w relatively\\n2\\nlittle.\\nOnly directions along which the parameters contribute significantly to reducing\\nthe objective function are preserved relatively intact. In directions that do not\\ncontribute to reducing the objective function, a small eigenvalue of the Hessian\\ntells us that movement in this direction will not significantly increase the gradient.\\nComponents of the weight vector corresponding to such unimportant directions\\nare decayed away through the use of the regularization throughout training.\\nSo far we have discussed weight decay in terms of its effect on the optimization\\nof an abstract, general, quadratic cost function. How do these effects relate to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='machine learning in particular? We can find out by studying linear regression, a\\nmodel for which the true cost function is quadratic and therefore amenable to the\\nsame kind of analysis we have used so far. Applying the analysis again, we will\\nbe able to obtain a special case of the same results, but with the solution now\\nphrased in terms of the training data. For linear regression, the cost function is\\n233\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nthe sum of squared errors:\\n(Xw y) (Xw y). (7.14)\\n\\ue03e\\n− −\\nWhen we add L2 regularization, the objective function changes to\\n1\\n(Xw y) (Xw y)+ αw w. (7.15)\\n\\ue03e \\ue03e\\n− − 2\\nThis changes the normal equations for the solution from\\nw = (X X) 1X y (7.16)\\n\\ue03e − \\ue03e\\nto\\nw = (X X + αI) 1X y. (7.17)\\n\\ue03e − \\ue03e\\nThematrix X X inequation7.16is proportionaltothecovariance matrix 1 X X.\\n\\ue03e m \\ue03e\\nUsing L2 regularization replaces this matrix with X \\ue03eX + αI −1 in equation 7.17.\\nThe new matrix is the same as the original one, but with the addition of α to the\\n\\ue000 \\ue001'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='diagonal. The diagonal entries of this matrix correspond to the variance of each\\ninput feature. We can see that L2 regularization causes the learning algorithm\\nto “perceive” the input X as having higher variance, which makes it shrink the\\nweights on features whose covariance with the output target is low compared to\\nthis added variance.\\n7.1.2\\nL1\\nRegularization\\nWhile L2 weight decay is the most common form of weight decay, there are other\\nways to penalize the size of the model parameters. Another option is to use L1\\nregularization.\\nFormally, L1 regularization on the model parameter w is defined as:\\nΩ(θ) = w = w , (7.18)\\n1 i\\n|| || | |\\ni\\n\\ue058\\nthat is, as the sum of absolute values of the individual parameters.2 We will\\nnow discuss the effect of L1 regularization on the simple linear regression model,\\nwith no bias parameter, that we studied in our analysis of L2 regularization. In\\nparticular, we are interested in delineating the differences between L1 and L2 forms'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='2As with L2 regularization, we could regularize the parameters towards a value that is not\\nzero, but instead towards some parameter value w(o). In that case the L1 regularization would\\nintroduce the term Ω(θ) = w w(o) = \\ue050 w w(o) .\\n|| − ||1 i | i − i |\\n234\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nof regularization. As with L2 weight decay, L1 weight decay controls the strength\\nof the regularization by scaling the penalty Ω using a positive hyperparameter α.\\n˜\\nThus, the regularized objective function J(w;X,y) is given by\\n˜\\nJ(w;X,y) = α w +J(w;X,y), (7.19)\\n1\\n|| ||\\nwith the corresponding gradient (actually, sub-gradient):\\n˜\\nJ(w;X,y) = αsign(w) + J(X,y;w) (7.20)\\nw w\\n∇ ∇\\nwhere sign(w) is simply the sign of w applied element-wise.\\nBy inspecting equation 7.20, we can see immediately that the effect of L1\\nregularization is quite different from that of L2 regularization. Specifically, we can\\nsee that the regularization contribution to the gradient no longer scales linearly'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='with each w; instead it is a constant factor with a sign equal to sign(w ). One\\ni i\\nconsequence of this form of the gradient is that we will not necessarily see clean\\nalgebraic solutions to quadratic approximations of J(X,y;w) as we did for L2\\nregularization.\\nOur simple linear model has a quadratic cost function that we can represent\\nvia its Taylor series. Alternately, we could imagine that this is a truncated Taylor\\nseries approximating the cost function of a more sophisticated model. The gradient\\nin this setting is given by\\nˆ\\nJ(w) = H(w w ), (7.21)\\nw ∗\\n∇ −\\nwhere, again, H is the Hessian matrix of J with respect to w evaluated at w .\\n∗\\nBecause the L1 penalty does not admit clean algebraic expressions in the case\\nof a fully general Hessian, we will also make the further simplifying assumption\\nthat the Hessian is diagonal, H = diag([H ,...,H ]), where each H > 0.\\n1,1 n,n i,i\\nThis assumption holds if the data for the linear regression problem has been'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='preprocessed to remove all correlation between the input features, which may be\\naccomplished using PCA.\\nOur quadratic approximation of the L1 regularized objective function decom-\\nposes into a sum over the parameters:\\n1\\nJˆ (w;X,y) = J(w ;X,y) + H (w w )2 +α w . (7.22)\\n∗ i,i i ∗i i\\n2 − | |\\ni \\ue014 \\ue015\\n\\ue058\\nTheproblem ofminimizing thisapproximatecostfunction hasan analyticalsolution\\n(for each dimension i), with the following form:\\nα\\nw = sign(w )max w ,0 . (7.23)\\ni i∗ ∗i\\n| |− H\\ni,i\\n235\\n\\ue01a \\ue01b\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nConsider the situation where w > 0 for all i. There are two possible outcomes:\\n∗i\\n1. The case wherew α . Here the optimal value of w under the regularized\\n∗i ≤ H i,i i\\nobjectiveissimplyw = 0. Thisoccursbecausethecontributionof J(w;X,y)\\ni\\n˜\\nto the regularized objective J(w;X,y) is overwhelmed—in direction i—by\\nthe L1 regularization which pushes the value of w to zero.\\ni\\n2. The case wherew > α . In this case, the regularization does not move the\\n∗i H\\ni,i'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='optimal value of w to zero but instead it just shifts it in that direction by a\\ni\\ndistance equal to α .\\nH\\ni,i\\nA similar process happens when w < 0, but with the L1 penalty making w less\\ni∗ i\\nnegative by α , or 0.\\nH\\ni,i\\nIn comparison to L2 regularization, L1 regularization results in a solution that\\nis more sparse. Sparsity in this context refers to the fact that some parameters\\nhave an optimal value of zero. The sparsity of L1 regularization is a qualitatively\\ndifferent behavior than arises with L2 regularization. Equation 7.13 gave the\\nsolution w˜ for L2 regularization. If we revisit that equation using the assumption\\nof a diagonal and positive definite Hessian H that we introduced for our analysis of\\nL1 regularization, we find that w˜ = H i,i w . If w was nonzero, then w˜ remains\\ni H +α ∗i ∗i i\\ni,i\\nnonzero. This demonstrates that L2 regularization does not cause the parameters\\nto become sparse, while L1 regularization may do so for large enough α.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The sparsity property induced by L1 regularization has been used extensively\\nas a feature selectionmechanism. Feature selection simplifies a machine learning\\nproblem by choosing which subset of the available features should be used. In\\nparticular, the well known LASSO (Tibshirani, 1995) (least absolute shrinkage and\\nselection operator) model integrates an L1 penalty with a linear model and a least\\nsquares cost function. The L1 penalty causes a subset of the weights to become\\nzero, suggesting that the corresponding features may safely be discarded.\\nIn section 5.6.1, we saw that many regularization strategies can be interpreted\\nas MAP Bayesian inference, and that in particular, L2 regularization is equivalent\\nto MAP Bayesian inference with a Gaussian prior on the weights. For L1 regu-\\nlarization, the penalty αΩ(w) = α w used to regularize a cost function is\\ni i\\n| |\\nequivalent to the log-prior term that is maximized by MAP Bayesian inference\\n\\ue050'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='when the prior is an isotropic Laplace distribution (equation 3.26) over w\\nRn:\\n∈\\n1\\nlogp(w) = logLaplace(w ;0, ) = α w +nlogα nlog2. (7.24)\\ni 1\\nα − || || −\\ni\\n236\\n\\ue058\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nFrom the point of view of learning via maximization with respect to w, we can\\nignore the logα log2 terms because they do not depend on w.\\n−\\n7.2 Norm Penalties as Constrained Optimization\\nConsider the cost function regularized by a parameter norm penalty:\\n˜\\nJ(θ;X,y) = J(θ;X,y)+ αΩ(θ). (7.25)\\nRecall from section 4.4 that we can minimize a function subject to constraints\\nby constructing a generalized Lagrange function, consisting of the original objective\\nfunction plus a set of penalties. Each penalty is a product between a coefficient,\\ncalled a Karush–Kuhn–Tucker (KKT) multiplier, and a function representing\\nwhether the constraint is satisfied. If we wanted to constrain Ω(θ) to be less than\\nsome constant k, we could construct a generalized Lagrange function'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='(θ,α;X,y) = J(θ;X,y)+ α(Ω(θ) k). (7.26)\\nL −\\nThe solution to the constrained problem is given by\\nθ = argmin max (θ,α). (7.27)\\n∗\\nθ α,α 0L\\n≥\\nAs described in section 4.4, solving this problem requires modifying both θ\\nand α. Section 4.5 provides a worked example of linear regression with an L2\\nconstraint. Many different procedures are possible—some may use gradient descent,\\nwhile others may use analytical solutions for where the gradient is zero—but in all\\nprocedures α must increase whenever Ω(θ) > k and decrease whenever Ω(θ)< k.\\nAll positive α encourage Ω(θ) to shrink. The optimal value α will encourage Ω(θ)\\n∗\\nto shrink, but not so strongly to make Ω(θ) become less than k.\\nTo gain some insight into the effect of the constraint, we can fix α and view\\n∗\\nthe problem as just a function of θ:\\nθ = argmin (θ,α ) = argminJ(θ;X,y) + α Ω(θ). (7.28)\\n∗ ∗ ∗\\nL\\nθ θ\\n˜\\nThis is exactly the same as the regularized training problem of minimizing J.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='We can thus think of a parameter norm penalty as imposing a constraint on the\\nweights. If Ω is the L2 norm, then the weights are constrained to lie in an L2\\nball. If Ω is the L1 norm, then the weights are constrained to lie in a region of\\n237\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nlimited L1 norm. Usually we do not know the size of the constraint region that we\\nimpose by using weight decay with coefficient α because the value of α does not\\n∗ ∗\\ndirectly tell us the value of k. In principle, one can solve for k, but the relationship\\nbetween k and α depends on the form of J. While we do not know the exact size\\n∗\\nof the constraint region, we can control it roughly by increasing or decreasing α\\nin order to grow or shrink the constraint region. Larger α will result in a smaller\\nconstraint region. Smaller α will result in a larger constraint region.\\nSometimes we may wish to use explicit constraints rather than penalties. As'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='described in section 4.4, we can modify algorithms such as stochastic gradient\\ndescent to take a step downhill on J(θ) and then project θ back to the nearest\\npoint that satisfies Ω(θ) < k. This can be useful if we have an idea of what value\\nof k is appropriate and do not want to spend time searching for the value of α that\\ncorresponds to this k.\\nAnotherreason touse explicitconstraintsandreprojectionrather than enforcing\\nconstraints with penalties is that penalties can cause non-convex optimization\\nprocedures to get stuck in local minima corresponding to small θ. When training\\nneural networks, this usually manifests as neural networks that train with several\\n“dead units.” These are units that do not contribute much to the behavior of the\\nfunction learned by the network because the weights going into or out of them are\\nall very small. When training with a penalty on the norm of the weights, these\\nconfigurations can be locally optimal, even if it is possible to significantly reduce'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='J by making the weights larger. Explicit constraints implemented by re-projection\\ncan work much better in these cases because they do not encourage the weights\\nto approach the origin. Explicit constraints implemented by re-projection only\\nhave an effect when the weights become large and attempt to leave the constraint\\nregion.\\nFinally, explicit constraints with reprojection can be useful because they impose\\nsome stability on the optimization procedure. When using high learning rates, it\\nis possible to encounter a positive feedback loop in which large weights induce\\nlarge gradients which then induce a large update to the weights. If these updates\\nconsistently increase the size of the weights, then θ rapidly moves away from\\nthe origin until numerical overflow occurs. Explicit constraints with reprojection\\nprevent this feedback loop from continuing to increase the magnitude of the weights\\nwithout bound. Hinton et al. (2012c) recommend using constraints combined with'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a high learning rate to allowrapid exploration ofparameter spacewhile maintaining\\nsome stability.\\nIn particular, Hinton et al. (2012c) recommend a strategy introduced by Srebro\\nand Shraibman (2005): constraining the norm of each column of the weight matrix\\n238\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nof a neural net layer, rather than constraining the Frobenius norm of the entire\\nweight matrix. Constraining the norm of each column separately prevents any one\\nhidden unit from having very large weights. If we converted this constraint into a\\npenalty in a Lagrange function, it would be similar to L2 weight decay but with a\\nseparate KKT multiplier for the weights of each hidden unit. Each of these KKT\\nmultipliers would be dynamically updated separately to make each hidden unit\\nobey the constraint. In practice, column norm limitation is always implemented as\\nan explicit constraint with reprojection.\\n7.3 Regularization and Under-Constrained Problems'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In some cases, regularization is necessary for machine learning problems to be prop-\\nerly defined. Many linear models in machine learning, including linear regression\\nand PCA, depend on inverting the matrix X X. This is not possible whenever\\n\\ue03e\\nX X is singular. This matrix can be singular whenever the data generating distri-\\n\\ue03e\\nbution truly has no variance in some direction, or when no variance is observed in\\nsome direction because there are fewer examples (rows of X) than input features\\n(columns of X). In this case, many forms of regularization correspond to inverting\\nX X + αI instead. This regularized matrix is guaranteed to be invertible.\\n\\ue03e\\nThese linear problems have closed form solutions when the relevant matrix\\nis invertible. It is also possible for a problem with no closed form solution to be\\nunderdetermined. An example is logistic regression applied to a problem where\\nthe classes are linearly separable. If a weight vector w is able to achieve perfect'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='classification, then 2w will also achieve perfect classification and higher likelihood.\\nAn iterative optimization procedure like stochastic gradient descent will continually\\nincrease the magnitude of w and, in theory, will neverhalt. In practice, a numerical\\nimplementation of gradient descent will eventually reach sufficiently large weights\\nto cause numerical overflow, at which point its behavior will depend on how the\\nprogrammer has decided to handle values that are not real numbers.\\nMost forms of regularization are able to guarantee the convergence of iterative\\nmethods applied to underdetermined problems. For example, weight decay will\\ncause gradient descent to quit increasing the magnitude of the weights when the\\nslope of the likelihood is equal to the weight decay coefficient.\\nThe idea of using regularization to solve underdetermined problems extends\\nbeyond machine learning. The same idea is useful for several basic linear algebra\\nproblems.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='As we saw in section 2.9, we can solve underdetermined linear equations using\\n239\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nthe Moore-Penrose pseudoinverse. Recall that one definition of the pseudoinverse\\nX+ of a matrix X is\\nX+ = lim(X X + αI) 1X . (7.29)\\n\\ue03e − \\ue03e\\nα 0\\n\\ue026\\nWe can now recognize equation 7.29 as performing linear regression with weight\\ndecay. Specifically, equation 7.29 is the limit of equation 7.17 as the regularization\\ncoefficient shrinks to zero. We can thus interpret the pseudoinverse as stabilizing\\nunderdetermined problems using regularization.\\n7.4 Dataset Augmentation\\nThe best way to make a machine learning model generalize better is to train it on\\nmore data. Of course, in practice, the amount of data we have is limited. One way\\nto get around this problem is to create fake data and add it to the training set.\\nFor some machine learning tasks, it is reasonably straightforward to create new\\nfake data.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='This approach is easiest for classification. A classifier needs to take a compli-\\ncated, high dimensional input x and summarize it with a single category identity y.\\nThis means that the main task facing a classifier is to be invariant to a wide variety\\nof transformations. We can generate new (x,y) pairs easily just by transforming\\nthe x inputs in our training set.\\nThis approach is not as readily applicable to many other tasks. For example, it\\nis difficult to generate new fake data for a density estimation task unless we have\\nalready solved the density estimation problem.\\nDataset augmentation has been a particularly effective technique for a specific\\nclassification problem: object recognition. Images are high dimensional and include\\nan enormous variety of factors of variation, many of which can be easily simulated.\\nOperations like translating the training images a few pixels in each direction can\\noften greatly improve generalization, even if the model has already been designed to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='be partially translation invariant by using the convolution and pooling techniques\\ndescribed in chapter 9. Many other operations such as rotating the image or scaling\\nthe image have also proven quite effective.\\nOne must be careful not to apply transformations that would change the correct\\nclass. For example, optical character recognition tasks require recognizing the\\ndifference between ‘b’ and ‘d’ and the difference between ‘6’ and ‘9’, so horizontal\\nflips and 180 rotations are not appropriate ways of augmenting datasets for these\\n◦\\ntasks.\\n240\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nThere are also transformations that we would like our classifiers to be invariant\\nto, but which are not easy to perform. For example, out-of-plane rotation can not\\nbe implemented as a simple geometric operation on the input pixels.\\nDataset augmentation is effective for speech recognition tasks as well (Jaitly\\nand Hinton, 2013).\\nInjecting noise in the input to a neural network (Sietsma and Dow, 1991)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='can also be seen as a form of data augmentation. For many classification and\\neven some regression tasks, the task should still be possible to solve even if small\\nrandom noise is added to the input. Neural networks prove not to be very robust\\nto noise, however (Tang and Eliasmith, 2010). One way to improve the robustness\\nof neural networks is simply to train them with random noise applied to their\\ninputs. Input noise injection is part of some unsupervised learning algorithms such\\nas the denoising autoencoder (Vincent et al., 2008). Noise injection also works\\nwhen the noise is applied to the hidden units, which can be seen as doing dataset\\naugmentation at multiple levels of abstraction. Poole et al. (2014) recently showed\\nthat this approach can be highly effective provided that the magnitude of the\\nnoise is carefully tuned. Dropout, a powerful regularization strategy that will be\\ndescribed in section 7.12, can be seen as a process of constructing new inputs by\\nmultiplying by noise.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='When comparing machine learning benchmark results, it is important to take\\nthe effect of dataset augmentation into account. Often, hand-designed dataset\\naugmentationschemescandramatically reducethe generalizationerror ofa machine\\nlearning technique. To compare the performance of one machine learning algorithm\\nto another, it is necessary to perform controlled experiments. When comparing\\nmachine learning algorithm A and machine learning algorithm B, it is necessary\\nto make sure that both algorithms were evaluated using the same hand-designed\\ndataset augmentation schemes. Suppose that algorithm A performs poorly with\\nno dataset augmentation and algorithm B performs well when combined with\\nnumerous synthetic transformations of the input. In such a case it is likely the\\nsynthetic transformations caused the improved performance, rather than the use\\nof machine learning algorithm B. Sometimes deciding whether an experiment'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='has been properly controlled requires subjective judgment. For example, machine\\nlearning algorithms that inject noise into the input are performing a form of dataset\\naugmentation. Usually, operations that are generally applicable (such as adding\\nGaussian noise to the input) are considered part of the machine learning algorithm,\\nwhile operations that are specific to one application domain (such as randomly\\ncropping an image) are considered to be separate pre-processing steps.\\n241\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\n7.5 Noise Robustness\\nSection 7.4 has motivated the use of noise applied to the inputs as a dataset\\naugmentation strategy. For some models, the addition of noise with infinitesimal\\nvariance at the input of the model is equivalent to imposing a penalty on the\\nnorm of the weights (Bishop, 1995a,b). In the general case, it is important to\\nremember that noise injection can be much more powerful than simply shrinking'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the parameters, especially when the noise is added to the hidden units. Noise\\napplied to the hidden units is such an important topic that it merit its own separate\\ndiscussion; the dropout algorithm described in section 7.12 is the main development\\nof that approach.\\nAnother way that noise has been used in the service of regularizing models\\nis by adding it to the weights. This technique has been used primarily in the\\ncontext of recurrent neural networks (Jim et al., 1996; Graves, 2011). This can\\nbe interpreted as a stochastic implementation of Bayesian inference over the\\nweights. The Bayesian treatment of learning would consider the model weights\\nto be uncertain and representable via a probability distribution that reflects this\\nuncertainty. Adding noise to the weights is a practical, stochastic way to reflect\\nthis uncertainty.\\nNoise applied to the weights can also be interpreted as equivalent (under some'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='assumptions) to a more traditional form of regularization, encouraging stability of\\nthe function to be learned. Consider the regression setting, where we wish to train\\na function yˆ(x) that maps a set of features xto a scalar using the least-squares\\ncost function between the model predictions yˆ(x) and the true values y:\\nJ = E (yˆ(x) y)2 . (7.30)\\np(x,y)\\n−\\nThe training set consists of m labeled\\ne\\ue002\\nxamples\\n(x\\ue003(1),y(1)),...,(x(m),y(m))\\n.\\n{ }\\nWe now assume that with each input presentation we also include a random\\nperturbation \\ue00f (\\ue00f; 0,ηI) of the network weights. Let us imagine that we\\nW\\n∼ N\\nhave a standard l-layer MLP. We denote the perturbed model as yˆ (x). Despite\\n\\ue00fW\\nthe injection of noise, we are still interested in minimizing the squared error of the\\noutput of the network. The objective function thus becomes:\\nJ˜ = E (yˆ (x) y)2 (7.31)\\nW p(x,y,\\ue00fW) \\ue00fW\\n−\\n= E \\ue068 yˆ2 (x) 2yyˆ \\ue069 (x)+ y2 . (7.32)\\np(x,y,\\ue00fW) \\ue00fW\\n−\\n\\ue00fW\\nFor small η, the minimization of J with added weight noise (with covariance'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='ηI) is equivalent to minimization of J with an additional regularization term:\\n\\ue002 242 \\ue003\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nηE yˆ(x) 2 . This form of regularization encourages the parameters to\\np(x,y) W\\n\\ue06b∇ \\ue06b\\ngo to regions of parameter space where small perturbations of the weights have\\n\\ue002 \\ue003\\na relatively small influence on the output. In other words, it pushes the model\\ninto regions where the model is relatively insensitive to small variations in the\\nweights, finding points that are not merely minima, but minima surrounded by\\nflat regions (Hochreiter and Schmidhuber, 1995). In the simplified case of linear\\nregression (where, for instance, yˆ(x) = w x+b), this regularization term collapses\\n\\ue03e\\ninto ηE x 2 , which is not a function of parameters and therefore does not\\np(x)\\n\\ue06b \\ue06b\\ncontribute to the gradient of J˜ with respect to the model parameters.\\n\\ue002 \\ue003 W\\n7.5.1 Injecting Noise at the Output Targets\\nMost datasets have some amount of mistakes in the y labels. It can be harmful to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='maximize logp(y x) when y is a mistake. One way to prevent this is to explicitly\\n|\\nmodel the noise on the labels. For example, we can assume that for some small\\nconstant \\ue00f, the training set label y is correct with probability 1 \\ue00f, and otherwise\\n−\\nany of the other possible labels might be correct. This assumption is easy to\\nincorporate into the cost function analytically, rather than by explicitly drawing\\nnoise samples. For example, label smoothing regularizes a model based on a\\nsoftmax with k output values by replacing the hard 0 and 1 classification targets\\nwith targets of \\ue00f and 1 \\ue00f, respectively. The standard cross-entropy loss may\\nk 1 −\\nthen be used with−these soft targets. Maximum likelihood learning with a softmax\\nclassifier and hard targets may actually never converge—the softmax can never\\npredict a probability of exactly 0 or exactly 1, so it will continue to learn larger\\nand larger weights, making more extreme predictions forever. It is possible to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='prevent this scenario using other regularization strategies like weight decay. Label\\nsmoothing has the advantage of preventing the pursuit of hard probabilities without\\ndiscouraging correct classification. This strategy has been used since the 1980s\\nand continues to be featured prominently in modern neural networks (Szegedy\\net al., 2015).\\n7.6 Semi-Supervised Learning\\nIn the paradigm of semi-supervised learning, both unlabeled examples from P(x)\\nand labeled examples from P(x,y) are used to estimate P (y x) or predict yfrom\\n|\\nx.\\nIn the context of deep learning, semi-supervised learning usually refers to\\nlearning a representation h = f(x). The goal is to learn a representation so\\n243\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nthat examples from the same class have similar representations. Unsupervised\\nlearning can provide useful cues for how to group examples in representation\\nspace. Examples that cluster tightly in the input space should be mapped to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='similar representations. A linear classifier in the new space may achieve better\\ngeneralization in many cases (Belkin and Niyogi, 2002; Chapelle et al., 2003). A\\nlong-standing variant of this approach is the application of principal components\\nanalysis as a pre-processing step before applying a classifier (on the projected\\ndata).\\nInstead of having separate unsupervised and supervised components in the\\nmodel, one can construct models in which a generative model of either P (x) or\\nP(x,y) shares parameters with a discriminative model of P(y x). One can\\n|\\nthen trade-off the supervised criterion logP(y x) with the unsupervised or\\n− |\\ngenerative one (such as logP(x) or logP(x,y)). The generative criterion then\\n− −\\nexpresses a particular form of prior belief about the solution to the supervised\\nlearning problem (Lasserre et al., 2006), namely that the structure of P(x) is\\nconnected to the structure of P(y x) in a way that is captured by the shared\\n|'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='parametrization. By controlling how much of the generative criterion is included\\nin the total criterion, one can find a better trade-off than with a purely generative\\nor a purely discriminative training criterion (Lasserre et al., 2006; Larochelle and\\nBengio, 2008).\\nSalakhutdinov and Hinton (2008) describe a method for learning the kernel\\nfunction of a kernel machine used for regression, in which the usage of unlabeled\\nexamples for modeling P(x) improves P(y x) quite significantly.\\n|\\nSee Chapelle et al. (2006) for more information about semi-supervised learning.\\n7.7 Multi-Task Learning\\nMulti-task learning (Caruana, 1993) is a way to improve generalization by pooling\\nthe examples (which can be seen as soft constraints imposed on the parameters)\\narising out of several tasks. In the same way that additional training examples\\nput more pressure on the parameters of the model towards values that generalize\\nwell, when part of a model is shared across tasks, that part of the model is more'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='constrained towards good values (assuming the sharing is justified), often yielding\\nbetter generalization.\\nFigure 7.2 illustrates a very common form of multi-task learning, in which\\ndifferent supervised tasks (predicting y(i) given x) share the same input x, as well\\nas some intermediate-level representation h(shared) capturing a common pool of\\n244\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nfactors. The model can generally be divided into two kinds of parts and associated\\nparameters:\\n1. Task-specific parameters (which only benefit from the examples of their task\\nto achieve good generalization). These are the upper layers of the neural\\nnetwork in figure 7.2.\\n2. Generic parameters, shared across all the tasks (which benefit from the\\npooled data of all the tasks). These are the lower layers of the neural network\\nin figure 7.2.\\nyy((11)) yy((22))\\nhh((11)) hh((22)) hh((33))\\nhh((sshhaarreedd))\\nxx\\nFigure 7.2: Multi-task learning can be cast in several ways in deep learning frameworks'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='and this figure illustrates the common situation where the tasks share a common input but\\ninvolve different target random variables. The lower layers of a deep network (whether it\\nis supervised and feedforward or includes a generative component with downward arrows)\\ncan be shared across such tasks, while task-specific parameters (associated respectively\\nwith the weights into and from h(1) and h(2)) can be learned on top of those yielding a\\nshared representation h(shared). The underlying assumption is that there exists a common\\npool of factors that explain the variations in the input x, while each task is associated\\nwith a subset of these factors. In this example, it is additionally assumed that top-level\\nhidden units h(1) and h(2) are specialized to each task (respectively predicting y(1) and\\ny(2)) while some intermediate-level representationh(shared) is shared across all tasks. In\\nthe unsupervised learning context, it makes sense for some of the top-level factors to be'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='associated with none of the output tasks (h(3)): these are the factors that explain some of\\nthe input variations but are not relevant for predicting y(1) or y(2).\\nImproved generalization and generalization error bounds (Baxter, 1995) can be\\nachieved because of the shared parameters, for which statistical strength can be\\n245\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\n0.20\\n0.15\\n0.10\\n0.05\\n0.00\\n0 50 100 150 200 250\\nTime (epochs)\\n)doohilekil-gol\\nevitagen(\\nssoL\\nTraining set loss\\nValidation set loss\\nFigure 7.3: Learning curves showing how the negative log-likelihood loss changes over\\ntime (indicated as number of training iterations over the dataset, or epochs). In this\\nexample, we train a maxout network on MNIST. Observe that the training objective\\ndecreases consistently over time, but the validation set average loss eventually begins to\\nincrease again, forming an asymmetric U-shaped curve.\\ngreatly improved (in proportion with the increased number of examples for the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='shared parameters, compared to the scenario of single-task models). Of course this\\nwill happen only if some assumptions about the statistical relationship between\\nthe different tasks are valid, meaning that there is something shared across some\\nof the tasks.\\nFrom the point of view of deep learning, the underlying prior belief is the\\nfollowing: among the factors that explain the variations observed in the data\\nassociated with the different tasks, some are shared across two or more tasks.\\n7.8 Early Stopping\\nWhen training large models with sufficient representational capacity to overfit\\nthe task, we often observe that training error decreases steadily over time, but\\nvalidation set error begins to rise again. See figure 7.3 for an example of this\\nbehavior. This behavior occurs very reliably.\\nThis means we can obtain a model with better validation set error (and thus,\\nhopefully better test set error) by returning to the parameter setting at the point in'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='time with the lowest validation set error. Every time the error on the validation set\\nimproves, we store a copy of the model parameters. When the training algorithm\\nterminates, we return these parameters, rather than the latest parameters. The\\n246\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nalgorithm terminates when no parameters have improved over the best recorded\\nvalidation error for some pre-specified number of iterations. This procedure is\\nspecified more formally in algorithm 7.1.\\nAlgorithm 7.1 The early stopping meta-algorithm for determining the best\\namount of time to train. This meta-algorithm is a general strategy that works\\nwell with a variety of training algorithms and ways of quantifying error on the\\nvalidation set.\\nLet n be the number of steps between evaluations.\\nLet p be the “patience,” the number of times to observe worsening validation set\\nerror before giving up.\\nLet θ be the initial parameters.\\no\\nθ θ\\no\\n←\\ni 0\\n←\\nj 0\\n←\\nv\\n← ∞\\nθ θ\\n∗\\n←\\ni i\\n∗\\n←\\nwhile j < p do'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Update θ by running the training algorithm for n steps.\\ni i+ n\\n←\\nv ValidationSetError(θ)\\n\\ue030\\n←\\nif v < v then\\n\\ue030\\nj 0\\n←\\nθ θ\\n∗\\n←\\ni i\\n∗\\n←\\nv v\\n\\ue030\\n←\\nelse\\nj j + 1\\n←\\nend if\\nend while\\nBest parameters are θ , best number of training steps is i\\n∗ ∗\\nThis strategy is known as early stopping. It is probably the most commonly\\nused form of regularization in deep learning. Its popularity is due both to its\\neffectiveness and its simplicity.\\nOneway to think ofearly stopping is as averyefficienthyperparameter selection\\nalgorithm. Inthis view, thenumber oftrainingsteps isjust anotherhyperparameter.\\nWe can see in figure 7.3 that this hyperparameter has a U-shaped validation set\\n247\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nperformance curve. Most hyperparameters that control model capacity have such a\\nU-shapedvalidation set performancecurve,as illustrated infigure 5.3. In the caseof\\nearly stopping, we are controlling the effective capacity of the model bydetermining'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='how many steps it can take to fit the training set. Most hyperparameters must be\\nchosen using an expensive guess and check process, where we set a hyperparameter\\nat the start of training, then run training for several steps to see its effect. The\\n“training time” hyperparameter is unique in that by definition a single run of\\ntraining tries out many values of the hyperparameter. The only significant cost\\nto choosing this hyperparameter automatically via early stopping is running the\\nvalidation set evaluation periodically during training. Ideally, this is done in\\nparallel to the training process on a separate machine, separate CPU, or separate\\nGPU from the main training process. If such resources are not available, then the\\ncost of these periodic evaluations may be reduced by using a validation set that is\\nsmall compared to the training set or by evaluating the validation set error less\\nfrequently and obtaining a lower resolution estimate of the optimal training time.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='An additional cost to early stopping is the need to maintain a copy of the\\nbest parameters. This cost is generally negligible, because it is acceptable to store\\nthese parameters in a slower and larger form of memory (for example, training in\\nGPU memory, but storing the optimal parameters in host memory or on a disk\\ndrive). Since the best parameters are written to infrequently and never read during\\ntraining, these occasional slow writes have little effect on the total training time.\\nEarly stopping is a very unobtrusive form of regularization, in that it requires\\nalmost no change in the underlying training procedure, the objective function,\\nor the set of allowable parameter values. This means that it is easy to use early\\nstopping without damaging the learning dynamics. This is in contrast to weight\\ndecay, where one must be careful not to use too much weight decay and trap the\\nnetwork in a bad local minimum corresponding to a solution with pathologically\\nsmall weights.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Early stopping may be used either alone or in conjunction with other regulariza-\\ntion strategies. Even when using regularization strategies that modify the objective\\nfunction to encourage better generalization, it is rare for the best generalization to\\noccur at a local minimum of the training objective.\\nEarly stopping requires a validation set, which means some training data is not\\nfed to the model. To best exploit this extra data, one can perform extra training\\nafter the initial training with early stopping has completed. In the second, extra\\ntraining step, all of the training data is included. There are two basic strategies\\none can use for this second training procedure.\\nOne strategy (algorithm 7.2) is to initialize the model again and retrain on all\\n248\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nof the data. In this second training pass, we train for the same number of steps as\\nthe early stopping procedure determined was optimal in the first pass. There are'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='some subtleties associated with this procedure. For example, there is not a good\\nway of knowing whether to retrain for the same number of parameter updates or\\nthe same number of passes through the dataset. On the second round of training,\\neach pass through the dataset will require more parameter updates because the\\ntraining set is bigger.\\nAlgorithm 7.2 A meta-algorithm for using early stopping to determine how long\\nto train, then retraining on all the data.\\nLet X(train) and y(train) be the training set.\\nSplit X(train) and y(train) into (X(subtrain), X(valid)) and (y(subtrain), y(valid))\\nrespectively.\\nRun early stopping (algorithm 7.1) starting from random θ using X(subtrain) and\\ny(subtrain) for training data and X(valid) and y(valid) for validation data. This\\nreturns i , the optimal number of steps.\\n∗\\nSet θ to random values again.\\nTrain on X(train) and y(train) for i steps.\\n∗\\nAnother strategy for using all of the data is to keep the parameters obtained'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='from the first round of training and then continue training but now using all of\\nthe data. At this stage, we now no longer have a guide for when to stop in terms\\nof a number of steps. Instead, we can monitor the average loss function on the\\nvalidation set, and continue training until it falls below the value of the training\\nset objective at which the early stopping procedure halted. This strategy avoids\\nthe high cost of retraining the model from scratch, but is not as well-behaved. For\\nexample, there is not any guarantee that the objective on the validation set will\\never reach the target value, so this strategy is not even guaranteed to terminate.\\nThis procedure is presented more formally in algorithm 7.3.\\nEarly stopping is also useful because it reduces the computational cost of the\\ntrainingprocedure. Besides theobviousreductionin costdueto limitingthenumber\\nof training iterations, it also has the benefit of providing regularization without'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='requiring the addition of penalty terms to the cost function or the computation of\\nthe gradients of such additional terms.\\nHow early stopping acts as a regularizer: So far we have stated that early\\nstopping is a regularization strategy, but we have supported this claim only by\\nshowing learning curves where the validation set error has a U-shaped curve. What\\n249\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nAlgorithm 7.3 Meta-algorithm using early stopping to determine at what objec-\\ntive value we start to overfit, then continue training until that value is reached.\\nLet X(train) and y(train) be the training set.\\nSplit X(train) and y(train) into (X(subtrain), X(valid)) and (y(subtrain), y(valid))\\nrespectively.\\nRun early stopping (algorithm 7.1) starting from random θ using X(subtrain) and\\ny(subtrain) for training data and X(valid) and y(valid) for validation data. This\\nupdates θ.\\n\\ue00f J(θ,X(subtrain),y(subtrain))\\n←\\nwhile J(θ,X(valid),y(valid)) > \\ue00f do'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Train on X(train) and y(train) for n steps.\\nend while\\nis the actual mechanism by which early stopping regularizes the model? Bishop\\n(1995a) and Sjöberg and Ljung (1995) argued that early stopping has the effect of\\nrestricting the optimization procedure to a relatively small volume of parameter\\nspace in the neighborhood of the initial parameter value θ , as illustrated in\\no\\nfigure 7.4. More specifically, imagine taking τ optimization steps (corresponding\\nto τ training iterations) and with learning rate \\ue00f. We can view the product \\ue00fτ\\nas a measure of effective capacity. Assuming the gradient is bounded, restricting\\nboth the number of iterations and the learning rate limits the volume of parameter\\nspace reachable from θ . In this sense, \\ue00fτ behaves as if it were the reciprocal of\\no\\nthe coefficient used for weight decay.\\nIndeed, we can show how—in the case of a simple linear model with a quadratic\\nerror function and simple gradient descent—early stopping is equivalent to L2\\nregularization.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In order to compare with classical L2 regularization, we examine a simple\\nsetting where the only parameters are linear weights (θ = w). We can model\\nthe cost function J with a quadratic approximation in the neighborhood of the\\nempirically optimal value of the weights w :\\n∗\\n1\\nˆ\\nJ(θ) = J(w )+ (w w ) H(w w ), (7.33)\\n∗ ∗ \\ue03e ∗\\n2 − −\\nwhere H is the Hessian matrix of J with respect to w evaluated at w . Given the\\n∗\\nassumption that w is a minimum of J(w), weknowthat H is positive semidefinite.\\n∗\\nUnder a local Taylor series approximation, the gradient is given by:\\nˆ\\nJ(w) = H(w w ). (7.34)\\nw ∗\\n∇ −\\n250\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nw\\n1\\n2w\\nw\\n∗\\nw˜\\nw\\n1\\nw\\n2\\nw\\n∗\\nw˜\\nFigure 7.4: An illustration of the effect of early stopping. (Left)The solid contour lines\\nindicatethecontoursofthenegativelog-likelihood. Thedashedlineindicatesthetrajectory\\ntaken by SGD beginning from the origin. Rather than stopping at the point w that\\n∗'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='minimizes the cost, early stopping results in the trajectory stopping at an earlier point w˜.\\n(Right)An illustration of the effect of L2 regularization for comparison. The dashed circles\\nindicate the contours of the L2 penalty, which causes the minimum of the total cost to lie\\nnearer the origin than the minimum of the unregularized cost.\\nWe are going to study the trajectory followed by the parameter vector during\\ntraining. For simplicity, let us set the initial parameter vector to the origin,3 that\\nis w(0) = 0. Let us study the approximate behavior of gradient descent on J by\\nˆ\\nanalyzing gradient descent on J:\\nw(τ) = w(τ 1) \\ue00f Jˆ (w(τ 1)) (7.35)\\n− w −\\n− ∇\\n= w(τ 1) \\ue00fH(w(τ 1) w ) (7.36)\\n− − ∗\\n− −\\nw(τ) w = (I \\ue00fH)(w(τ 1) w ). (7.37)\\n∗ − ∗\\n− − −\\nLet us now rewrite this expression in the space of the eigenvectors of H, exploiting\\nthe eigendecomposition of H: H = QΛQ , whereΛ is a diagonal matrix and Q\\n\\ue03e\\nis an orthonormal basis of eigenvectors.\\nw(τ) w = (I \\ue00fQΛQ )(w(τ 1) w ) (7.38)\\n∗ \\ue03e − ∗'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='− − −\\nQ (w(τ) w ) = (I \\ue00fΛ)Q (w(τ 1) w ) (7.39)\\n\\ue03e ∗ \\ue03e − ∗\\n− − −\\n3For neural networks, to obtain symmetry breaking between hidden units, we cannot initialize\\nall the parameters to 0, as discussed in section 6.2. However, the argument holds for any other\\ninitial value w(0).\\n251\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nAssuming that w(0) = 0 and that \\ue00f is chosen to be small enough to guarantee\\n1 \\ue00fλ < 1, the parameter trajectory during training after τ parameter updates\\ni\\n| − |\\nis as follows:\\nQ w(τ) = [I (I \\ue00fΛ)τ]Q w . (7.40)\\n\\ue03e \\ue03e ∗\\n− −\\nNow, the expression for Q w˜ in equation 7.13 for L2 regularization can be rear-\\n\\ue03e\\nranged as:\\nQ w˜ = (Λ +αI) 1ΛQ w (7.41)\\n\\ue03e − \\ue03e ∗\\nQ w˜ = [I (Λ +αI) 1α]Q w (7.42)\\n\\ue03e − \\ue03e ∗\\n−\\nComparing equation 7.40 and equation 7.42, we see that if the hyperparameters \\ue00f,\\nα, and τ are chosen such that\\n(I \\ue00fΛ)τ = (Λ+ αI) 1α, (7.43)\\n−\\n−\\nthen L2 regularization and early stopping can be seen to be equivalent (at least'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='under the quadratic approximation of the objective function). Going even further,\\nby taking logarithms and using the series expansion for log(1+x), we can conclude\\nthat if all λ are small (that is, \\ue00fλ 1 and λ /α 1) then\\ni i i\\n\\ue01c \\ue01c\\n1\\nτ , (7.44)\\n≈ \\ue00fα\\n1\\nα . (7.45)\\n≈ τ\\ue00f\\nThat is, under these assumptions, the number of training iterations τ plays a role\\ninversely proportional to the L2 regularization parameter, and the inverse of τ\\ue00f\\nplays the role of the weight decay coefficient.\\nParameter values corresponding to directions of significant curvature (of the\\nobjective function) are regularized less than directions of less curvature. Of course,\\nin the context of early stopping, this really means that parameters that correspond\\nto directions of significant curvature tend to learn early relative to parameters\\ncorresponding to directions of less curvature.\\nThe derivations in this section have shown that a trajectory of length τ ends'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='at a point that corresponds to a minimum of the L2-regularized objective. Early\\nstopping is of course more than the mere restriction of the trajectory length;\\ninstead, early stopping typically involves monitoring the validation set error in\\norder to stop the trajectory at a particularly good point in space. Early stopping\\ntherefore has the advantage over weight decay that early stopping automatically\\ndetermines the correct amount of regularization while weight decay requires many\\ntraining experiments with different values of its hyperparameter.\\n252\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\n7.9 Parameter Tying and Parameter Sharing\\nThus far, in this chapter, when we have discussed adding constraints or penalties\\nto the parameters, we have always done so with respect to a fixed region or point.\\nFor example, L2 regularization (or weight decay) penalizes model parameters for\\ndeviating from the fixed value of zero. However, sometimes we may need other'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='ways to express our prior knowledge about suitable values of the model parameters.\\nSometimes we might not know precisely what values the parameters should take\\nbut we know, from knowledge of the domain and model architecture, that there\\nshould be some dependencies between the model parameters.\\nA common type of dependency that we often want to express is that certain\\nparameters should be close to one another. Consider the following scenario: we\\nhave two models performing the same classification task (with the same set of\\nclasses) but with somewhat different input distributions. Formally, we have model\\nA with parameters w(A) and model B with parameters w(B). The two models\\nmap the input to two different, but related outputs: yˆ(A) = f(w(A),x) and\\nyˆ(B) = g(w(B),x).\\nLet us imagine that the tasks are similar enough (perhaps with similar input\\nand output distributions) that we believe the model parameters should be close\\n(A) (B)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to each other: i, w should be close to w . We can leverage this information\\n∀ i i\\nthrough regularization. Specifically, we can use a parameter norm penalty of the\\nform: Ω(w(A),w(B)) = w(A) w(B) 2. Here we used an L2 penalty, but other\\n2\\n\\ue06b − \\ue06b\\nchoices are also possible.\\nThis kind of approach was proposed by Lasserre et al. (2006), who regularized\\nthe parameters of one model, trained as a classifier in a supervised paradigm, to\\nbe close to the parameters of another model, trained in an unsupervised paradigm\\n(to capture the distribution of the observed input data). The architectures were\\nconstructed such that many of the parameters in the classifier model could be\\npaired to corresponding parameters in the unsupervised model.\\nWhile a parameter norm penalty is one way to regularize parameters to be\\nclose to one another, the more popular way is to use constraints: to force sets\\nof parameters to be equal. This method of regularization is often referred to as'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='parameter sharing,because weinterpretthevariousmodelsormodelcomponents\\nas sharing a unique set of parameters. A significant advantage of parameter sharing\\nover regularizing the parameters to be close (via a norm penalty) is that only a\\nsubset of the parameters (the unique set) need to be stored in memory. In certain\\nmodels—such as the convolutional neural network—this can lead to significant\\nreduction in the memory footprint of the model.\\n253\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nConvolutional Neural Networks By far the most popular and extensive use\\nof parameter sharing occurs in convolutional neural networks (CNNs) applied\\nto computer vision.\\nNatural images havemanystatistical properties that are invariant to translation.\\nFor example, a photo of a cat remains a photo of a cat if it is translated one pixel\\nto the right. CNNs take this property into account by sharing parameters across\\nmultiple image locations. The same feature (a hidden unit with the same weights)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='is computed over different locations in the input. This means that we can find a\\ncat with the same cat detector whether the cat appears at column i or column\\ni+ 1 in the image.\\nParametersharinghasallowedCNNstodramaticallylowerthenumberofunique\\nmodel parameters and to significantly increase network sizes without requiring a\\ncorresponding increase in training data. It remains one of the best examples of\\nhow to effectively incorporate domain knowledge into the network architecture.\\nCNNs will be discussed in more detail in chapter 9.\\n7.10 Sparse Representations\\nWeight decay acts by placing a penalty directly on the model parameters. Another\\nstrategy is to place a penalty on the activations of the units in a neural network,\\nencouraging their activations to be sparse. This indirectly imposes a complicated\\npenalty on the model parameters.\\nWe have already discussed (in section 7.1.2) how L1 penalization induces\\na sparse parametrization—meaning that many of the parameters become zero'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='(or close to zero). Representational sparsity, on the other hand, describes a\\nrepresentation where many of the elements of the representation are zero (or close\\nto zero). A simplified view of this distinction can be illustrated in the context of\\nlinear regression:\\n2\\n18 4 0 0 2 0 0\\n− 3\\n5 0 0 1 0 3 0 \\uf8ee \\uf8f9\\n\\uf8ee \\uf8f9 \\uf8ee − \\uf8f9 2\\n15 = 0 5 0 0 0 0 −\\n\\uf8ef 5 \\uf8fa (7.46)\\n\\uf8ef 9 \\uf8fa \\uf8ef 1 0 0 1 0 4 \\uf8fa \\uf8ef − \\uf8fa\\n\\uf8ef − \\uf8fa \\uf8ef − − \\uf8fa \\uf8ef 1 \\uf8fa\\n\\uf8ef 3 \\uf8fa \\uf8ef 1 0 0 0 5 0 \\uf8fa \\uf8ef \\uf8fa\\n\\uf8ef − \\uf8fa \\uf8ef − \\uf8fa \\uf8ef 4 \\uf8fa\\n\\uf8f0 \\uf8fb \\uf8f0 \\uf8fb \\uf8ef \\uf8fa\\ny Rm A Rm n \\uf8f0x R\\uf8fbn\\n×\\n∈ ∈ ∈\\n254\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\n0\\n14 3 1 2 5 4 1\\n− − − 2\\n1 4 2 3 1 1 3 \\uf8ee \\uf8f9\\n\\uf8ee \\uf8f9 \\uf8ee − − \\uf8f9 0\\n19 = 1 5 4 2 3 2\\n− − − \\uf8ef 0 \\uf8fa (7.47)\\n\\uf8ef 2 \\uf8fa \\uf8ef 3 1 2 3 0 3 \\uf8fa \\uf8ef \\uf8fa\\n\\uf8ef \\uf8fa \\uf8ef − − \\uf8fa \\uf8ef 3 \\uf8fa\\n\\uf8ef 23 \\uf8fa \\uf8ef 5 4 2 2 5 1 \\uf8fa \\uf8ef − \\uf8fa\\n\\uf8ef \\uf8fa \\uf8ef − − − − \\uf8fa \\uf8ef 0 \\uf8fa\\n\\uf8f0 \\uf8fb \\uf8f0 \\uf8fb \\uf8ef \\uf8fa\\ny Rm B Rm n \\uf8f0h R\\uf8fbn\\n×\\n∈ ∈ ∈\\nIn the first expression, we have an example of a sparsely parametrized linear\\nregression model. In the second, we have linear regression with a sparse representa-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='tion h of the data x. That is, h is a function of x that, in some sense, represents\\nthe information present in x, but does so with a sparse vector.\\nRepresentationalregularizationisaccomplishedbythesamesortsofmechanisms\\nthat we have used in parameter regularization.\\nNorm penalty regularization of representations is performed by adding to the\\nloss function J a norm penalty on the representation. This penalty is denoted\\n˜\\nΩ(h). As before, we denote the regularized loss function by J:\\n˜\\nJ(θ;X,y) = J(θ;X,y)+ αΩ(h) (7.48)\\nwhere α [0, ) weights the relative contribution of the norm penalty term, with\\n∈ ∞\\nlarger values of α corresponding to more regularization.\\nJust as an L1 penalty on the parameters induces parameter sparsity, an L1\\npenalty on the elements of the representation induces representational sparsity:\\nΩ(h) = h = h . Of course, the L1 penalty is only one choice of penalty\\n|| ||1 i| i |\\nthat can result in a sparse representation. Others include the penalty derived from\\n\\ue050'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a Student-t prior on the representation (Olshausen and Field, 1996; Bergstra, 2011)\\nand KL divergence penalties (Larochelle and Bengio, 2008) that are especially\\nuseful for representations with elements constrained to lie on the unit interval.\\nLee et al. (2008) and Goodfellow et al. (2009) both provide examples of strategies\\nbased on regularizing the average activation across several examples, 1 h(i), to\\nm i\\nbe near some target value, such as a vector with .01 for each entry.\\n\\ue050\\nOther approaches obtain representational sparsity with a hard constraint on\\nthe activation values. For example, orthogonal matching pursuit (Pati et al.,\\n1993) encodes an input x with the representation h that solves the constrained\\noptimization problem\\nargmin x Wh 2, (7.49)\\n\\ue06b − \\ue06b\\nh, h <k\\n0\\n\\ue06b \\ue06b\\nwhere h is the number of non-zero entries of h. This problem can be solved\\n0\\n\\ue06b \\ue06b\\nefficiently when W is constrained to be orthogonal. This method is often called\\n255\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='OMP-k with the value of k specified to indicate the number of non-zero features\\nallowed. Coates and Ng (2011) demonstrated that OMP-1 can be a very effective\\nfeature extractor for deep architectures.\\nEssentially any model that has hidden units can be made sparse. Throughout\\nthis book, we will see many examples of sparsity regularization used in a variety of\\ncontexts.\\n7.11 Bagging and Other Ensemble Methods\\nBagging (short for bootstrap aggregating) is a technique for reducing gen-\\neralization error by combining several models (Breiman, 1994). The idea is to\\ntrain several different models separately, then have all of the models vote on the\\noutput for test examples. This is an example of a general strategy in machine\\nlearning called model averaging. Techniques employing this strategy are known\\nas ensemble methods.\\nThe reason that model averaging works is that different models will usually\\nnot make all the same errors on the test set.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Consider for example a set of k regression models. Suppose that each model\\nmakes an error \\ue00f on each example, with the errors drawn from a zero-mean\\ni\\nmultivariate normal distribution with variances E [\\ue00f2] = v and covariances E [\\ue00f \\ue00f ] =\\ni i j\\nc. Then the error made by the average prediction of all the ensemble models is\\n1 \\ue00f . The expected squared error of the ensemble predictor is\\nk i i\\n\\ue050\\n2\\n1 1\\nE \\ue00f = E \\ue00f2 + \\ue00f \\ue00f (7.50)\\n\\uf8ee k i \\uf8f9 k2 \\uf8ee \\uf8eb i i j \\uf8f6\\uf8f9\\n\\ue020 \\ue021\\ni i j=i\\n\\ue058 \\ue058 \\ue058\\ue036\\n\\uf8f0 \\uf8fb 1 \\uf8f0k \\uf8ed1 \\uf8f8\\uf8fb\\n= v + − c. (7.51)\\nk k\\nIn the case where the errors are perfectly correlated and c = v, the mean squared\\nerror reduces to v, so the model averaging does not help at all. In the case where\\nthe errors are perfectly uncorrelated and c = 0, the expected squared error of the\\nensemble is only 1v. This means that the expected squared error of the ensemble\\nk\\ndecreases linearly with the ensemble size. In other words, on average, the ensemble\\nwill perform at least as well as any of its members, and if the members make'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='independent errors, the ensemble will perform significantly better than its members.\\nDifferent ensemble methods construct the ensemble of models in different ways.\\nForexample, each member ofthe ensemblecould be formedbytraining acompletely\\n256\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nOriginal dataset\\nFirst resampled dataset First ensemble member\\n8\\nSecond resampled dataset Second ensemble member\\n8\\nFigure 7.5: A cartoon depiction of how bagging works. Suppose we train an 8 detector on\\nthe dataset depicted above, containing an 8, a 6 and a 9. Suppose we make two different\\nresampled datasets. The bagging training procedure is to construct each of these datasets\\nby sampling with replacement. The first dataset omits the 9 and repeats the 8. On this\\ndataset, the detector learns that a loop on top of the digit corresponds to an 8. On\\nthe second dataset, we repeat the 9 and omit the 6. In this case, the detector learns'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='that a loop on the bottom of the digit corresponds to an 8. Each of these individual\\nclassification rules is brittle, but if we average their output then the detector is robust,\\nachieving maximal confidence only when both loops of the 8 are present.\\ndifferent kind of model using a different algorithm or objective function. Bagging\\nis a method that allows the same kind of model, training algorithm and objective\\nfunction to be reused several times.\\nSpecifically, bagging involves constructing k different datasets. Each dataset\\nhas the same number of examples as the original dataset, but each dataset is\\nconstructed by sampling with replacement from the original dataset. This means\\nthat, with high probability, each dataset is missing some of the examples from the\\noriginal dataset and also contains several duplicate examples (on average around\\n2/3 of the examples from the original dataset are found in the resulting training'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='set, if it has the same size as the original). Model i is then trained on dataset\\ni. The differences between which examples are included in each dataset result in\\ndifferences between the trained models. See figure 7.5 for an example.\\nNeural networks reach a wide enough variety of solution points that they can\\noften benefit from model averaging even if all of the models are trained on the same\\ndataset. Differences in random initialization, random selection of minibatches,\\ndifferences in hyperparameters, or different outcomes of non-deterministic imple-\\nmentations of neural networks are often enough to cause different members of the\\n257\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nensemble to make partially independent errors.\\nModel averaging is an extremely powerful and reliable method for reducing\\ngeneralization error. Its use is usually discouraged when benchmarking algorithms\\nfor scientific papers, because any machine learning algorithm can benefit substan-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='tially from model averaging at the price of increased computation and memory.\\nFor this reason, benchmark comparisons are usually made using a single model.\\nMachine learning contests are usually won by methods using model averag-\\ning over dozens of models. A recent prominent example is the Netflix Grand\\nPrize (Koren, 2009).\\nNot all techniques for constructing ensembles are designed to makethe ensemble\\nmore regularized than the individual models. For example, a technique called\\nboosting (Freund and Schapire, 1996b,a) constructs an ensemble with higher\\ncapacity than the individual models. Boosting has been applied to build ensembles\\nof neural networks (Schwenk and Bengio, 1998) by incrementally adding neural\\nnetworks to the ensemble. Boosting has also been applied interpreting an individual\\nneural network as an ensemble (Bengio et al., 2006a), incrementally adding hidden\\nunits to the neural network.\\n7.12 Dropout\\nDropout (Srivastava et al., 2014) provides a computationally inexpensive but'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='powerful method of regularizing a broad family of models. To a first approximation,\\ndropout can be thought of as a method of making bagging practical for ensembles\\nof very many large neural networks. Bagging involves training multiple models,\\nand evaluating multiple models on each test example. This seems impractical\\nwhen each model is a large neural network, since training and evaluating such\\nnetworks is costly in terms of runtime and memory. It is common to use ensembles\\nof five to ten neural networks—Szegedy et al. (2014a) used six to win the ILSVRC—\\nbut more than this rapidly becomes unwieldy. Dropout provides an inexpensive\\napproximation to training and evaluating a bagged ensemble of exponentially many\\nneural networks.\\nSpecifically, dropout trains the ensemble consisting of all sub-networks that\\ncan be formed by removing non-output units from an underlying base network,\\nas illustrated in figure 7.6. In most modern neural networks, based on a series of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='affine transformations and nonlinearities, we can effectively remove a unit from a\\nnetwork by multiplying its output value by zero. This procedure requires some\\nslight modification for models such as radial basis function networks, which take\\n258\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nthe difference between the unit’s state and some reference value. Here, we present\\nthe dropout algorithm in terms of multiplication by zero for simplicity, but it can\\nbe trivially modified to work with other operations that remove a unit from the\\nnetwork.\\nRecall that to learn with bagging, we define k different models, construct k\\ndifferent datasets by sampling from the training set with replacement, and then\\ntrain model ion dataset i. Dropout aims to approximate this process, but with an\\nexponentially large number of neural networks. Specifically, to train with dropout,\\nwe use a minibatch-based learning algorithm that makes small steps, such as'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='stochastic gradient descent. Each time we load an example into a minibatch, we\\nrandomly sample a different binary mask to apply to all of the input and hidden\\nunits in the network. The mask for each unit is sampled independently from all of\\nthe others. The probability of sampling a mask value of one (causing a unit to be\\nincluded) is a hyperparameter fixed before training begins. It is not a function\\nof the current value of the model parameters or the input example. Typically,\\nan input unit is included with probability 0.8 and a hidden unit is included with\\nprobability 0.5. We then run forward propagation, back-propagation, and the\\nlearning update as usual. Figure 7.7 illustrates how to run forward propagation\\nwith dropout.\\nMore formally, suppose that a mask vector µ specifies which units to include,\\nand J(θ,µ) defines the cost of the model defined by parameters θ and mask µ.\\nE\\nThen dropout training consists in minimizing J(θ,µ). The expectation contains\\nµ'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='exponentially many terms but we can obtain an unbiased estimate of its gradient\\nby sampling values of µ.\\nDropout training is not quite the same as bagging training. In the case of\\nbagging, the models are all independent. In the case of dropout, the models share\\nparameters, with each model inheriting a different subset of parameters from the\\nparent neural network. This parameter sharing makes it possible to represent an\\nexponential number of models with a tractable amount of memory. In the case of\\nbagging, each model is trained to convergence on its respective training set. In the\\ncase of dropout, typically most models are not explicitly trained at all—usually,\\nthe model is large enough that it would be infeasible to sample all possible sub-\\nnetworks within the lifetime of the universe. Instead, a tiny fraction of the possible\\nsub-networks are each trained for a single step, and the parameter sharing causes\\nthe remaining sub-networks to arrive at good settings of the parameters. These'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='are the only differences. Beyond these, dropout follows the bagging algorithm. For\\nexample, the training set encountered by each sub-network is indeed a subset of\\nthe original training set sampled with replacement.\\n259\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nyy yy yy yy\\nhh hh hh hh hh hh hh\\n11 22 11 22 11 22 22\\nxx xx xx xx xx xx\\n11 22 22 11 11 22\\nyy\\nyy yy yy yy\\nhh11 hh11 hh22 hh22\\nhh hh\\n11 22\\nxx xx xx xx xx\\n11 22 11 22 22\\nyy yy yy yy\\nxx xx\\n11 22\\nhh hh hh\\n11 11 22\\nBase network\\nxx xx xx xx\\n11 22 11 11\\nyy yy yy yy\\nhh hh\\n22 11\\nxx\\n22\\nEnsemble of subnetworks\\nFigure 7.6: Dropout trains an ensemble consisting of all sub-networks that can be\\nconstructed by removing non-output units from an underlying base network. Here, we\\nbegin with a base network with two visible units and two hidden units. There are sixteen\\npossible subsets of these four units. We show all sixteen subnetworks that may be formed\\nby dropping out different subsets of units from the original network. In this small example,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a large proportion of the resulting networks have no input units or no path connecting\\nthe input to the output. This problem becomes insignificant for networks with wider\\nlayers, where the probabilityof dropping all possible paths from inputs to outputs becomes\\nsmaller.\\n260\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nyy\\nhh hh\\n11 22\\nxx xx\\n11 22\\nyy\\nˆˆ ˆˆ\\nhh hh\\n11 22\\nµµhh\\n11\\nhh\\n11\\nhh\\n22\\nµµhh\\n22\\nxxˆˆ xxˆˆ\\n11 22\\nµµ xx xx µµ\\nxx11 11 22 xx22\\nFigure 7.7: An example of forward propagation through a feedforward network using\\ndropout. (Top)In this example, we use a feedforward network with two input units, one\\nhidden layer with two hidden units, and one output unit. (Bottom)To perform forward\\npropagation with dropout, we randomly sample a vector µ with one entry for each input\\nor hidden unit in the network. The entries of µ are binary and are sampled independently\\nfrom each other. The probability of each entry being 1 is a hyperparameter, usually 0.5'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='for the hidden layers and 0.8 for the input. Each unit in the network is multiplied by\\nthe corresponding mask, and then forward propagation continues through the rest of the\\nnetwork as usual. This is equivalent to randomly selecting one of the sub-networks from\\nfigure 7.6 and running forward propagation through it.\\n261\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nTo make a prediction, a bagged ensemble must accumulate votes from all of\\nits members. We refer to this process as inference in this context. So far, our\\ndescription of bagging and dropout has not required that the model be explicitly\\nprobabilistic. Now, we assume that the model’s role is to output a probability\\ndistribution. Inthecaseof bagging,eachmodeliproducesaprobabilitydistribution\\np(i)(y x). The prediction of the ensemble is given by the arithmetic mean of all\\n|\\nof these distributions,\\nk\\n1\\np(i)(y x). (7.52)\\nk |\\ni=1\\n\\ue058\\nIn the case of dropout, each sub-model defined by mask vector µ defines a prob-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='ability distribution p(y x,µ). The arithmetic mean over all masks is given\\n|\\nby\\np(µ)p(y x,µ) (7.53)\\n|\\nµ\\n\\ue058\\nwhere p(µ) is the probability distribution that was used to sample µ at training\\ntime.\\nBecause this sum includes an exponential number of terms, it is intractable\\nto evaluate except in cases where the structure of the model permits some form\\nof simplification. So far, deep neural nets are not known to permit any tractable\\nsimplification. Instead, we can approximate the inference with sampling, by\\naveraging together the output from many masks. Even 10-20 masks are often\\nsufficient to obtain good performance.\\nHowever, there is an even better approach, that allows us to obtain a good\\napproximation to the predictions of the entire ensemble, at the cost of only one\\nforward propagation. To do so, we change to using the geometric mean rather than\\nthe arithmetic mean of the ensemble members’ predicted distributions. Warde-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Farley et al. (2014) present arguments and empirical evidence that the geometric\\nmean performs comparably to the arithmetic mean in this context.\\nThegeometric mean of multipleprobabilitydistributions isnot guaranteedto be\\na probability distribution. To guarantee that the result is a probability distribution,\\nwe impose the requirement that none of the sub-models assigns probability 0 to any\\nevent, and we renormalize the resulting distribution. The unnormalized probability\\ndistribution defined directly by the geometric mean is given by\\np˜ ensemble(y x) = 2d p(y x,µ) (7.54)\\n| |\\n\\ue073 µ\\n\\ue059\\nwhere d is the number of units that may be dropped. Here we use a uniform\\ndistribution over µ to simplify the presentation, but non-uniform distributions are\\n262\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nalso possible. To make predictions we must re-normalize the ensemble:\\np˜ (y x)\\nensemble\\np (y x) = | . (7.55)\\nensemble\\n| p˜ (y x)\\ny ensemble \\ue030\\n\\ue030 |\\n\\ue050'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='A key insight (Hinton et al., 2012c) involved in dropout is that we can approxi-\\nmate p by evaluating p(y x) in one model: the model with all units, but\\nensemble\\n|\\nwith the weights going out of unit i multiplied by the probability of including unit\\ni. The motivation for this modification is to capture the right expected value of the\\noutput from that unit. We call this approach the weight scaling inference rule.\\nThere is not yet any theoretical argument for the accuracy of this approximate\\ninference rule in deep nonlinear networks, but empirically it performs very well.\\nBecause we usually use an inclusion probability of 1, the weight scaling rule\\n2\\nusually amounts to dividing the weights by 2 at the end of training, and then using\\nthe model as usual. Another way to achieve the same result is to multiply the\\nstates of the units by 2 during training. Either way, the goal is to make sure that\\nthe expected total input to a unit at test time is roughly the same as the expected'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='total input to that unit at train time, even though half the units at train time are\\nmissing on average.\\nFor many classes of models that do not have nonlinear hidden units, the weight\\nscaling inference rule is exact. For a simple example, consider a softmax regression\\nclassifier with n input variables represented by the vector v:\\nP(y = y v) = softmax W v + b . (7.56)\\n\\ue03e\\n| y\\n\\ue010 \\ue011\\nWe can index into the family of sub-models by element-wise multiplication of the\\ninput with a binary vector d:\\nP(y = y v;d) = softmax W (d v)+ b . (7.57)\\n\\ue03e\\n| \\ue00c y\\n\\ue010 \\ue011\\nThe ensemble predictor is defined by re-normalizing the geometric mean over all\\nensemble members’ predictions:\\n˜\\nP (y = y v)\\nensemble\\nP (y = y v) = | (7.58)\\nensemble | P˜ (y = y v)\\ny \\ue030 ensemble \\ue030 |\\n\\ue050\\nwhere\\n˜\\nP ensemble(y = y\\n|\\nv) = 2n P(y = y\\n|\\nv;d). (7.59)\\nd 0,1 n\\n∈{ }\\n263\\n\\ue073\\n\\ue059\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\n˜\\nTo see that the weight scaling rule is exact, we can simplify P :\\nensemble\\n˜\\nP (y = y v) = P(y = y v;d) (7.60)\\nensemble\\n|\\n2n\\n|'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='\\ue073d 0,1 n\\n∈\\ue059{ }\\n= softmax(W (d v)+ b) (7.61)\\n2n \\ue03e\\n\\ue00c\\ny\\n\\ue073d 0,1 n\\n∈{\\ue059}\\nexp W (d v)+ b\\ny\\ue03e,: y\\n= n \\ue00c (7.62)\\n2\\n\\ue076 exp\\ue000 W (d v)+\\ue001b\\n\\ue075 \\ue075d ∈{\\ue0590,1 }n y \\ue030 y\\ue03e \\ue030,: \\ue00c y \\ue030\\n\\ue074 \\ue010 \\ue011\\n\\ue050\\n2n exp W (d v)+ b\\nd 0,1 n y\\ue03e,: y\\n\\ue00c\\n= ∈{ } (7.63)\\n\\ue071\\n\\ue051 \\ue000 \\ue001\\n2n\\nexp W (d v)+ b\\nd ∈{0,1 }n y \\ue030 y\\ue03e \\ue030,: \\ue00c y \\ue030\\n\\ue072\\n\\ue010 \\ue011\\n\\ue051 \\ue050\\n˜\\nBecause P will be normalized, we can safely ignore multiplication by factors that\\nare constant with respect to y:\\n˜\\nP (y = y v) exp W (d v)+ b (7.64)\\nensemble\\n| ∝\\n2n y\\ue03e,:\\n\\ue00c\\ny\\n\\ue073d 0,1 n\\n∈\\ue059{ } \\ue000 \\ue001\\n1\\n= exp W (d v)+ b (7.65)\\n\\uf8eb2n\\ny\\ue03e,:\\n\\ue00c\\ny\\n\\uf8f6\\nd 0,1 n\\n∈\\ue058{ }\\n\\uf8ed \\uf8f8\\n1\\n= exp W v + b . (7.66)\\n2\\ny\\ue03e,: y\\n\\ue012 \\ue013\\nSubstituting this backinto equation 7.58 we obtain a softmax classifier with weights\\n1W.\\n2\\nThe weight scaling rule is also exact in other settings, including regression\\nnetworks with conditionally normal outputs, and deep networks that have hidden\\nlayers without nonlinearities. However, the weight scaling rule is only an approxi-\\nmation for deep models that have nonlinearities. Though the approximation has'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='not been theoretically characterized, it often works well, empirically. Goodfellow\\net al. (2013a) found experimentally that the weight scaling approximation can work\\nbetter (in terms of classification accuracy) than Monte Carlo approximations to the\\nensemble predictor. This held true even when the Monte Carlo approximation was\\nallowed to sample up to 1,000 sub-networks. Gal and Ghahramani (2015) found\\nthat some models obtain better classification accuracy using twenty samples and\\n264\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nthe Monte Carlo approximation. It appears that the optimal choice of inference\\napproximation is problem-dependent.\\nSrivastava et al. (2014) showed that dropout is more effective than other\\nstandard computationally inexpensive regularizers, such as weight decay, filter\\nnorm constraints and sparse activity regularization. Dropout may also be combined\\nwith other forms of regularization to yield a further improvement.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='One advantage of dropout is that it is very computationally cheap. Using\\ndropout during training requires only O(n) computation per example per update,\\nto generate n random binary numbers and multiply them by the state. Depending\\non the implementation, it may also require O(n) memory to store these binary\\nnumbers until the back-propagation stage. Running inference in the trained model\\nhas the same cost per-example as if dropout were not used, though we must pay\\nthe cost of dividing the weights by 2 once before beginning to run inference on\\nexamples.\\nAnother significant advantage of dropout is that it does not significantly limit\\nthe type of model or training procedure that can be used. It works well with nearly\\nany model that uses a distributed representation and can be trained with stochastic\\ngradient descent. This includes feedforward neural networks, probabilistic models\\nsuch as restricted Boltzmann machines (Srivastava et al., 2014), and recurrent'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='neural networks (Bayer and Osendorfer, 2014; Pascanu et al., 2014a). Many other\\nregularization strategies of comparable power impose more severe restrictions on\\nthe architecture of the model.\\nThough the cost per-step of applying dropout to a specific model is negligible,\\nthe cost of using dropout in a complete system can be significant. Because dropout\\nis a regularization technique, it reduces the effective capacity of a model. To offset\\nthis effect, we must increase the size of the model. Typically the optimal validation\\nset error is much lower when using dropout, but this comes at the cost of a much\\nlarger model and many more iterations of the training algorithm. For very large\\ndatasets, regularization confers little reduction in generalization error. In these\\ncases, the computational cost of using dropout and larger models may outweigh\\nthe benefit of regularization.\\nWhen extremely few labeled training examples are available, dropout is less'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='effective. Bayesian neural networks (Neal, 1996) outperform dropout on the\\nAlternative Splicing Dataset (Xiong et al., 2011) where fewer than 5,000 examples\\nare available (Srivastava et al., 2014). When additional unlabeled data is available,\\nunsupervised feature learning can gain an advantage over dropout.\\nWager et al. (2013) showed that, when applied to linear regression, dropout\\nis equivalent to L2 weight decay, with a different weight decay coefficient for\\n265\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\neach input feature. The magnitude of each feature’s weight decay coefficient is\\ndetermined by its variance. Similar results hold for other linear models. For deep\\nmodels, dropout is not equivalent to weight decay.\\nThe stochasticity used while training with dropout is not necessary for the\\napproach’s success. It is just a means of approximating the sum over all sub-\\nmodels. Wang and Manning (2013) derived analytical approximations to this'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='marginalization. Their approximation, known as fast dropout resulted in faster\\nconvergence time due to the reduced stochasticity in the computation of the\\ngradient. This method can also be applied at test time, as a more principled\\n(but also more computationally expensive) approximation to the average over all\\nsub-networks than the weight scaling approximation. Fast dropout has been used\\nto nearly match the performance of standard dropout on small neural network\\nproblems, but has not yet yielded a significant improvement or been applied to a\\nlarge problem.\\nJust as stochasticity is not necessary to achieve the regularizing effect of\\ndropout, it is also not sufficient. To demonstrate this, Warde-Farley et al. (2014)\\ndesigned control experiments using a method called dropout boosting that they\\ndesigned to use exactly the same mask noise as traditional dropout but lack\\nits regularizing effect. Dropout boosting trains the entire ensemble to jointly'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='maximize the log-likelihood on the training set. In the same sense that traditional\\ndropout is analogous to bagging, this approach is analogous to boosting. As\\nintended, experiments with dropout boosting show almost no regularization effect\\ncompared to training the entire network as a single model. This demonstrates that\\nthe interpretation of dropout as bagging has value beyond the interpretation of\\ndropout as robustness to noise. The regularization effect of the bagged ensemble is\\nonly achieved when the stochastically sampled ensemble members are trained to\\nperform well independently of each other.\\nDropout has inspired other stochastic approaches to training exponentially\\nlarge ensembles of models that share weights. DropConnect is a special case of\\ndropout where each product between a single scalar weight and a single hidden\\nunit state is considered a unit that can be dropped (Wan et al., 2013). Stochastic'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='pooling is a form of randomized pooling (see section 9.3) for building ensembles\\nof convolutional networks with each convolutional network attending to different\\nspatial locations of each feature map. So far, dropout remains the most widely\\nused implicit ensemble method.\\nOne of the key insights of dropout is that training a network with stochastic\\nbehavior and making predictions by averaging over multiple stochastic decisions\\nimplements a form of bagging with parameter sharing. Earlier, we described\\n266\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\ndropout as bagging an ensemble of models formed by including or excluding\\nunits. However, there is no need for this model averaging strategy to be based on\\ninclusion and exclusion. In principle, any kind of random modification is admissible.\\nIn practice, we must choose modification families that neural networks are able\\nto learn to resist. Ideally, we should also use model families that allow a fast'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='approximate inference rule. We can think of any form of modification parametrized\\nby a vector µ as training an ensemble consisting of p(y x,µ) for all possible\\n|\\nvalues of µ. There is no requirement that µ have a finite number of values. For\\nexample, µcan be real-valued. Srivastava et al. (2014) showed that multiplying the\\nweights by µ (1,I) can outperform dropout based on binary masks. Because\\n∼ N\\nE\\n[µ] =1 the standard network automatically implements approximate inference\\nin the ensemble, without needing any weight scaling.\\nSo far we have described dropout purely as a means of performing efficient,\\napproximate bagging. However, there is another view of dropout that goes further\\nthan this. Dropout trains not just a bagged ensemble of models, but an ensemble\\nof models that share hidden units. This means each hidden unit must be able to\\nperform well regardless of which other hidden units are in the model. Hidden units'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='must be prepared to be swapped and interchanged between models. Hinton et al.\\n(2012c) were inspired by an idea from biology: sexual reproduction, which involves\\nswapping genes between two different organisms, creates evolutionary pressure for\\ngenes to become not just good, but to become readily swapped between different\\norganisms. Such genes and such features are very robust to changes in their\\nenvironment because they are not able to incorrectly adapt to unusual features\\nof any one organism or model. Dropout thus regularizes each hidden unit to be\\nnot merely a good feature but a feature that is good in many contexts. Warde-\\nFarley et al. (2014) compared dropout training to training of large ensembles and\\nconcluded that dropout offers additional improvements to generalization error\\nbeyond those obtained by ensembles of independent models.\\nIt is important to understand that a large portion of the power of dropout'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='arises from the fact that the masking noise is applied to the hidden units. This\\ncan be seen as a form of highly intelligent, adaptive destruction of the information\\ncontent of the input rather than destruction of the raw values of the input. For\\nexample, if the model learns a hidden unit h that detects a face by finding the nose,\\ni\\nthen dropping h corresponds to erasing the information that there is a nose in\\ni\\nthe image. The model must learn another h , either that redundantly encodes the\\ni\\npresence of a nose, or that detects the face by another feature, such as the mouth.\\nTraditional noise injection techniques that add unstructured noise at the input are\\nnot able to randomly erase the information about a nose from an image of a face\\nunless the magnitude of the noise is so great that nearly all of the information in\\n267\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nthe image is removed. Destroying extracted features rather than original values'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='allows the destruction process to make use of all of the knowledge about the input\\ndistribution that the model has acquired so far.\\nAnother important aspect of dropout is that the noise is multiplicative. If the\\nnoise were additive with fixed scale, then a rectified linear hidden unit h with\\ni\\nadded noise \\ue00fcould simply learn to have h become very large in order to make\\ni\\nthe added noise \\ue00f insignificant by comparison. Multiplicative noise does not allow\\nsuch a pathological solution to the noise robustness problem.\\nAnotherdeeplearningalgorithm, batchnormalization,reparametrizesthemodel\\nin a way that introduces both additive and multiplicative noise on the hidden\\nunits at training time. The primary purpose of batch normalization is to improve\\noptimization, but the noise can have a regularizing effect, and sometimes makes\\ndropout unnecessary. Batch normalization is described further in section 8.7.1.\\n7.13 Adversarial Training'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In many cases, neural networks have begun to reach human performance when\\nevaluated on an i.i.d. test set. It is natural therefore to wonder whether these\\nmodels have obtained a true human-level understanding of these tasks. In order\\nto probe the level of understanding a network has of the underlying task, we can\\nsearch for examples that the model misclassifies. Szegedy et al. (2014b) found that\\neven neural networks that perform at human level accuracy have a nearly 100%\\nerror rate on examples that are intentionally constructed by using an optimization\\nprocedure to search for an input x near a data point x such that the model\\n\\ue030\\noutput is very different at x . In many cases, x can be so similar to x that a\\n\\ue030 \\ue030\\nhuman observer cannot tell the difference between the original example and the\\nadversarial example, but the network can make highly different predictions. See\\nfigure 7.8 for an example.\\nAdversarialexamples havemanyimplications, for example, in computersecurity,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='that are beyond the scope of this chapter. However, they are interesting in the\\ncontext of regularization because one can reduce the error rate on the original i.i.d.\\ntest set via adversarial training—training on adversarially perturbed examples\\nfrom the training set (Szegedy et al., 2014b; Goodfellow et al., 2014b).\\nGoodfellow et al. (2014b) showed that one of the primary causes of these\\nadversarial examples is excessive linearity. Neural networks are built out of\\nprimarily linear building blocks. In some experiments the overall function they\\nimplement proves to be highly linear as a result. These linear functions are easy\\n268\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\n+ .007 =\\n×\\nx+\\nx sign( J(θ,x,y))\\nx\\n∇ \\ue00fsign( J(θ,x,y))\\nx\\n∇\\ny =“panda” “nematode” “gibbon”\\nw/ 57.7% w/ 8.2% w/ 99.3 %\\nconfidence confidence confidence\\nFigure 7.8: A demonstration of adversarial example generation applied to GoogLeNet\\n(Szegedy et al., 2014a) on ImageNet. By adding an imperceptibly small vector whose'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='elements are equal to the sign of the elements of the gradient of the cost function with\\nrespect to the input, we can change GoogLeNet’s classification of the image. Reproduced\\nwith permission from Goodfellow et al. (2014b).\\nto optimize. Unfortunately, the value of a linear function can change very rapidly\\nif it has numerous inputs. If we change each input by \\ue00f, then a linear function\\nwith weights w can change by as much as \\ue00f w , which can be a very large\\n1\\n|| ||\\namount if w is high-dimensional. Adversarial training discourages this highly\\nsensitive locally linear behavior by encouraging the network to be locally constant\\nin the neighborhood of the training data. This can be seen as a way of explicitly\\nintroducing a local constancy prior into supervised neural nets.\\nAdversarial training helps to illustrate the power of using a large function\\nfamily in combination with aggressive regularization. Purely linear models, like'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='logistic regression, are not able to resist adversarial examples because they are\\nforced to be linear. Neural networks are able to represent functions that can range\\nfrom nearly linear to nearly locally constant and thus have the flexibility to capture\\nlinear trends in the training data while still learning to resist local perturbation.\\nAdversarial examples also provide a means of accomplishing semi-supervised\\nlearning. At a point x that is not associated with a label in the dataset, the\\nmodel itself assigns some label yˆ. The model’s label yˆ may not be the true label,\\nbut if the model is high quality, then yˆ has a high probability of providing the\\ntrue label. We can seek an adversarial example x that causes the classifier to\\n\\ue030\\noutput a label y with y = yˆ. Adversarial examples generated using not the true\\n\\ue030 \\ue030\\n\\ue036\\nlabel but a label provided by a trained model are called virtual adversarial\\nexamples (Miyato et al., 2015). The classifier may then be trained to assign the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='same label to x and x . This encourages the classifier to learn a function that is\\n\\ue030\\n269\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nrobust to small changes anywhere along the manifold where the unlabeled data\\nlies. The assumption motivating this approach is that different classes usually lie\\non disconnected manifolds, and a small perturbation should not be able to jump\\nfrom one class manifold to another class manifold.\\n7.14 Tangent Distance, Tangent Prop, and Manifold\\nTangent Classifier\\nMany machine learning algorithms aim to overcome the curse of dimensionality\\nby assuming that the data lies near a low-dimensional manifold, as described in\\nsection 5.11.3.\\nOne of the early attempts to take advantage of the manifold hypothesis is the\\ntangent distance algorithm (Simard et al., 1993, 1998). It is a non-parametric\\nnearest-neighbor algorithm in which the metric used is not the generic Euclidean\\ndistance but one that is derived from knowledge of the manifolds near which'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='probability concentrates. It is assumed that we are trying to classify examples and\\nthat examples on the same manifold share the same category. Since the classifier\\nshould be invariant to the local factors of variation that correspond to movement\\non the manifold, it would make sense to use as nearest-neighbor distance between\\npoints x and x the distance between the manifolds M and M to which they\\n1 2 1 2\\nrespectively belong. Although that may be computationally difficult (it would\\nrequire solving an optimization problem, to find the nearest pair of points on M\\n1\\nand M ), a cheap alternative that makes sense locally is to approximate M by its\\n2 i\\ntangent plane at x and measure the distance between the two tangents, or between\\ni\\na tangent plane and a point. That can be achieved by solving a low-dimensional\\nlinear system (in the dimension of the manifolds). Of course, this algorithm requires\\none to specify the tangent vectors.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Ina related spirit, thetangent prop algorithm(Simard et al.,1992) (figure7.9)\\ntrains a neural net classifier with an extra penalty to make each output f(x) of\\nthe neural net locally invariant to known factors of variation. These factors of\\nvariation correspond to movement along the manifold near which examples of the\\nsame class concentrate. Local invariance is achieved by requiring f(x) to be\\nx\\n∇\\northogonal to the known manifold tangent vectors v(i) at x, or equivalently that\\nthe directional derivative of f at x in the directions v(i) be small by adding a\\nregularization penalty Ω:\\n2\\nΩ(f) = ( f(x)) v(i) . (7.67)\\nx \\ue03e\\n∇\\ni\\n270\\n\\ue058\\ue010 \\ue011\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nThis regularizer can of course be scaled by an appropriate hyperparameter, and, for\\nmostneuralnetworks, wewouldneed tosumover manyoutputs ratherthanthe lone\\noutput f(x) described here for simplicity. As with the tangent distance algorithm,\\nthe tangent vectors are derived a priori, usually from the formal knowledge of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the effect of transformations such as translation, rotation, and scaling in images.\\nTangent prop has been used not just for supervised learning (Simard et al., 1992)\\nbut also in the context of reinforcement learning (Thrun, 1995).\\nTangent propagation is closely related to dataset augmentation. In both\\ncases, the user of the algorithm encodes his or her prior knowledge of the task\\nby specifying a set of transformations that should not alter the output of the\\nnetwork. The difference is that in the case of dataset augmentation, the network is\\nexplicitly trained to correctly classify distinct inputs that were created by applying\\nmore than an infinitesimal amount of these transformations. Tangent propagation\\ndoes not require explicitly visiting a new input point. Instead, it analytically\\nregularizes the model to resist perturbation in the directions corresponding to\\nthe specified transformation. While this analytical approach is intellectually'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='elegant, it has two major drawbacks. First, it only regularizes the model to resist\\ninfinitesimal perturbation. Explicit dataset augmentation confers resistance to\\nlargerperturbations. Second, theinfinitesimal approachposesdifficulties formodels\\nbased on rectified linear units. These models can only shrink their derivatives\\nby turning units off or shrinking their weights. They are not able to shrink their\\nderivatives by saturating at a high value with large weights, as sigmoid or tanh\\nunits can. Dataset augmentation works well with rectified linear units because\\ndifferent subsets of rectified units can activate for different transformed versions of\\neach original input.\\nTangent propagation is also related to double backprop(Drucker and LeCun,\\n1992) and adversarial training (Szegedy et al., 2014b; Goodfellow et al., 2014b).\\nDouble backprop regularizes the Jacobian to be small, while adversarial training\\nfinds inputs near the original inputs and trains the model to produce the same'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='output on these as on the original inputs. Tangent propagation and dataset\\naugmentation using manually specified transformations both require that the\\nmodel should be invariant to certain specified directions of change in the input.\\nDouble backprop and adversarial training both require that the model should be\\ninvariant to all directions of change in the input so long as the change is small. Just\\nas dataset augmentation is the non-infinitesimal version of tangent propagation,\\nadversarial training is the non-infinitesimal version of double backprop.\\nThe manifold tangent classifier (Rifai et al., 2011c), eliminates the need to\\nknow the tangent vectors a priori. As we will see in chapter 14, autoencoders can\\n271\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nx\\n1\\n2x\\nNormal\\nTangent\\nFigure 7.9: Illustration of the main idea of the tangent prop algorithm (Simard et al.,\\n1992) and manifold tangent classifier (Rifai et al., 2011c), which both regularize the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='classifier output function f(x). Each curve represents the manifold for a different class,\\nillustrated here as a one-dimensional manifold embedded in a two-dimensional space.\\nOn one curve, we have chosen a single point and drawn a vector that is tangent to the\\nclass manifold (parallel to and touching the manifold) and a vector that is normal to the\\nclass manifold (orthogonal to the manifold). In multiple dimensions there may be many\\ntangent directions and many normal directions. We expect the classification function to\\nchange rapidly as it moves in the direction normal to the manifold, and not to change as\\nit moves along the class manifold. Both tangent propagation and the manifold tangent\\nclassifier regularize f(x) to not change very much as x moves along the manifold. Tangent\\npropagation requires the user to manually specify functions that compute the tangent\\ndirections (such as specifying that small translations of images remain in the same class'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='manifold) while the manifold tangent classifier estimates the manifold tangent directions\\nby training an autoencoder to fit the training data. The use of autoencoders to estimate\\nmanifolds will be described in chapter 14.\\nestimate the manifold tangent vectors. The manifold tangent classifier makes use\\nof this technique to avoid needing user-specified tangent vectors. As illustrated\\nin figure 14.10, these estimated tangent vectors go beyond the classical invariants\\nthat arise out of the geometry of images (such as translation, rotation and scaling)\\nand include factors that must be learned because they are object-specific (such as\\nmoving body parts). The algorithm proposed with the manifold tangent classifier\\nis therefore simple: (1) use an autoencoder to learn the manifold structure by\\nunsupervised learning, and (2) use these tangents to regularize a neuralnet classifier\\nas in tangent prop (equation 7.67).\\nThis chapter has described most of the general strategies used to regularize'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='neural networks. Regularization is a central theme of machine learning and as such\\n272\\nCHAPTER 7. REGULARIZATION FOR DEEP LEARNING\\nwill be revisited periodically by most of the remaining chapters. Another central\\ntheme of machine learning is optimization, described next.\\n273\\nChapter 8\\nOptimization for Training Deep\\nModels\\nDeep learning algorithms involve optimization in many contexts. For example,\\nperforming inference in models such as PCA involves solving an optimization\\nproblem. We often use analytical optimization to write proofs or design algorithms.\\nOf all of the many optimization problems involved in deep learning, the most\\ndifficult is neural network training. It is quite common to invest days to months of\\ntime on hundreds of machines in order to solve even a single instance of the neural\\nnetwork training problem. Because this problem is so important and so expensive,\\na specialized set of optimization techniques have been developed for solving it.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='This chapter presents these optimization techniques for neural network training.\\nIf you are unfamiliar with the basic principles of gradient-based optimization,\\nwe suggest reviewing chapter 4. That chapter includes a brief overview of numerical\\noptimization in general.\\nThis chapter focuses on one particular case of optimization: finding the param-\\neters θ of a neural network that significantly reduce a cost function J(θ), which\\ntypically includes a performance measure evaluated on the entire training set as\\nwell as additional regularization terms.\\nWe begin with a description of how optimization used as a training algorithm\\nfor a machine learning task differs from pure optimization. Next, we present several\\nof the concrete challenges that make optimization of neural networks difficult. We\\nthen define several practical algorithms, including both optimization algorithms\\nthemselves and strategies for initializing the parameters. More advanced algorithms'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='adapt their learning rates during training or leverage information contained in\\n274\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nthe second derivatives of the cost function. Finally, we conclude with a review of\\nseveral optimization strategies that are formed by combining simple optimization\\nalgorithms into higher-level procedures.\\n8.1 How Learning Differs from Pure Optimization\\nOptimization algorithms used for training of deep models differ from traditional\\noptimization algorithms in several ways. Machine learning usually acts indirectly.\\nIn most machine learning scenarios, we care about some performance measure\\nP, that is defined with respect to the test set and may also be intractable. We\\ntherefore optimize P only indirectly. We reduce a different cost function J(θ) in\\nthe hope that doing so will improve P. This is in contrast to pure optimization,\\nwhere minimizing J is a goal in and of itself. Optimization algorithms for training'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='deep models also typically include some specialization on the specific structure of\\nmachine learning objective functions.\\nTypically, the cost function can be written as an average over the training set,\\nsuch as\\nE\\nJ(θ) = L(f(x;θ),y), (8.1)\\n(x,y) pˆ\\ndata\\n∼\\nwhere L is the per-example loss function, f(x;θ) is the predicted output when\\nthe input is x, pˆ is the empirical distribution. In the supervised learning case,\\ndata\\ny is the target output. Throughout this chapter, we develop the unregularized\\nsupervised case, where the arguments to L are f(x;θ) and y. However, it is trivial\\nto extend this development, for example, to include θ or x as arguments, or to\\nexclude y as arguments, in order to develop various forms of regularization or\\nunsupervised learning.\\nEquation 8.1 defines an objective function with respect to the training set. We\\nwould usually prefer to minimize the corresponding objective function where the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='expectation is taken across the data generating distribution p rather than just\\ndata\\nover the finite training set:\\nE\\nJ (θ) = L(f(x;θ),y). (8.2)\\n∗ (x,y) p\\ndata\\n∼\\n8.1.1 Empirical Risk Minimization\\nThe goal of a machine learning algorithm is to reduce the expected generalization\\nerror given by equation 8.2. This quantity is known as the risk. We emphasize here\\nthat the expectation is taken over the true underlying distribution p . If we knew\\ndata\\nthe true distribution p (x,y), risk minimization would be an optimization task\\ndata\\n275\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nsolvable by an optimization algorithm. However, when we do not know p (x,y)\\ndata\\nbut only have a training set of samples, we have a machine learning problem.\\nThe simplest way to convert a machine learning problem back into an op-\\ntimization problem is to minimize the expected loss on the training set. This\\nmeans replacing the true distribution p(x,y) with the empirical distribution pˆ(x,y)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='defined by the training set. We now minimize the empirical risk\\nm\\n1\\nE [L(f(x;θ),y)] = L(f(x(i);θ),y(i)) (8.3)\\nx,y pˆdata(x,y)\\nm\\n∼\\ni=1\\n\\ue058\\nwhere m is the number of training examples.\\nThe training process based on minimizing this average training error is known\\nas empirical risk minimization. In this setting, machine learning is still very\\nsimilar to straightforward optimization. Rather than optimizing the risk directly,\\nwe optimize the empirical risk, and hope that the risk decreases significantly as\\nwell. A variety of theoretical results establish conditions under which the true risk\\ncan be expected to decrease by various amounts.\\nHowever, empirical risk minimization is prone to overfitting. Models with\\nhigh capacity can simply memorize the training set. In many cases, empirical\\nrisk minimization is not really feasible. The most effective modern optimization\\nalgorithms are based on gradient descent, but many useful loss functions, such'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='as 0-1 loss, have no useful derivatives (the derivative is either zero or undefined\\neverywhere). These two problems mean that, in the context of deep learning, we\\nrarely use empirical risk minimization. Instead, we must use a slightly different\\napproach, in which the quantity that we actually optimize is even more different\\nfrom the quantity that we truly want to optimize.\\n8.1.2 Surrogate Loss Functions and Early Stopping\\nSometimes, the loss function we actually care about (say classification error) is not\\none that can be optimized efficiently. For example, exactly minimizing expected 0-1\\nloss is typically intractable (exponential in the input dimension), even for a linear\\nclassifier (Marcotte and Savard, 1992). In such situations, one typically optimizes\\na surrogate loss function instead, which acts as a proxy but has advantages.\\nFor example, the negative log-likelihood of the correct class is typically used as a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='surrogate for the 0-1 loss. The negative log-likelihood allows the model to estimate\\nthe conditional probability of the classes, given the input, and if the model can\\ndo that well, then it can pick the classes that yield the least classification error in\\nexpectation.\\n276\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nIn some cases, a surrogate loss function actually results in being able to learn\\nmore. For example, the test set 0-1 loss often continues to decrease for a long\\ntime after the training set 0-1 loss has reached zero, when training using the\\nlog-likelihood surrogate. This is because even when the expected 0-1 loss is zero,\\none can improve the robustness of the classifier by further pushing the classes apart\\nfrom each other, obtaining a more confident and reliable classifier, thus extracting\\nmore information from the training data than would have been possible by simply\\nminimizing the average 0-1 loss on the training set.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='A very important difference between optimization in general and optimization\\nas we use it for training algorithms is that training algorithms do not usually halt\\nat a local minimum. Instead, a machine learning algorithm usually minimizes\\na surrogate loss function but halts when a convergence criterion based on early\\nstopping (section 7.8) is satisfied. Typically the early stopping criterion is based\\non the true underlying loss function, such as 0-1 loss measured on a validation set,\\nand is designed to cause the algorithm to halt whenever overfitting begins to occur.\\nTraining often halts while the surrogate loss function still has large derivatives,\\nwhich is very different from the pure optimization setting, where an optimization\\nalgorithm is considered to have converged when the gradient becomes very small.\\n8.1.3 Batch and Minibatch Algorithms\\nOne aspect of machine learning algorithms that separates them from general'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='optimization algorithms is that the objective function usually decomposes as a sum\\nover the training examples. Optimization algorithms for machine learning typically\\ncompute each update to the parameters based on an expected value of the cost\\nfunction estimated using only a subset of the terms of the full cost function.\\nFor example, maximum likelihood estimation problems, when viewed in log\\nspace, decompose into a sum over each example:\\nm\\nθ = argmax logp (x(i),y(i);θ). (8.4)\\nML model\\nθ\\ni=1\\n\\ue058\\nMaximizing this sum is equivalent to maximizing the expectation over the\\nempirical distribution defined by the training set:\\nE\\nJ(θ) = logp (x,y;θ). (8.5)\\nx,y pˆ\\ndata\\nmodel\\n∼\\nMost of the properties of the objective function J used by most of our opti-\\nmization algorithms are also expectations over the training set. For example, the\\n277\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nmost commonly used property is the gradient:\\nE\\nJ(θ) = logp (x,y;θ). (8.6)\\n∇θ x,y ∼pˆdata∇θ model'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Computing this expectation exactly is very expensive because it requires\\nevaluating the model on every example in the entire dataset. In practice, we can\\ncompute these expectations by randomly sampling a small number of examples\\nfrom the dataset, then taking the average over only those examples.\\nRecall that the standard error of the mean (equation 5.46) estimated from n\\nsamples is given by σ/√n, where σ is the true standard deviation of the value of\\nthe samples. The denominator of √n shows that there are less than linear returns\\nto using more examples to estimate the gradient. Compare two hypothetical\\nestimates of the gradient, one based on 100 examples and another based on 10,000\\nexamples. The latter requires 100 times more computation than the former, but\\nreduces the standard error of the mean only by a factor of 10. Most optimization\\nalgorithms converge much faster (in terms of total computation, not in terms of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='number of updates) if they are allowed to rapidly compute approximate estimates\\nof the gradient rather than slowly computing the exact gradient.\\nAnother consideration motivating statistical estimation of the gradient from a\\nsmall number of samples is redundancy in the training set. In the worst case, all\\nm samples in the training set could be identical copies of each other. A sampling-\\nbased estimate of the gradient could compute the correct gradient with a single\\nsample, using m times less computation than the naive approach. In practice, we\\nare unlikely to truly encounter this worst-case situation, but we may find large\\nnumbers of examples that all make very similar contributions to the gradient.\\nOptimization algorithms that use the entire training set are called batch or\\ndeterministic gradient methods, because they process all of the training examples\\nsimultaneously in a large batch. This terminology can be somewhat confusing'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='because the word “batch” is also often used to describe the minibatch used by\\nminibatch stochastic gradient descent. Typically the term “batch gradient descent”\\nimplies the use of the full training set, while the use of the term “batch” to describe\\na group of examples does not. For example, it is very common to use the term\\n“batch size” to describe the size of a minibatch.\\nOptimization algorithms that use only a single example at a time are sometimes\\ncalled stochastic or sometimes online methods. The term online is usually\\nreserved for the case where the examples are drawn from a stream of continually\\ncreated examples rather than from a fixed-size training set over which several\\npasses are made.\\nMost algorithms used for deep learning fall somewhere in between, using more\\n278\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nthan one but less than all of the training examples. These were traditionally called\\nminibatch or minibatch stochastic methods and it is now common to simply'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='call them stochastic methods.\\nThe canonical example of a stochastic method is stochastic gradient descent,\\npresented in detail in section 8.3.1.\\nMinibatch sizes are generally driven by the following factors:\\nLarger batches provide a more accurate estimate of the gradient, but with\\n•\\nless than linear returns.\\nMulticore architectures are usually underutilized by extremely small batches.\\n•\\nThis motivates using some absolute minimum batch size, below which there\\nis no reduction in the time to process a minibatch.\\nIf all examples in the batch are to be processed in parallel (as is typically\\n•\\nthe case), then the amount of memory scales with the batch size. For many\\nhardware setups this is the limiting factor in batch size.\\nSome kinds of hardware achieve better runtime with specific sizes of arrays.\\n•\\nEspecially when using GPUs, it is common for power of 2 batch sizes to offer\\nbetter runtime. Typical power of 2 batch sizes range from 32 to 256, with 16'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='sometimes being attempted for large models.\\nSmall batches can offer a regularizing effect (Wilson and Martinez, 2003),\\n•\\nperhaps due to the noise they add to the learning process. Generalization\\nerror is often best for a batch size of 1. Training with such a small batch\\nsize might require a small learning rate to maintain stability due to the high\\nvariance in the estimate of the gradient. The total runtime can be very high\\ndue to the need to make more steps, both because of the reduced learning\\nrate and because it takes more steps to observe the entire training set.\\nDifferent kinds of algorithms use different kinds of information from the mini-\\nbatch in different ways. Some algorithms are more sensitive to sampling error than\\nothers, either because they use information that is difficult to estimate accurately\\nwith few samples, or because they use information in ways that amplify sampling\\nerrors more. Methods that compute updates based only on the gradient g are'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='usually relatively robust and can handle smaller batch sizes like 100. Second-order\\nmethods, which use also the Hessian matrix H and compute updates such as\\nH 1g, typically require much larger batch sizes like 10,000. These large batch\\n−\\nsizes are required to minimize fluctuations in the estimates of H 1g. Suppose\\n−\\nthat H is estimated perfectly but has a poor condition number. Multiplication by\\n279\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nH or its inverse amplifies pre-existing errors, in this case, estimation errors in g.\\nVery small changes in the estimate of g can thus cause large changes in the update\\nH 1g, even if H were estimated perfectly. Of course, H will be estimated only\\n−\\napproximately, so the update H 1g will contain even more error than we would\\n−\\npredict from applying a poorly conditioned operation to the estimate of g.\\nIt is also crucial that the minibatches be selected randomly. Computing an'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='unbiased estimate of the expected gradient from a set of samples requires that those\\nsamples be independent. We also wish for two subsequent gradient estimates to be\\nindependent from each other, so two subsequent minibatches of examples should\\nalso be independent from each other. Many datasets are most naturally arranged\\nin a way where successive examples are highly correlated. For example, we might\\nhave a dataset of medical data with a long list of blood sample test results. This\\nlist might be arranged so that first we have five blood samples taken at different\\ntimes from the first patient, then we have three blood samples taken from the\\nsecond patient, then the blood samples from the third patient, and so on. If we\\nwere to draw examples in order from this list, then each of our minibatches would\\nbe extremely biased, because it would represent primarily one patient out of the\\nmany patients in the dataset. In cases such as these where the order of the dataset'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='holds some significance, it is necessary to shuffle the examples before selecting\\nminibatches. For very large datasets, for example datasets containing billions of\\nexamples in a data center, it can be impractical to sample examples truly uniformly\\nat random every time we want to construct a minibatch. Fortunately, in practice\\nit is usually sufficient to shuffle the order of the dataset once and then store it in\\nshuffled fashion. This will impose a fixed set of possible minibatches of consecutive\\nexamples that all models trained thereafter will use, and each individual model\\nwill be forced to reuse this ordering every time it passes through the training\\ndata. However, this deviation from true random selection does not seem to have a\\nsignificant detrimental effect. Failing to ever shuffle the examples in any way can\\nseriously reduce the effectiveness of the algorithm.\\nMany optimization problems in machine learning decompose over examples'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='well enough that we can compute entire separate updates over different examples\\nin parallel. In other words, we can compute the update that minimizes J(X) for\\none minibatch of examples X at the same time that we compute the update for\\nseveral other minibatches. Such asynchronous parallel distributed approaches are\\ndiscussed further in section 12.1.3.\\nAn interesting motivation for minibatch stochastic gradient descent is that it\\nfollows the gradient of the true generalization error (equation 8.2) so long as no\\nexamples are repeated. Most implementations of minibatch stochastic gradient\\n280\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\ndescent shuffle the dataset once and then pass through it multiple times. On the\\nfirst pass, each minibatch is used to compute an unbiased estimate of the true\\ngeneralization error. On the second pass, the estimate becomes biased because it is\\nformed by re-sampling values that have already been used, rather than obtaining'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='new fair samples from the data generating distribution.\\nThe fact that stochastic gradient descent minimizes generalization error is\\neasiest to see in the online learning case, where examples or minibatches are drawn\\nfrom a stream of data. In other words, instead of receiving a fixed-size training\\nset, the learner is similar to a living being who sees a new example at each instant,\\nwith every example (x,y) coming from the data generating distribution p (x,y).\\ndata\\nIn this scenario, examples are never repeated; every experience is a fair sample\\nfrom p .\\ndata\\nThe equivalence is easiest to derive when both x and y are discrete. In this\\ncase, the generalization error (equation 8.2) can be written as a sum\\nJ (θ) = p (x,y)L(f(x;θ),y), (8.7)\\n∗ data\\nx y\\n\\ue058\\ue058\\nwith the exact gradient\\ng = J (θ) = p (x,y) L(f(x;θ),y). (8.8)\\nθ ∗ data θ\\n∇ ∇\\nx y\\n\\ue058\\ue058\\nWe have already seen the same fact demonstrated for the log-likelihood in equa-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='tion 8.5 and equation 8.6; we observe now that this holds for other functions L\\nbesides the likelihood. A similar result can be derived when x and y are continuous,\\nunder mild assumptions regarding p and L.\\ndata\\nHence, we can obtain an unbiased estimator of the exact gradient of the\\ngeneralization error by sampling a minibatch of examples x(1),...x(m) with cor-\\n{ }\\nresponding targets y(i) from the data generating distribution p , and computing\\ndata\\nthe gradient of the loss with respect to the parameters for that minibatch:\\n1\\nˆg = L(f(x(i);θ),y(i)). (8.9)\\nθ\\nm∇\\ni\\n\\ue058\\nUpdating θ in the direction of gˆ performs SGD on the generalization error.\\nOf course, this interpretation only applies when examples are not reused.\\nNonetheless, it is usually best to make several passes through the training set,\\nunless the training set is extremely large. When multiple such epochs are used,\\nonly the first epoch follows the unbiased gradient of the generalization error, but\\n281'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nof course, the additional epochs usually provide enough benefit due to decreased\\ntraining error to offset the harm they cause by increasing the gap between training\\nerror and test error.\\nWith some datasets growing rapidly in size, faster than computing power, it\\nis becoming more common for machine learning applications to use each training\\nexample only once or even to make an incomplete pass through the training\\nset. When using an extremely large training set, overfitting is not an issue, so\\nunderfitting and computational efficiency become the predominant concerns. See\\nalso Bottou and Bousquet (2008) for a discussion of the effect of computational\\nbottlenecks on generalization error, as the number of training examples grows.\\n8.2 Challenges in Neural Network Optimization\\nOptimization in general is an extremely difficult task. Traditionally, machine\\nlearning has avoided the difficulty of general optimization by carefully designing'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the objective function and constraints to ensure that the optimization problem is\\nconvex. When training neural networks, we must confront the general non-convex\\ncase. Even convex optimization is not without its complications. In this section,\\nwe summarize several of the most prominent challenges involved in optimization\\nfor training deep models.\\n8.2.1 Ill-Conditioning\\nSome challenges arise even when optimizing convex functions. Of these, the most\\nprominent is ill-conditioning of the Hessian matrix H. This is a very general\\nproblem in most numerical optimization, convex or otherwise, and is described in\\nmore detail in section 4.3.1.\\nThe ill-conditioning problem is generally believed to be present in neural\\nnetwork training problems. Ill-conditioning can manifest by causing SGD to get\\n“stuck” in the sense that even very small steps increase the cost function.\\nRecall from equation 4.9 that a second-order Taylor series expansion of the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='cost function predicts that a gradient descent step of \\ue00fg will add\\n−\\n1\\n\\ue00f2g Hg \\ue00fg g (8.10)\\n\\ue03e \\ue03e\\n2 −\\nto the cost. Ill-conditioning of the gradient becomes a problem when 1\\ue00f2g Hg\\n2 \\ue03e\\nexceeds \\ue00fg g. To determine whether ill-conditioning is detrimental to a neural\\n\\ue03e\\nnetwork training task, one can monitor the squared gradient norm g g and\\n\\ue03e\\n282\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\n16\\n14\\n12\\n10\\n8\\n6\\n4\\n2\\n0\\n2\\n− 50 0 50 100 150 200 250\\n−\\nTraining time (epochs)\\nmron\\ntneidarG\\n1.0\\n0.9\\n0.8\\n0.7\\n0.6\\n0.5\\n0.4\\n0.3\\n0.2\\n0.1\\n0 50 100 150 200 250\\nTraining time (epochs)\\netar\\nrorre\\nnoitacfiissalC\\nFigure 8.1: Gradient descent often does not arrive at a critical point of any kind. In this\\nexample, the gradient norm increases throughout training of a convolutional network used\\nfor object detection. (Left)A scatterplot showing how the norms of individual gradient\\nevaluations are distributed over time. To improve legibility, only one gradient norm'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='is plotted per epoch. The running average of all gradient norms is plotted as a solid\\ncurve. The gradient norm clearly increases over time, rather than decreasing as we would\\nexpect if the training process converged to a critical point. (Right)Despite the increasing\\ngradient, the training process is reasonably successful. The validation set classification\\nerror decreases to a low level.\\nthe g Hg term. In many cases, the gradient norm does not shrink significantly\\n\\ue03e\\nthroughout learning, but the g Hg term grows by more than an order of magnitude.\\n\\ue03e\\nThe result is that learning becomes very slow despite the presence of a strong\\ngradient because the learning rate must be shrunk to compensate for even stronger\\ncurvature. Figure 8.1 shows an example of the gradient increasing significantly\\nduring the successful training of a neural network.\\nThough ill-conditioning is present in other settings besides neural network\\ntraining, some of the techniques used to combat it in other contexts are less'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='applicable to neural networks. For example, Newton’s method is an excellent tool\\nfor minimizing convex functions with poorly conditioned Hessian matrices, but in\\nthe subsequent sections we will argue that Newton’s method requires significant\\nmodification before it can be applied to neural networks.\\n8.2.2 Local Minima\\nOne of the most prominent features of a convex optimization problem is that it\\ncan be reduced to the problem of finding a local minimum. Any local minimum is\\n283\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nguaranteed to be a global minimum. Some convex functions have a flat region at\\nthe bottom rather than a single global minimum point, but any point within such\\na flat region is an acceptable solution. When optimizing a convex function, we\\nknow that we have reached a good solution if we find a critical point of any kind.\\nWith non-convex functions, such as neural nets, it is possible to have many'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='local minima. Indeed, nearly any deep model is essentially guaranteed to have\\nan extremely large number of local minima. However, as we will see, this is not\\nnecessarily a major problem.\\nNeural networks and any models with multiple equivalently parametrized latent\\nvariables all have multiple local minima because of the model identifiability\\nproblem. A model is said to be identifiable if a sufficiently large training set can\\nrule out all but one setting of the model’s parameters. Models with latent variables\\nare often not identifiable because we can obtain equivalent models by exchanging\\nlatent variables with each other. For example, we could take a neural network and\\nmodify layer 1 by swapping the incoming weight vector for unit i with the incoming\\nweight vector for unit j, then doing the same for the outgoing weight vectors. If we\\nhave m layers with nunits each, then there are n!m ways of arranging the hidden\\nunits. This kind of non-identifiability is known as weight space symmetry.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In addition to weight space symmetry, many kinds of neural networks have\\nadditional causes of non-identifiability. For example, in any rectified linear or\\nmaxout network, we can scale all of the incoming weights and biases of a unit by\\nα if we also scale all of its outgoing weights by 1. This means that—if the cost\\nα\\nfunction does not include terms such as weight decay that depend directly on the\\nweights rather than the models’ outputs—every local minimum of a rectified linear\\nor maxout network lies on an (m n)-dimensional hyperbola of equivalent local\\n×\\nminima.\\nThese model identifiability issues mean that there can be an extremely large\\nor even uncountably infinite amount of local minima in a neural network cost\\nfunction. However, all of these local minima arising from non-identifiability are\\nequivalent to each other in cost function value. As a result, these local minima are\\nnot a problematic form of non-convexity.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Local minima can be problematic if they have high cost in comparison to the\\nglobal minimum. One can construct small neural networks, even without hidden\\nunits, that have local minima with higher cost than the global minimum (Sontag\\nand Sussman, 1989; Brady et al., 1989; Gori and Tesi, 1992). If local minima\\nwith high cost are common, this could pose a serious problem for gradient-based\\noptimization algorithms.\\nIt remains an open question whether there are many local minima of high cost\\n284\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nfor networks of practical interest and whether optimization algorithms encounter\\nthem. For many years, most practitioners believed that local minima were a\\ncommon problem plaguing neural network optimization. Today, that does not\\nappear to be the case. The problem remains an active area of research, but experts\\nnow suspect that, for sufficiently large neural networks, most local minima have a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='low cost function value, and that it is not important to find a true global minimum\\nrather than to find a point in parameter space that has low but not minimal cost\\n(Saxe et al., 2013; Dauphin et al., 2014; Goodfellow et al., 2015; Choromanska\\net al., 2014).\\nMany practitioners attribute nearly all difficulty with neural network optimiza-\\ntion to local minima. We encourage practitioners to carefully test for specific\\nproblems. A test that can rule out local minima as the problem is to plot the\\nnorm of the gradient over time. If the norm of the gradient does not shrink to\\ninsignificant size, the problem is neither local minima nor any other kind of critical\\npoint. This kind of negative test can rule out local minima. In high dimensional\\nspaces, it can be very difficult to positively establish that local minima are the\\nproblem. Many structures other than local minima also have small gradients.\\n8.2.3 Plateaus, Saddle Points and Other Flat Regions'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='For many high-dimensional non-convex functions, local minima (and maxima)\\nare in fact rare compared to another kind of point with zero gradient: a saddle\\npoint. Some points around a saddle point have greater cost than the saddle point,\\nwhile others have a lower cost. At a saddle point, the Hessian matrix has both\\npositive and negative eigenvalues. Points lying along eigenvectors associated with\\npositive eigenvalues have greater cost than the saddle point, while points lying\\nalong negative eigenvalues have lower value. We can think of a saddle point as\\nbeing a local minimum along one cross-section of the cost function and a local\\nmaximum along another cross-section. See figure 4.5 for an illustration.\\nMany classes of random functions exhibit the following behavior: in low-\\ndimensional spaces, local minima are common. In higher dimensional spaces, local\\nminima are rare and saddle points are more common. For a function f :\\nRn R\\nof\\n→'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='this type, the expected ratio of the number of saddle points to local minima grows\\nexponentially with n. To understand the intuition behind this behavior, observe\\nthat the Hessian matrix at a local minimum has only positive eigenvalues. The\\nHessian matrix at a saddle point has a mixture of positive and negative eigenvalues.\\nImagine that the sign of each eigenvalue is generated by flipping a coin. In a single\\ndimension, it is easy to obtain a local minimum by tossing a coin and getting heads\\nonce. In n-dimensional space, it is exponentially unlikely that all n coin tosses will\\n285\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nbe heads. See Dauphin et al. (2014) for a review of the relevant theoretical work.\\nAn amazing property of many random functions is that the eigenvalues of the\\nHessian become more likely to be positive as we reach regions of lower cost. In\\nour coin tossing analogy, this means we are more likely to have our coin come up'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='heads n times if we are at a critical point with low cost. This means that local\\nminima are much more likely to have low cost than high cost. Critical points with\\nhigh cost are far more likely to be saddle points. Critical points with extremely\\nhigh cost are more likely to be local maxima.\\nThis happens for many classes of random functions. Does it happen for neural\\nnetworks? Baldi and Hornik (1989) showed theoretically that shallow autoencoders\\n(feedforward networks trained to copy their input to their output, described in\\nchapter 14) with no nonlinearities have global minima and saddle points but no\\nlocal minima with higher cost than the global minimum. They observed without\\nproof that these results extend to deeper networks without nonlinearities. The\\noutput of such networks is a linear function of their input, but they are useful\\nto study as a model of nonlinear neural networks because their loss function is'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a non-convex function of their parameters. Such networks are essentially just\\nmultiple matrices composed together. Saxe et al. (2013) provided exact solutions\\nto the complete learning dynamics in such networks and showed that learning in\\nthese models captures many of the qualitative features observed in the training of\\ndeep models with nonlinear activation functions. Dauphin et al. (2014) showed\\nexperimentally that real neural networks also have loss functions that contain very\\nmany high-cost saddle points. Choromanska et al. (2014) provided additional\\ntheoretical arguments, showing that another class of high-dimensional random\\nfunctions related to neural networks does so as well.\\nWhat are the implications of the proliferation of saddle points for training algo-\\nrithms? For first-order optimization algorithms that use only gradient information,\\nthe situation is unclear. The gradient can often become very small near a saddle'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='point. On the other hand, gradient descent empirically seems to be able to escape\\nsaddle points in many cases. Goodfellow et al. (2015) provided visualizations of\\nseveral learning trajectories of state-of-the-art neural networks, with an example\\ngiven in figure 8.2. These visualizations show a flattening of the cost function near\\na prominent saddle point where the weights are all zero, but they also show the\\ngradient descent trajectory rapidly escaping this region. Goodfellow et al. (2015)\\nalso argue that continuous-time gradient descent may be shown analytically to be\\nrepelled from, rather than attracted to, a nearby saddle point, but the situation\\nmay be different for more realistic uses of gradient descent.\\nFor Newton’s method, it is clear that saddle points constitute a problem.\\n286\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nJ\\n(\\nθ\\n)\\nθ\\nof\\n2\\nProjection1ofθ\\nP r o\\nje c\\ntio n\\nFigure 8.2: A visualization of the cost function of a neural network. Image adapted'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='with permission from Goodfellow et al. (2015). These visualizations appear similar for\\nfeedforward neural networks, convolutional networks, and recurrent networks applied\\nto real object recognition and natural language processing tasks. Surprisingly, these\\nvisualizations usually do not show many conspicuous obstacles. Prior to the success of\\nstochastic gradient descent for training very large models beginning in roughly 2012,\\nneural net cost function surfaces were generally believed to have much more non-convex\\nstructure than is revealed by these projections. The primary obstacle revealed by this\\nprojection is a saddle point of high cost near where the parameters are initialized, but, as\\nindicated by the blue path, the SGD training trajectory escapes this saddle point readily.\\nMost of training time is spent traversing the relatively flat valley of the cost function,\\nwhich may be due to high noise in the gradient, poor conditioning of the Hessian matrix'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='in this region, or simply the need to circumnavigate the tall “mountain” visible in the\\nfigure via an indirect arcing path.\\n287\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nGradient descent is designed to move “downhill” and is not explicitly designed\\nto seek a critical point. Newton’s method, however, is designed to solve for a\\npoint where the gradient is zero. Without appropriate modification, it can jump\\nto a saddle point. The proliferation of saddle points in high dimensional spaces\\npresumably explains why second-order methods have not succeeded in replacing\\ngradient descent for neural network training. Dauphin et al. (2014) introduced a\\nsaddle-free Newton method for second-order optimization and showed that it\\nimproves significantly over the traditional version. Second-order methods remain\\ndifficult to scale to large neural networks, but this saddle-free approach holds\\npromise if it could be scaled.\\nThere are other kinds of points with zero gradient besides minima and saddle'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='points. There are also maxima, which are much like saddle points from the\\nperspective of optimization—many algorithms are not attracted to them, but\\nunmodified Newton’s method is. Maxima of many classes of random functions\\nbecome exponentially rare in high dimensional space, just like minima do.\\nThere may also be wide, flat regions of constant value. In these locations, the\\ngradient and also the Hessian are all zero. Such degenerate locations pose major\\nproblems for all numerical optimization algorithms. In a convex problem, a wide,\\nflat region must consist entirely of global minima, but in a general optimization\\nproblem, such a region could correspond to a high value of the objective function.\\n8.2.4 Cliffs and Exploding Gradients\\nNeural networks with many layers often have extremely steep regions resembling\\ncliffs, as illustrated in figure 8.3. These result from the multiplication of several\\nlarge weights together. On the face of an extremely steep cliff structure, the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='gradient update step can move the parameters extremely far, usually jumping off\\nof the cliff structure altogether.\\n288\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\n\\ue029\\n\\ue062\\n\\ue077\\ue03b\\n\\ue028\\n\\ue04a\\n\\ue077\\n\\ue062\\nFigure 8.3: The objective function for highly nonlinear deep neural networks or for\\nrecurrent neural networks often contains sharp nonlinearities in parameter space resulting\\nfrom the multiplication of several parameters. These nonlinearities give rise to very\\nhigh derivatives in some places. When the parameters get close to such a cliff region, a\\ngradient descent update can catapult the parameters very far, possibly losing most of the\\noptimization work that had been done. Figure adapted with permission from Pascanu\\net al. (2013).\\nThe cliff can be dangerous whether we approach it from above or from below,\\nbut fortunately its most serious consequences can be avoided using the gradient\\nclipping heuristic described in section 10.11.1. The basic idea is to recall that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the gradient does not specify the optimal step size, but only the optimal direction\\nwithin an infinitesimal region. When the traditional gradient descent algorithm\\nproposes to make a very large step, the gradient clipping heuristic intervenes to\\nreduce the step size to be small enough that it is less likely to go outside the region\\nwhere the gradient indicates the direction of approximately steepest descent. Cliff\\nstructures are most common in the cost functions for recurrent neural networks,\\nbecause such models involve a multiplication of many factors, with one factor\\nfor each time step. Long temporal sequences thus incur an extreme amount of\\nmultiplication.\\n8.2.5 Long-Term Dependencies\\nAnother difficulty that neural network optimization algorithms must overcome\\narises when the computational graph becomes extremely deep. Feedforward\\nnetworks with many layers have such deep computational graphs. So do recurrent'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='networks, described in chapter 10, which construct very deep computational graphs\\n289\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nby repeatedly applying the same operation at each time step of a long temporal\\nsequence. Repeated application of the same parameters gives rise to especially\\npronounced difficulties.\\nFor example, suppose that a computational graph contains a path that consists\\nof repeatedly multiplying by a matrix W. After t steps, this is equivalent to mul-\\ntiplying by Wt. Suppose that W has an eigendecomposition W = V diag(λ)V 1.\\n−\\nIn this simple case, it is straightforward to see that\\nW t = V diag(λ)V 1 t = V diag(λ)tV 1. (8.11)\\n− −\\nAny eigenvalues λ that are\\ue000not near an abs\\ue001olute value of 1 will either explode if they\\ni\\nare greater than 1 in magnitude or vanish if they are less than 1 in magnitude. The\\nvanishing and exploding gradient problem refers to the fact that gradients\\nthrough such a graph are also scaled according to diag(λ)t. Vanishing gradients'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='make it difficult to know which direction the parameters should move to improve\\nthe cost function, while exploding gradients can make learning unstable. The cliff\\nstructures described earlier that motivate gradient clipping are an example of the\\nexploding gradient phenomenon.\\nThe repeated multiplication by W at each time step described here is very\\nsimilar to the power method algorithm used to find the largest eigenvalue of\\na matrix W and the corresponding eigenvector. From this point of view it is\\nnot surprising that x Wt will eventually discard all components of x that are\\n\\ue03e\\northogonal to the principal eigenvector of W.\\nRecurrent networks use the same matrix W at each time step, but feedforward\\nnetworks do not, so even very deep feedforward networks can largely avoid the\\nvanishing and exploding gradient problem (Sussillo, 2014).\\nWe defer a further discussion of the challenges of training recurrent networks'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='until section 10.7, after recurrent networks have been described in more detail.\\n8.2.6 Inexact Gradients\\nMost optimization algorithms are designed with the assumption that we have\\naccess to the exact gradient or Hessian matrix. In practice, we usually only have\\na noisy or even biased estimate of these quantities. Nearly every deep learning\\nalgorithm relies on sampling-based estimates at least insofar as using a minibatch\\nof training examples to compute the gradient.\\nIn other cases, the objectivefunction wewant to minimize is actually intractable.\\nWhen the objective function is intractable, typically its gradient is intractable as\\nwell. In such cases we can only approximate the gradient. These issues mostly arise\\n290\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nwith the more advanced models in part III. For example, contrastive divergence\\ngives a technique for approximating the gradient of the intractable log-likelihood\\nof a Boltzmann machine.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Various neural network optimization algorithms are designed to account for\\nimperfections in the gradient estimate. One can also avoid the problem by choosing\\na surrogate loss function that is easier to approximate than the true loss.\\n8.2.7 Poor Correspondence between Local and Global Structure\\nMany of the problems we have discussed so far correspond to properties of the\\nloss function at a single point—it can be difficult to make a single step if J(θ) is\\npoorly conditioned at the current point θ, or if θ lies on a cliff, or if θ is a saddle\\npoint hiding the opportunity to make progress downhill from the gradient.\\nIt is possible to overcome all of these problems at a single point and still\\nperform poorly if the direction that results in the most improvement locally does\\nnot point toward distant regions of much lower cost.\\nGoodfellow et al. (2015) argue that much of the runtime of training is due to\\nthe length of the trajectory needed to arrive at the solution. Figure 8.2 shows that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the learning trajectory spends most of its time tracing out a wide arc around a\\nmountain-shaped structure.\\nMuch of research into the difficulties of optimization has focused on whether\\ntraining arrives at a global minimum, a local minimum, or a saddle point, but in\\npractice neural networks do not arrive at a critical point of any kind. Figure 8.1\\nshows that neural networks often do not arrive at a region of small gradient. Indeed,\\nsuch critical points do not even necessarily exist. For example, the loss function\\nlogp(y x;θ) can lack a global minimum point and instead asymptotically\\n− |\\napproach some value as the model becomes more confident. For a classifier with\\ndiscrete y and p(y x) provided by a softmax, the negative log-likelihood can\\n|\\nbecome arbitrarily close to zero if the model is able to correctly classify every\\nexample in the training set, but it is impossible to actually reach the value of\\nzero. Likewise, a model of real values p(y x) = (y;f(θ),β 1) can have negative\\n−\\n| N'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='log-likelihood that asymptotes to negative infinity—if f(θ) is able to correctly\\npredict the value of all training set y targets, the learning algorithm will increase\\nβ without bound. See figure 8.4 for an example of a failure of local optimization to\\nfind a good cost function value even in the absence of any local minima or saddle\\npoints.\\nFuture research will need to develop further understanding of the factors that\\ninfluence the length of the learning trajectory and better characterize the outcome\\n291\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nθ\\n)θ(J\\nFigure 8.4: Optimization based on local downhill moves can fail if the local surface does\\nnot point toward the global solution. Here we provide an example of how this can occur,\\neven if there are no saddle points and no local minima. This example cost function\\ncontains only asymptotes toward low values, not minima. The main cause of difficulty in'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='this case is being initialized on the wrong side of the “mountain” and not being able to\\ntraverse it. In higher dimensional space, learning algorithms can often circumnavigate\\nsuch mountains but the trajectory associated with doing so may be long and result in\\nexcessive training time, as illustrated in figure 8.2.\\nof the process.\\nMany existing research directions are aimed at finding good initial points for\\nproblems that have difficult global structure, rather than developing algorithms\\nthat use non-local moves.\\nGradient descent and essentially all learning algorithms that are effective for\\ntraining neural networks are based on making small, local moves. The previous\\nsections have primarily focused on how the correct direction of these local moves\\ncan be difficult to compute. We may be able to compute some properties of the\\nobjective function, such as its gradient, only approximately, with bias or variance'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='in our estimate of the correct direction. In these cases, local descent may or may\\nnot define a reasonably short path to a valid solution, but we are not actually\\nable to follow the local descent path. The objective function may have issues\\nsuch as poor conditioning or discontinuous gradients, causing the region where\\nthe gradient provides a good model of the objective function to be very small. In\\nthese cases, local descent with steps of size \\ue00f may define a reasonably short path\\nto the solution, but we are only able to compute the local descent direction with\\nsteps of size δ \\ue00f. In these cases, local descent may or may not define a path\\n\\ue01c\\nto the solution, but the path contains many steps, so following the path incurs a\\n292\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nhigh computational cost. Sometimes local information provides us no guide, when\\nthe function has a wide flat region, or if we manage to land exactly on a critical'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='point (usually this latter scenario only happens to methods that solve explicitly\\nfor critical points, such as Newton’s method). In these cases, local descent does\\nnot define a path to a solution at all. In other cases, local moves can be too greedy\\nand lead us along a path that moves downhill but away from any solution, as in\\nfigure 8.4, or along an unnecessarily long trajectory to the solution, as in figure 8.2.\\nCurrently, we do not understand which of these problems are most relevant to\\nmaking neural network optimization difficult, and this is an active area of research.\\nRegardless of which of these problems are most significant, all of them might be\\navoided if there exists a region of space connected reasonably directly to a solution\\nby a path that local descent can follow, and if we are able to initialize learning\\nwithin that well-behaved region. This last view suggests research into choosing\\ngood initial points for traditional optimization algorithms to use.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='8.2.8 Theoretical Limits of Optimization\\nSeveral theoretical results show that there are limits on the performance of any\\noptimization algorithm we might design for neural networks (Blum and Rivest,\\n1992; Judd, 1989; Wolpert and MacReady, 1997). Typically these results have\\nlittle bearing on the use of neural networks in practice.\\nSome theoretical results apply only to the case where the units of a neural\\nnetwork output discrete values. However, most neural network units output\\nsmoothly increasing values that make optimization via local search feasible. Some\\ntheoretical results show that there exist problem classes that are intractable, but\\nit can be difficult to tell whether a particular problem falls into that class. Other\\nresults show that finding a solution for a network of a given size is intractable, but\\nin practice we can find a solution easily by using a larger network for which many\\nmore parameter settings correspond to an acceptable solution. Moreover, in the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='context of neural network training, we usually do not care about finding the exact\\nminimum of a function, but seek only to reduce its value sufficiently to obtain good\\ngeneralization error. Theoretical analysis of whether an optimization algorithm\\ncan accomplish this goal is extremely difficult. Developing more realistic bounds\\non the performance of optimization algorithms therefore remains an important\\ngoal for machine learning research.\\n293\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\n8.3 Basic Algorithms\\nWe have previously introduced the gradient descent (section 4.3) algorithm that\\nfollows the gradient of an entire training set downhill. This may be accelerated\\nconsiderably by using stochastic gradient descent to follow the gradient of randomly\\nselected minibatches downhill, as discussed in section 5.9 and section 8.1.3.\\n8.3.1 Stochastic Gradient Descent\\nStochastic gradient descent (SGD) and its variants are probably the most used'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='optimization algorithms for machine learning in general and for deep learning\\nin particular. As discussed in section 8.1.3, it is possible to obtain an unbiased\\nestimate of the gradient by taking the average gradient on a minibatch of m\\nexamples drawn i.i.d from the data generating distribution.\\nAlgorithm 8.1 shows how to follow this estimate of the gradient downhill.\\nAlgorithm 8.1 Stochastic gradient descent (SGD) update at training iteration k\\nRequire: Learning rate \\ue00f .\\nk\\nRequire: Initial parameter θ\\nwhile stopping criterion not met do\\nSample a minibatch of m examples from the training set x(1),...,x(m) with\\n{ }\\ncorresponding targets y(i).\\nCompute gradient estimate: gˆ + 1 L(f(x(i);θ),y(i))\\n← m∇θ i\\nApply update: θ θ \\ue00fˆg\\n← − \\ue050\\nend while\\nA crucial parameter for the SGD algorithm is the learning rate. Previously, we\\nhave described SGD as using a fixed learning rate \\ue00f. In practice, it is necessary to\\ngradually decrease the learning rate over time, so we now denote the learning rate'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='at iteration k as \\ue00f .\\nk\\nThis is because the SGD gradient estimator introduces a source of noise (the\\nrandom sampling of m training examples) that does not vanish even when we arrive\\nat a minimum. By comparison, the true gradient of the total cost function becomes\\nsmall and then 0 when we approach and reach a minimum using batch gradient\\ndescent, so batch gradient descent can use a fixed learning rate. A sufficient\\ncondition to guarantee convergence of SGD is that\\n∞\\n\\ue00f = , and (8.12)\\nk\\n∞\\nk=1\\n294\\n\\ue058\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\n∞\\n\\ue00f2 < . (8.13)\\nk\\n∞\\nk=1\\n\\ue058\\nIn practice, it is common to decay the learning rate linearly until iteration τ:\\n\\ue00f = (1 α)\\ue00f +α\\ue00f (8.14)\\nk 0 τ\\n−\\nwith α = k. After iteration τ, it is common to leave \\ue00f constant.\\nτ\\nThe learning rate may be chosen by trial and error, but it is usually best\\nto choose it by monitoring learning curves that plot the objective function as a\\nfunction of time. This is more of an art than a science, and most guidance on this'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='subject should be regarded with some skepticism. When using the linear schedule,\\nthe parameters to choose are \\ue00f , \\ue00f , and τ. Usually τ may be set to the number of\\n0 τ\\niterations required to make a few hundred passes through the training set. Usually\\n\\ue00f should be set to roughly 1% the value of \\ue00f . The main question is how to set \\ue00f .\\nτ 0 0\\nIf it is too large, the learning curve will show violent oscillations, with the cost\\nfunction often increasing significantly. Gentle oscillations are fine, especially if\\ntraining with a stochastic cost function such as the cost function arising from the\\nuse of dropout. If the learning rate is too low, learning proceeds slowly, and if the\\ninitial learning rate is too low, learning may become stuck with a high cost value.\\nTypically, the optimal initial learning rate, in terms of total training time and the\\nfinal cost value, is higher than the learning rate that yields the best performance'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='after the first 100 iterations or so. Therefore, it is usually best to monitor the first\\nseveral iterations and use a learning rate that is higher than the best-performing\\nlearning rate at this time, but not so high that it causes severe instability.\\nThe most important property of SGD and related minibatch or online gradient-\\nbased optimization is that computation time per update does not grow with the\\nnumber of training examples. This allows convergence even when the number\\nof training examples becomes very large. For a large enough dataset, SGD may\\nconverge to within some fixed tolerance of its final test set error before it has\\nprocessed the entire training set.\\nTo study the convergence rate of an optimization algorithm it is common to\\nmeasure the excess error J(θ) min J(θ), which is the amount that the current\\nθ\\n−\\ncost function exceeds the minimum possible cost. When SGD is applied to a convex\\nproblem, the excess error is O( 1 ) after k iterations, while in the strongly convex\\n√k'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='case it is O(1). These bounds cannot be improved unless extra conditions are\\nk\\nassumed. Batch gradient descent enjoys better convergence rates than stochastic\\ngradient descent in theory. However, the Cramér-Rao bound (Cramér, 1946; Rao,\\n1945) states that generalization error cannot decrease faster than O(1). Bottou\\nk\\n295\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nand Bousquet (2008) argue that it therefore may not be worthwhile to pursue\\nan optimization algorithm that converges faster than O( 1) for machine learning\\nk\\ntasks—faster convergence presumably corresponds to overfitting. Moreover, the\\nasymptotic analysis obscures many advantages that stochastic gradient descent\\nhas after a small number of steps. With large datasets, the ability of SGD to make\\nrapid initial progress while evaluating the gradient for only very few examples\\noutweighs its slow asymptotic convergence. Most of the algorithms described in'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the remainder of this chapter achieve benefits that matter in practice but are lost\\nin the constant factors obscured by the O(1) asymptotic analysis. One can also\\nk\\ntrade off the benefits of both batch and stochastic gradient descent by gradually\\nincreasing the minibatch size during the course of learning.\\nFor more information on SGD, see Bottou (1998).\\n8.3.2 Momentum\\nWhile stochastic gradient descent remains a very popular optimization strategy,\\nlearning with it can sometimes be slow. The method of momentum (Polyak, 1964)\\nis designed to accelerate learning, especially in the face of high curvature, small but\\nconsistent gradients, or noisy gradients. The momentum algorithm accumulates\\nan exponentially decaying moving average of past gradients and continues to move\\nin their direction. The effect of momentum is illustrated in figure 8.5.\\nFormally, the momentum algorithm introduces a variable v that plays the role'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of velocity—it is the direction and speed at which the parameters move through\\nparameter space. The velocity is set to an exponentially decaying average of the\\nnegative gradient. The name momentum derives from a physical analogy, in\\nwhich the negative gradient is a force moving a particle through parameter space,\\naccording to Newton’s laws of motion. Momentum in physics is mass times velocity.\\nIn the momentum learning algorithm, we assume unit mass, so the velocity vectorv\\nmay also be regarded as the momentum of the particle. A hyperparameter α [0,1)\\n∈\\ndetermines how quickly the contributions of previous gradients exponentially decay.\\nThe update rule is given by:\\nm\\n1\\nv αv \\ue00f L(f(x(i);θ),y(i)) , (8.15)\\nθ\\n← − ∇ m\\n\\ue020 \\ue021\\ni=1\\n\\ue058\\nθ θ +v. (8.16)\\n←\\nThe velocity v accumulates the gradient elements 1 m L(f(x(i);θ),y(i)) .\\n∇θ m i=1\\nThe larger α is relative to \\ue00f, the more previous gradients affect the current direction.\\nThe SGD algorithm with momentum is given in algorithm 8.2.\\n296\\n\\ue000 \\ue050 \\ue001'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\n20\\n10\\n0\\n10\\n−\\n20\\n−\\n30\\n− 30 20 10 0 10 20\\n− − −\\nFigure 8.5: Momentum aims primarily to solve two problems: poor conditioning of the\\nHessian matrix and variance in the stochastic gradient. Here, we illustrate how momentum\\novercomes the first of these two problems. The contour lines depict a quadratic loss\\nfunction with a poorly conditioned Hessian matrix. The red path cutting across the\\ncontours indicates the path followed by the momentum learning rule as it minimizes this\\nfunction. At each step along the way, we draw an arrow indicating the step that gradient\\ndescent would take at that point. We can see that a poorly conditioned quadratic objective\\nlooks like a long, narrow valley or canyon with steep sides. Momentum correctly traverses\\nthe canyon lengthwise, while gradient steps waste time moving back and forth across the\\nnarrow axis of the canyon. Compare also figure 4.6, which shows the behavior of gradient\\ndescent without momentum.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='297\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nPreviously, the size of the step was simply the norm of the gradient multiplied\\nby the learning rate. Now, the size of the step depends on how large and how\\naligned a sequence of gradients are. The step size is largest when many successive\\ngradients point in exactly the same direction. If the momentum algorithm always\\nobserves gradient g, then it will accelerate in the direction of g, until reaching a\\n−\\nterminal velocity where the size of each step is\\n\\ue00f g\\n|| ||. (8.17)\\n1 α\\n−\\nIt is thus helpful to think of the momentum hyperparameter in terms of 1 . For\\n1 α\\nexample, α = .9 corresponds to multiplying the maximum speed by 10 rel−ative to\\nthe gradient descent algorithm.\\nCommon values of α used in practice include .5, .9, and .99. Like the learning\\nrate, α may also be adapted over time. Typically it begins with a small value and\\nis later raised. It is less important to adapt α over time than to shrink \\ue00f over time.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Algorithm 8.2 Stochastic gradient descent (SGD) with momentum\\nRequire: Learning rate \\ue00f, momentum parameter α.\\nRequire: Initial parameter θ, initial velocity v.\\nwhile stopping criterion not met do\\nSample a minibatch of m examples from the training set x(1),...,x(m) with\\n{ }\\ncorresponding targets y(i).\\nCompute gradient estimate: g 1 L(f(x(i);θ),y(i))\\n← m∇θ i\\nCompute velocity update: v αv \\ue00fg\\n← − \\ue050\\nApply update: θ θ + v\\n←\\nend while\\nWe can view the momentum algorithm as simulating a particle subject to\\ncontinuous-time Newtonian dynamics. The physical analogy can help to build\\nintuition for how the momentum and gradient descent algorithms behave.\\nThe position of the particle at any point in time is given by θ(t). The particle\\nexperiences net force f(t). This force causes the particle to accelerate:\\n∂2\\nf(t) = θ(t). (8.18)\\n∂t2\\nRather than viewing this as a second-order differential equation of the position,\\nwe can introduce the variable v(t) representing the velocity of the particle at time'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='t and rewrite the Newtonian dynamics as a first-order differential equation:\\n∂\\nv(t) = θ(t), (8.19)\\n∂t\\n298\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\n∂\\nf(t) = v(t). (8.20)\\n∂t\\nThe momentum algorithm then consists of solving the differential equations via\\nnumerical simulation. A simple numerical method for solving differential equations\\nis Euler’s method, which simply consists of simulating the dynamics defined by\\nthe equation by taking small, finite steps in the direction of each gradient.\\nThis explains the basic form of the momentum update, but what specifically are\\nthe forces? One force is proportional to the negative gradient of the cost function:\\nJ(θ). This force pushes the particle downhill along the cost function surface.\\nθ\\n−∇\\nThe gradient descent algorithm would simply take a single step based on each\\ngradient, but the Newtonian scenario used by the momentum algorithm instead\\nuses this force to alter the velocity of the particle. We can think of the particle'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='as being like a hockey puck sliding down an icy surface. Whenever it descends a\\nsteep part of the surface, it gathers speed and continues sliding in that direction\\nuntil it begins to go uphill again.\\nOne other force is necessary. If the only force is the gradient of the cost function,\\nthen the particle might never come to rest. Imagine a hockey puck sliding down\\none side of a valley and straight up the other side, oscillating back and forth forever,\\nassuming the ice is perfectly frictionless. To resolve this problem, we add one\\nother force, proportional to v(t). In physics terminology, this force corresponds\\n−\\nto viscous drag, as if the particle must push through a resistant medium such as\\nsyrup. This causes the particle to gradually lose energy over time and eventually\\nconverge to a local minimum.\\nWhy do we use v(t) and viscous drag in particular? Part of the reason to\\n−\\nuse v(t) is mathematical convenience—an integer power of the velocity is easy\\n−'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to work with. However, other physical systems have other kinds of drag based\\non other integer powers of the velocity. For example, a particle traveling through\\nthe air experiences turbulent drag, with force proportional to the square of the\\nvelocity, while a particle moving along the ground experiences dry friction, with a\\nforce of constant magnitude. We can reject each of these options. Turbulent drag,\\nproportional to the square of the velocity, becomes very weak when the velocity is\\nsmall. It is not powerful enough to force the particle to come to rest. A particle\\nwith a non-zero initial velocity that experiences only the force of turbulent drag\\nwill move away from its initial position forever, with the distance from the starting\\npoint growing like O(logt). We must therefore use a lower power of the velocity.\\nIf we use a power of zero, representing dry friction, then the force is too strong.\\nWhen the force due to the gradient of the cost function is small but non-zero, the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='constant force due to friction can cause the particle to come to rest before reaching\\na local minimum. Viscous drag avoids both of these problems—it is weak enough\\n299\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nthat the gradient can continue to cause motion until a minimum is reached, but\\nstrong enough to prevent motion if the gradient does not justify moving.\\n8.3.3 Nesterov Momentum\\nSutskever et al. (2013) introduced a variant of the momentum algorithm that was\\ninspired by Nesterov’s accelerated gradient method (Nesterov, 1983, 2004). The\\nupdate rules in this case are given by:\\nm\\n1\\nv αv \\ue00f L f(x(i);θ +αv),y(i) , (8.21)\\nθ\\n← − ∇ m\\n\\ue022 \\ue023\\n\\ue058i=1 \\ue010 \\ue011\\nθ θ +v, (8.22)\\n←\\nwhere the parameters α and \\ue00f play a similar role as in the standard momentum\\nmethod. The difference between Nesterov momentum and standard momentum is\\nwhere the gradientis evaluated. With Nesterovmomentumthe gradient is evaluated\\nafter the current velocity is applied. Thus one can interpret Nesterov momentum'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='as attempting to add a correction factor to the standard method of momentum.\\nThe complete Nesterov momentum algorithm is presented in algorithm 8.3.\\nIn the convex batch gradient case, Nesterov momentum brings the rate of\\nconvergence of the excess error from O(1/k) (after k steps) to O(1/k2) as shown\\nby Nesterov (1983). Unfortunately, in the stochastic gradient case, Nesterov\\nmomentum does not improve the rate of convergence.\\nAlgorithm 8.3 Stochastic gradient descent (SGD) with Nesterov momentum\\nRequire: Learning rate \\ue00f, momentum parameter α.\\nRequire: Initial parameter θ, initial velocity v.\\nwhile stopping criterion not met do\\nSample a minibatch of m examples from the training set x(1),...,x(m) with\\n{ }\\ncorresponding labels y(i).\\n˜\\nApply interim update: θ θ +αv\\n←\\nCompute gradient (at interim point): g 1 L(f(x(i);θ˜ ),y(i))\\n← m∇θ˜ i\\nCompute velocity update: v αv \\ue00fg\\n← − \\ue050\\nApply update: θ θ + v\\n←\\nend while\\n300\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='8.4 Parameter Initialization Strategies\\nSome optimization algorithms are not iterative by nature and simply solve for a\\nsolution point. Other optimization algorithms are iterative by nature but, when\\napplied to the right class of optimization problems, converge to acceptable solutions\\nin an acceptable amount of time regardless of initialization. Deep learning training\\nalgorithms usually do not have either of these luxuries. Training algorithms for deep\\nlearning models are usually iterative in nature and thus require the user to specify\\nsome initial point from which to begin the iterations. Moreover, training deep\\nmodels is a sufficiently difficult task that most algorithms are strongly affected by\\nthe choice of initialization. The initial point can determine whether the algorithm\\nconverges at all, with some initial points being so unstable that the algorithm\\nencounters numerical difficulties and fails altogether. When learning does converge,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the initial point can determine how quickly learning converges and whether it\\nconverges to a point with high or low cost. Also, points of comparable cost\\ncan have wildly varying generalization error, and the initial point can affect the\\ngeneralization as well.\\nModern initialization strategies are simple and heuristic. Designing improved\\ninitialization strategies is a difficult task because neural network optimization is\\nnot yet well understood. Most initialization strategies are based on achieving some\\nnice properties when the network is initialized. However, we do not have a good\\nunderstanding of which of these properties are preserved under which circumstances\\nafter learning begins to proceed. A further difficulty is that some initial points\\nmay be beneficial from the viewpoint of optimization but detrimental from the\\nviewpoint of generalization. Our understanding of how the initial point affects'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='generalization is especially primitive, offering little to no guidance for how to select\\nthe initial point.\\nPerhaps the only property known with complete certainty is that the initial\\nparameters need to “break symmetry” between different units. If two hidden\\nunits with the same activation function are connected to the same inputs, then\\nthese units must have different initial parameters. If they have the same initial\\nparameters, then a deterministic learning algorithm applied to a deterministic cost\\nand model will constantly update both of these units in the same way. Even if the\\nmodel or training algorithm is capable of using stochasticity to compute different\\nupdates for different units (for example, if one trains with dropout), it is usually\\nbest to initialize each unit to compute a different function from all of the other\\nunits. This may help to make sure that no input patterns are lost in the null\\nspace of forward propagation and no gradient patterns are lost in the null space'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of back-propagation. The goal of having each unit compute a different function\\n301\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nmotivates random initialization of the parameters. We could explicitly search\\nfor a large set of basis functions that are all mutually different from each other,\\nbut this often incurs a noticeable computational cost. For example, if we have at\\nmost as many outputs as inputs, we could use Gram-Schmidt orthogonalization\\non an initial weight matrix, and be guaranteed that each unit computes a very\\ndifferent function from each other unit. Random initialization from a high-entropy\\ndistribution over a high-dimensional space is computationally cheaper and unlikely\\nto assign any units to compute the same function as each other.\\nTypically, we set the biases for each unit to heuristically chosen constants, and\\ninitialize only the weights randomly. Extra parameters, for example, parameters'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='encoding the conditional variance of a prediction, are usually set to heuristically\\nchosen constants much like the biases are.\\nWe almost always initialize all the weights in the model to values drawn\\nrandomly from a Gaussian or uniform distribution. The choice of Gaussian\\nor uniform distribution does not seem to matter very much, but has not been\\nexhaustively studied. The scale of the initial distribution, however, does have a\\nlarge effect on both the outcome of the optimization procedure and on the ability\\nof the network to generalize.\\nLarger initial weights will yield a stronger symmetry breaking effect, helping\\nto avoid redundant units. They also help to avoid losing signal during forward or\\nback-propagation through the linear component of each layer—larger values in the\\nmatrix result in larger outputs of matrix multiplication. Initial weights that are\\ntoo large may, however, result in exploding values during forward propagation or'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='back-propagation. In recurrent networks, large weights can also result in chaos\\n(such extreme sensitivity to small perturbations of the input that the behavior\\nof the deterministic forward propagation procedure appears random). To some\\nextent, the exploding gradient problem can be mitigated by gradient clipping\\n(thresholding the values of the gradients before performing a gradient descent step).\\nLarge weights may also result in extreme values that cause the activation function\\nto saturate, causing complete loss of gradient through saturated units. These\\ncompeting factors determine the ideal initial scale of the weights.\\nThe perspectives of regularization and optimization can give very different\\ninsights into how we should initialize a network. The optimization perspective\\nsuggests that the weights should be large enough to propagate information success-\\nfully, but some regularization concerns encourage making them smaller. The use'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of an optimization algorithm such as stochastic gradient descent that makes small\\nincremental changes to the weights and tends to halt in areas that are nearer to\\nthe initial parameters (whether due to getting stuck in a region of low gradient, or\\n302\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\ndue to triggering some early stopping criterion based on overfitting) expresses a\\nprior that the final parameters should be close to the initial parameters. Recall\\nfrom section 7.8 that gradient descent with early stopping is equivalent to weight\\ndecay for some models. In the general case, gradient descent with early stopping is\\nnot the same as weight decay, but does provide a loose analogy for thinking about\\nthe effect of initialization. We can think of initializing the parameters θ to θ as\\n0\\nbeing similar to imposing a Gaussian prior p(θ) with mean θ . From this point\\n0\\nof view, it makes sense to choose θ to be near 0. This prior says that it is more\\n0'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='likely that units do not interact with each other than that they do interact. Units\\ninteract only if the likelihood term of the objective function expresses a strong\\npreference for them to interact. On the other hand, if we initialize θ to large\\n0\\nvalues, then our prior specifies which units should interact with each other, and\\nhow they should interact.\\nSome heuristics are available for choosing the initial scale of the weights. One\\nheuristic is to initialize the weights of a fully connected layer with minputs and\\nn outputs by sampling each weight from U( 1 , 1 ), while Glorot and Bengio\\n−√m √m\\n(2010) suggest using the normalized initialization\\n6 6\\nW U , . (8.23)\\ni,j\\n∼ − m+ n m+ n\\n\\ue020 \\ue072 \\ue072 \\ue021\\nThis latter heuristic is designed to compromise between the goal of initializing\\nall layers to have the same activation variance and the goal of initializing all\\nlayers to have the same gradient variance. The formula is derived using the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='assumption that the network consists only of a chain of matrix multiplications,\\nwith no nonlinearities. Real neural networks obviously violate this assumption,\\nbut many strategies designed for the linear model perform reasonably well on its\\nnonlinear counterparts.\\nSaxe et al. (2013) recommend initializing to random orthogonal matrices, with\\na carefully chosen scaling or gain factorg that accounts for the nonlinearity applied\\nat each layer. They derive specific values of the scaling factor for different types of\\nnonlinear activation functions. This initialization scheme is also motivated by a\\nmodel of a deep network as a sequence of matrix multiplies without nonlinearities.\\nUnder such a model, this initialization scheme guarantees that the total number of\\ntraining iterations required to reach convergence is independent of depth.\\nIncreasing the scaling factor g pushes the network toward the regime where\\nactivations increase in norm as they propagate forward through the network and'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='gradients increase in norm as they propagate backward. Sussillo (2014) showed\\nthat setting the gain factor correctly is sufficient to train networks as deep as\\n303\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\n1,000 layers, without needing to use orthogonal initializations. A key insight of\\nthis approach is that in feedforward networks, activations and gradients can grow\\nor shrink on each step of forward or back-propagation, following a random walk\\nbehavior. This is because feedforward networks use a different weight matrix at\\neach layer. If this random walk is tuned to preserve norms, then feedforward\\nnetworks can mostly avoid the vanishing and exploding gradients problem that\\narises when the same weight matrix is used at each step, described in section 8.2.5.\\nUnfortunately, these optimal criteria for initial weights often do not lead to\\noptimal performance. This may be for three different reasons. First, we may'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='be using the wrong criteria—it may not actually be beneficial to preserve the\\nnorm of a signal throughout the entire network. Second, the properties imposed\\nat initialization may not persist after learning has begun to proceed. Third, the\\ncriteria might succeed at improving the speed of optimization but inadvertently\\nincrease generalization error. In practice, we usually need to treat the scale of the\\nweights as a hyperparameter whose optimal value lies somewhere roughly near but\\nnot exactly equal to the theoretical predictions.\\nOne drawback to scaling rules that set all of the initial weights to have the\\nsame standard deviation, such as 1 , is that every individual weight becomes\\n√m\\nextremely small when the layers become large. Martens (2010) introduced an\\nalternative initialization scheme called sparse initialization in which each unit is\\ninitialized to have exactly knon-zero weights. The idea is to keep the total amount'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of input to the unit independent from the number of inputs m without making the\\nmagnitude of individual weight elements shrink with m. Sparse initialization helps\\nto achieve more diversity among the units at initialization time. However, it also\\nimposes a very strong prior on the weights that are chosen to have large Gaussian\\nvalues. Because it takes a long time for gradient descent to shrink “incorrect” large\\nvalues, this initialization scheme can cause problems for units such as maxout units\\nthat have several filters that must be carefully coordinated with each other.\\nWhen computational resources allow it, it is usually a good idea to treat the\\ninitial scale of the weights for each layer as a hyperparameter, and to choose these\\nscales using a hyperparameter search algorithm described in section 11.4.2, such\\nas random search. The choice of whether to use dense or sparse initialization\\ncan also be made a hyperparameter. Alternately, one can manually search for'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the best initial scales. A good rule of thumb for choosing the initial scales is to\\nlook at the range or standard deviation of activations or gradients on a single\\nminibatch of data. If the weights are too small, the range of activations across the\\nminibatch will shrink as the activations propagate forward through the network.\\nBy repeatedly identifying the first layer with unacceptably small activations and\\n304\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nincreasing its weights, it is possible to eventually obtain a network with reasonable\\ninitial activations throughout. If learning is still too slow at this point, it can be\\nuseful to look at the range or standard deviation of the gradients as well as the\\nactivations. This procedure can in principle be automated and is generally less\\ncomputationally costly than hyperparameter optimization based on validation set\\nerror because it is based on feedback from the behavior of the initial model on a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='single batch of data, rather than on feedback from a trained model on the validation\\nset. While long used heuristically, this protocol has recently been specified more\\nformally and studied by Mishkin and Matas (2015).\\nSo far we have focused on the initialization of the weights. Fortunately,\\ninitialization of other parameters is typically easier.\\nThe approach for setting the biases must be coordinated with the approach\\nfor settings the weights. Setting the biases to zero is compatible with most weight\\ninitialization schemes. There are a few situations where we may set some biases to\\nnon-zero values:\\nIf a bias is for an output unit, then it is often beneficial to initialize the bias to\\n•\\nobtain the right marginal statistics of the output. To do this, we assume that\\nthe initial weights are small enough that the output of the unit is determined\\nonly by the bias. This justifies setting the bias to the inverse of the activation'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='function applied to the marginal statistics of the output in the training set.\\nFor example, if the output is a distribution over classes and this distribution\\nis a highly skewed distribution with the marginal probability of class i given\\nby element c of some vector c, then we can set the bias vector b by solving\\ni\\nthe equation softmax(b) = c. This applies not only to classifiers but also to\\nmodels we will encounter in Part III, such as autoencoders and Boltzmann\\nmachines. These models have layers whose output should resemble the input\\ndata x, and it can be very helpful to initialize the biases of such layers to\\nmatch the marginal distribution over x.\\nSometimes we may want to choose the bias to avoid causing too much\\n•\\nsaturation at initialization. For example, we may set the bias of a ReLU\\nhidden unit to 0.1 rather than 0 to avoid saturating the ReLU at initialization.\\nThis approach is not compatible with weight initialization schemes that do'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='not expect strong input from the biases though. For example, it is not\\nrecommended for use with random walk initialization (Sussillo, 2014).\\nSometimes a unit controls whether other units are able to participate in a\\n•\\nfunction. In such situations, we have a unit with output u and another unit\\nh [0, 1], and they are multiplied together to produce an output uh. We\\n∈\\n305\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\ncan view h as a gate that determines whether uh u or uh 0. In these\\n≈ ≈\\nsituations, we want to set the bias for h so that h 1 most of the time at\\n≈\\ninitialization. Otherwise u does not have a chance to learn. For example,\\nJozefowicz et al. (2015) advocate setting the bias to 1 for the forget gate of\\nthe LSTM model, described in section 10.10.\\nAnother common type of parameter is a variance or precision parameter. For\\nexample, we can perform linear regression with a conditional variance estimate\\nusing the model\\np(y x) = (y wTx+ b,1/β) (8.24)\\n| N |'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='where β is a precision parameter. We can usually initialize variance or precision\\nparameters to 1 safely. Another approach is to assume the initial weights are close\\nenough to zero that the biases may be set while ignoring the effect of the weights,\\nthen set the biases to produce the correct marginal mean of the output, and set\\nthe variance parameters to the marginal variance of the output in the training set.\\nBesides these simple constant or random methods of initializing model parame-\\nters, it is possible to initialize model parameters using machine learning. A common\\nstrategy discussed in part III of this book is to initialize a supervised model with\\nthe parameters learned by an unsupervised model trained on the same inputs.\\nOne can also perform supervised training on a related task. Even performing\\nsupervised training on an unrelated task can sometimes yield an initialization that\\noffers faster convergence than a random initialization. Some of these initialization'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='strategies may yield faster convergence and better generalization because they\\nencode information about the distribution in the initial parameters of the model.\\nOthers apparently perform well primarily because they set the parameters to have\\nthe right scale or set different units to compute different functions from each other.\\n8.5 Algorithms with Adaptive Learning Rates\\nNeural network researchers have long realized that the learning rate was reliably one\\nof the hyperparameters that is the most difficult to set because it has a significant\\nimpact on model performance. As we have discussed in sections 4.3 and 8.2, the\\ncost is often highly sensitive to some directions in parameter space and insensitive\\nto others. The momentum algorithm can mitigate these issues somewhat, but\\ndoes so at the expense of introducing another hyperparameter. In the face of this,\\nit is natural to ask if there is another way. If we believe that the directions of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='sensitivity are somewhat axis-aligned, it can make sense to use a separate learning\\n306\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nrate for each parameter, and automatically adapt these learning rates throughout\\nthe course of learning.\\nThe delta-bar-delta algorithm (Jacobs, 1988) is an early heuristic approach\\nto adapting individual learning rates for model parameters during training. The\\napproach is based on a simple idea: if the partial derivative of the loss, with respect\\nto a given model parameter, remains the same sign, then the learning rate should\\nincrease. If the partial derivative with respect to that parameter changes sign,\\nthen the learning rate should decrease. Of course, this kind of rule can only be\\napplied to full batch optimization.\\nMore recently, a number of incremental (or mini-batch-based) methods have\\nbeen introduced that adapt the learning rates of model parameters. This section\\nwill briefly review a few of these algorithms.\\n8.5.1 AdaGrad'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The AdaGrad algorithm, shown in algorithm 8.4, individually adapts the learning\\nrates of all model parameters by scaling them inversely proportional to the square\\nroot of the sum of all of their historical squared values (Duchi et al., 2011). The\\nparameters with the largest partial derivative of the loss have a correspondingly\\nrapid decrease in their learning rate, while parameters with small partial derivatives\\nhave a relatively small decrease in their learning rate. The net effect is greater\\nprogress in the more gently sloped directions of parameter space.\\nIn the context of convex optimization, the AdaGrad algorithm enjoys some\\ndesirable theoretical properties. However, empirically it has been found that—for\\ntraining deep neural network models—the accumulation of squared gradients from\\nthe beginning of training can result in a premature and excessive decrease in the\\neffective learning rate. AdaGrad performs well for some but not all deep learning\\nmodels.\\n8.5.2 RMSProp'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The RMSProp algorithm (Hinton, 2012) modifies AdaGrad to perform better in\\nthe non-convex setting by changing the gradient accumulation into an exponentially\\nweighted moving average. AdaGrad is designed to converge rapidly when applied\\nto a convex function. When applied to a non-convex function to train a neural\\nnetwork, the learning trajectory may pass through many different structures and\\neventually arrive at a region that is a locally convex bowl. AdaGrad shrinks the\\nlearning rate according to the entire history of the squared gradient and may\\n307\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nAlgorithm 8.4 The AdaGrad algorithm\\nRequire: Global learning rate \\ue00f\\nRequire: Initial parameter θ\\nRequire: Small constant δ, perhaps 10 7, for numerical stability\\n−\\nInitialize gradient accumulation variable r = 0\\nwhile stopping criterion not met do\\nSample a minibatch of m examples from the training set x(1),...,x(m) with\\n{ }\\ncorresponding targets y(i).\\nCompute gradient: g 1 L(f(x(i);θ),y(i))'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='← m∇θ i\\nAccumulate squared gradient: r r +g g\\n\\ue050← \\ue00c\\nCompute update: ∆θ \\ue00f g. (Division and square root applied\\n← −δ+√r \\ue00c\\nelement-wise)\\nApply update: θ θ + ∆θ\\n←\\nend while\\nhave made the learning rate too small before arriving at such a convex structure.\\nRMSProp uses an exponentially decaying average to discard history from the\\nextreme past so that it can converge rapidly after finding a convex bowl, as if it\\nwere an instance of the AdaGrad algorithm initialized within that bowl.\\nRMSProp is shown in its standard form in algorithm 8.5 and combined with\\nNesterov momentum in algorithm 8.6. Compared to AdaGrad, the use of the\\nmoving average introduces a new hyperparameter, ρ, that controls the length scale\\nof the moving average.\\nEmpirically, RMSProp has been shown to be an effective and practical op-\\ntimization algorithm for deep neural networks. It is currently one of the go-to\\noptimization methods being employed routinely by deep learning practitioners.\\n8.5.3 Adam'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Adam (Kingma and Ba, 2014) is yet another adaptive learning rate optimization\\nalgorithm and is presented in algorithm 8.7. The name “Adam” derives from\\nthe phrase “adaptive moments.” In the context of the earlier algorithms, it is\\nperhaps best seen as a variant on the combination of RMSProp and momentum\\nwith a few important distinctions. First, in Adam, momentum is incorporated\\ndirectly as an estimate of the first order moment (with exponential weighting) of\\nthe gradient. The most straightforward way to add momentum to RMSProp is to\\napply momentum to the rescaled gradients. The use of momentum in combination\\nwith rescaling does not have a clear theoretical motivation. Second, Adam includes\\n308\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nAlgorithm 8.5 The RMSProp algorithm\\nRequire: Global learning rate \\ue00f, decay rate ρ.\\nRequire: Initial parameter θ\\nRequire: Small constant δ, usually 10 6, used to stabilize division by small\\n−\\nnumbers.\\nInitialize accumulation variables r = 0'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='while stopping criterion not met do\\nSample a minibatch of m examples from the training set x(1),...,x(m) with\\n{ }\\ncorresponding targets y(i).\\nCompute gradient: g 1 L(f(x(i);θ),y(i))\\n← m∇θ i\\nAccumulate squared gradient: r ρr +(1 ρ)g g\\n\\ue050← − \\ue00c\\nCompute parameter update: ∆θ = \\ue00f g. ( 1 applied element-wise)\\n−√δ+r\\ue00c √δ+r\\nApply update: θ θ + ∆θ\\n←\\nend while\\nbias corrections to the estimates of both the first-order moments (the momentum\\nterm) and the (uncentered) second-order moments to account for their initialization\\nat the origin (see algorithm 8.7). RMSProp also incorporates an estimate of the\\n(uncentered) second-order moment, however it lacks the correction factor. Thus,\\nunlike in Adam, the RMSProp second-order moment estimate may have high bias\\nearly in training. Adam is generally regarded as being fairly robust to the choice\\nof hyperparameters, though the learning rate sometimes needs to be changed from\\nthe suggested default.\\n8.5.4 Choosing the Right Optimization Algorithm'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In this section, we discussed a series of related algorithms that each seek to address\\nthe challenge of optimizing deep models by adapting the learning rate for each\\nmodel parameter. At this point, a natural question is: which algorithm should one\\nchoose?\\nUnfortunately, there is currently no consensus on this point. Schaul et al. (2014)\\npresented a valuable comparison of a large number of optimization algorithms\\nacross a wide range of learning tasks. While the results suggest that the family of\\nalgorithms with adaptive learning rates (represented by RMSProp and AdaDelta)\\nperformed fairly robustly, no single best algorithm has emerged.\\nCurrently, the most popular optimization algorithms actively in use include\\nSGD, SGD with momentum, RMSProp, RMSProp with momentum, AdaDelta\\nand Adam. The choice of which algorithm to use, at this point, seems to depend\\n309\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nAlgorithm 8.6 RMSProp algorithm with Nesterov momentum'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Require: Global learning rate \\ue00f, decay rate ρ, momentum coefficient α.\\nRequire: Initial parameter θ, initial velocity v.\\nInitialize accumulation variable r = 0\\nwhile stopping criterion not met do\\nSample a minibatch of m examples from the training set x(1),...,x(m) with\\n{ }\\ncorresponding targets y(i).\\n˜\\nCompute interim update: θ θ +αv\\n←\\nCompute gradient: g 1 L(f(x(i);θ˜ ),y(i))\\n← m∇θ˜ i\\nAccumulate gradient: r ρr +(1 ρ)g g\\n← \\ue050 − \\ue00c\\nCompute velocity update: v αv \\ue00f g. ( 1 applied element-wise)\\n← − √r \\ue00c √r\\nApply update: θ θ + v\\n←\\nend while\\nlargely on the user’s familiarity with the algorithm (for ease of hyperparameter\\ntuning).\\n8.6 Approximate Second-Order Methods\\nIn this section we discuss the application of second-order methods to the training\\nof deep networks. See LeCun et al. (1998a) for an earlier treatment of this subject.\\nFor simplicity of exposition, the only objective function we examine is the empirical\\nrisk:\\nm\\n1\\nJ(θ) = E [L(f(x;θ),y)] = L(f(x(i);θ),y(i)). (8.25)\\nx,y pˆ data(x,y)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='m\\n∼\\ni=1\\n\\ue058\\nHowever the methods we discuss here extend readily to more general objective\\nfunctions that, for instance, include parameter regularization terms such as those\\ndiscussed in chapter 7.\\n8.6.1 Newton’s Method\\nIn section 4.3, we introduced second-order gradient methods. In contrast to first-\\norder methods, second-order methods make use of second derivatives to improve\\noptimization. The most widely used second-order method is Newton’s method. We\\nnow describe Newton’s method in more detail, with emphasis on its application to\\nneural network training.\\n310\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nAlgorithm 8.7 The Adam algorithm\\nRequire: Step size \\ue00f (Suggested default: 0.001)\\nRequire: Exponential decay rates for moment estimates, ρ and ρ in [0,1).\\n1 2\\n(Suggested defaults: 0.9 and 0.999 respectively)\\nRequire: Small constant δ used for numerical stabilization. (Suggested default:\\n10 8)\\n−\\nRequire: Initial parameters θ\\nInitialize 1st and 2nd moment variables s = 0, r = 0'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Initialize time step t = 0\\nwhile stopping criterion not met do\\nSample a minibatch of m examples from the training set x(1),...,x(m) with\\n{ }\\ncorresponding targets y(i).\\nCompute gradient: g 1 L(f(x(i);θ),y(i))\\n←\\nm∇θ i\\nt t+ 1\\n← \\ue050\\nUpdate biased first moment estimate: s ρ s+ (1 ρ )g\\n1 1\\n← −\\nUpdate biased second moment estimate: r ρ r + (1 ρ )g g\\n2 2\\n← − \\ue00c\\nCorrect bias in first moment: sˆ s\\n← 1 ρt\\nCorrect bias in second moment: rˆ − 1 r\\n← 1 ρt\\n− 2\\nCompute update: ∆θ = \\ue00f\\nˆs\\n(operations applied element-wise)\\n− √rˆ+δ\\nApply update: θ θ + ∆θ\\n←\\nend while\\nNewton’s method is an optimization scheme based on using a second-order Tay-\\nlor series expansion to approximate J(θ) near some point θ , ignoring derivatives\\n0\\nof higher order:\\n1\\nJ(θ) J(θ )+ (θ θ ) J(θ )+ (θ θ ) H(θ θ ), (8.26)\\n0 0 \\ue03e θ 0 0 \\ue03e 0\\n≈ − ∇ 2 − −\\nwhere H is the Hessian of J with respect to θ evaluated at θ . If we then solve for\\n0\\nthe critical point of this function, we obtain the Newton parameter update rule:\\nθ = θ H 1 J(θ ) (8.27)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='∗ 0 − θ 0\\n− ∇\\nThus for a locally quadratic function (with positive definite H), by rescaling\\nthe gradient by H 1, Newton’s method jumps directly to the minimum. If the\\n−\\nobjective function is convex but not quadratic (there are higher-order terms), this\\nupdate can be iterated, yielding the training algorithm associated with Newton’s\\nmethod, given in algorithm 8.8.\\n311\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nAlgorithm 8.8 Newton’s method with objective J(θ) =\\n1 m L(f(x(i);θ),y(i)).\\nm i=1\\nRequire: Initial parameter θ\\n\\ue050 0\\nRequire: Training set of m examples\\nwhile stopping criterion not met do\\nCompute gradient: g 1 L(f(x(i);θ),y(i))\\n← m∇θ i\\nCompute Hessian: H 1 2 L(f(x(i);θ),y(i))\\n← m ∇θ\\ue050 i\\nCompute Hessian inverse: H 1\\n−\\n\\ue050\\nCompute update: ∆θ = H 1g\\n−\\n−\\nApply update: θ = θ +∆θ\\nend while\\nFor surfaces that are not quadratic, as long as the Hessian remains positive\\ndefinite, Newton’s method can be applied iteratively. This implies a two-step'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='iterative procedure. First, update or compute the inverse Hessian (i.e. by updat-\\ning the quadratic approximation). Second, update the parameters according to\\nequation 8.27.\\nIn section 8.2.3, we discussed how Newton’s method is appropriate only when\\nthe Hessian is positive definite. In deep learning, the surface of the objective\\nfunction is typically non-convex with many features, such as saddle points, that\\nare problematic for Newton’s method. If the eigenvalues of the Hessian are not\\nall positive, for example, near a saddle point, then Newton’s method can actually\\ncause updates to move in the wrong direction. This situation can be avoided\\nby regularizing the Hessian. Common regularization strategies include adding a\\nconstant, α, along the diagonal of the Hessian. The regularized update becomes\\nθ = θ [H (f(θ ))+ αI] 1 f(θ ). (8.28)\\n∗ 0 0 − θ 0\\n− ∇\\nThis regularization strategy is used in approximations to Newton’s method, such'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='as the Levenberg–Marquardt algorithm (Levenberg, 1944; Marquardt, 1963), and\\nworks fairly well as long as the negative eigenvalues of the Hessian are still relatively\\nclose to zero. In cases where there are more extreme directions of curvature, the\\nvalue of α would have to be sufficiently large to offset the negative eigenvalues.\\nHowever, as αincreases in size, the Hessian becomes dominated by the αI diagonal\\nand the direction chosen by Newton’s method converges to the standard gradient\\ndivided by α. When strong negative curvature is present, α may need to be so\\nlarge that Newton’s method would make smaller steps than gradient descent with\\na properly chosen learning rate.\\nBeyond the challenges created by certain features of the objective function,\\n312\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nsuch as saddle points, the application of Newton’s method for training large neural\\nnetworks is limited by the significant computational burden it imposes. The'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='number of elements in the Hessian is squared in the number of parameters, so with\\nk parameters (and for even very small neural networks the number of parameters\\nk can be in the millions), Newton’s method would require the inversion of a k k\\n×\\nmatrix—with computational complexity of O(k3). Also, since the parameters will\\nchange with every update, the inverse Hessian has to be computed at every training\\niteration. As a consequence, only networks with a very small number of parameters\\ncan be practically trained via Newton’s method. In the remainder of this section,\\nwe will discuss alternatives that attempt to gain some of the advantages of Newton’s\\nmethod while side-stepping the computational hurdles.\\n8.6.2 Conjugate Gradients\\nConjugate gradients is a method to efficiently avoid the calculation of the inverse\\nHessian by iteratively descending conjugate directions. The inspiration for this\\napproach follows from a careful study of the weakness of the method of steepest'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='descent (see section 4.3 for details), where line searches are applied iteratively in\\nthe direction associated with the gradient. Figure 8.6 illustrates how the method of\\nsteepest descent, when applied in a quadratic bowl, progresses in a rather ineffective\\nback-and-forth, zig-zag pattern. This happens because each line search direction,\\nwhen given by the gradient, is guaranteed to be orthogonal to the previous line\\nsearch direction.\\nLet the previous search direction be d . At the minimum, where the line\\nt 1\\n−\\nsearch terminates, the directional derivative is zero in direction d : J(θ)\\nt 1 θ\\n− ∇ ·\\nd = 0. Since the gradient at this point defines the current search direction,\\nt 1\\n−\\nd = J (θ) will have no contribution in the direction d . Thusd is orthogonal\\nt θ t 1 t\\n∇ −\\nto d . This relationship between d and d is illustrated in figure 8.6 for\\nt 1 t 1 t\\n− −\\nmultiple iterations of steepest descent. As demonstrated in the figure, the choice of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='orthogonal directions of descent do not preserve the minimum along the previous\\nsearch directions. This gives rise to the zig-zag pattern of progress, where by\\ndescending to the minimum in the current gradient direction, we must re-minimize\\nthe objective in the previous gradient direction. Thus, by following the gradient at\\nthe end of each line search we are, in a sense, undoing progress we have already\\nmade in the direction of the previous line search. The method of conjugate gradients\\nseeks to address this problem.\\nIn the method of conjugate gradients, we seek to find a search direction that\\nis conjugate to the previous line search direction, i.e. it will not undo progress\\nmade in that direction. At training iteration t, the next search direction d takes\\nt\\n313\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\n\\ue032\\ue030\\n\\ue031\\ue030\\n\\ue030\\n\\uf091\\ue031\\ue030\\n\\uf091\\ue032\\ue030\\n\\uf091\\ue033\\ue030\\n\\uf091\\ue033\\ue030 \\uf091\\ue032\\ue030 \\uf091\\ue031\\ue030 \\ue030 \\ue031\\ue030 \\ue032\\ue030\\nFigure 8.6: The method of steepest descent applied to a quadratic cost surface. The'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='method of steepest descent involves jumping to the point of lowest cost along the line\\ndefined by the gradient at the initial pointon each step. This resolves some of the problems\\nseen with using a fixed learning rate in figure 4.6, but even with the optimal step size\\nthe algorithm still makes back-and-forth progress toward the optimum. By definition, at\\nthe minimum of the objective along a given direction, the gradient at the final point is\\northogonal to that direction.\\nthe form:\\nd = J(θ)+ β d (8.29)\\nt θ t t 1\\n∇ −\\nwhere β is a coefficient whose magnitude controls how much of the direction, d ,\\nt t 1\\n−\\nwe should add back to the current search direction.\\nTwo directions, d and d , are defined as conjugate if d Hd = 0, where\\nt t 1 \\ue03et t 1\\n− −\\nH is the Hessian matrix.\\nThe straightforward way to impose conjugacy would involve calculation of the\\neigenvectors of H to choose β , which would not satisfy our goal of developing\\nt'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a method that is more computationally viable than Newton’s method for large\\nproblems. Can we calculate the conjugate directions without resorting to these\\ncalculations? Fortunately the answer to that is yes.\\nTwo popular methods for computing the β are:\\nt\\n1. Fletcher-Reeves:\\nJ(θ ) J(θ )\\nθ t \\ue03e θ t\\nβ = ∇ ∇ (8.30)\\nt\\nJ(θ ) J(θ )\\nθ t 1 \\ue03e θ t 1\\n∇ − ∇ −\\n314\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\n2. Polak-Ribière:\\n( J(θ ) J(θ )) J(θ )\\nθ t θ t 1 \\ue03e θ t\\nβ t = ∇ − ∇ − ∇ (8.31)\\nJ(θ ) J(θ )\\nθ t 1 \\ue03e θ t 1\\n∇ − ∇ −\\nFor a quadratic surface, the conjugate directions ensure that the gradient along\\nthe previous direction does not increase in magnitude. We therefore stay at the\\nminimum along the previous directions. As a consequence, in a k-dimensional\\nparameter space, the conjugate gradient method requires at most k line searches to\\nachieve the minimum. The conjugate gradient algorithm is given in algorithm 8.9.\\nAlgorithm 8.9 The conjugate gradient method\\nRequire: Initial parameters θ\\n0'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Require: Training set of m examples\\nInitialize ρ = 0\\n0\\nInitialize g = 0\\n0\\nInitialize t = 1\\nwhile stopping criterion not met do\\nInitialize the gradient g = 0\\nt\\nCompute gradient: g 1 L(f(x(i);θ),y(i))\\nt ← m∇θ i\\nCompute β = (g t g t 1) \\ue03eg t (Polak-Ribière)\\nt − − \\ue050\\ng g\\n\\ue03et 1 t 1\\n(Nonlinear conjuga−te g− radient: optionally reset β to zero, for example if t is\\nt\\na multiple of some constant k, such as k = 5)\\nCompute search direction: ρ = g +β ρ\\nt t t t 1\\nPerform line search to find: \\ue00f =− argmin 1− m L(f(x(i);θ +\\ue00fρ ),y(i))\\n∗ \\ue00f m i=1 t t\\n(On a truly quadratic cost function, analytically solve for \\ue00f rather than\\n∗\\n\\ue050\\nexplicitly searching for it)\\nApply update: θ = θ +\\ue00f ρ\\nt+1 t ∗ t\\nt t+ 1\\n←\\nend while\\nNonlinear Conjugate Gradients: So far we have discussed the method of\\nconjugate gradients as it is applied to quadratic objective functions. Of course,\\nour primary interest in this chapter is to explore optimization methods for training'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='neural networks and other related deep learning models where the corresponding\\nobjective function is far from quadratic. Perhaps surprisingly, the method of\\nconjugate gradients is still applicable in this setting, though with some modification.\\nWithout any assurance that the objective is quadratic, the conjugate directions\\n315\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nare no longer assured to remain at the minimum of the objective for previous\\ndirections. As a result, the nonlinear conjugate gradients algorithm includes\\noccasional resets where the method of conjugate gradients is restarted with line\\nsearch along the unaltered gradient.\\nPractitioners report reasonable results in applications of the nonlinear conjugate\\ngradients algorithm to training neural networks, though it is often beneficial to\\ninitialize the optimization with a few iterations of stochastic gradient descent before\\ncommencing nonlinear conjugate gradients. Also, while the (nonlinear) conjugate'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='gradients algorithm has traditionally been cast as a batch method, minibatch\\nversions have been used successfully for the training of neural networks (Le et al.,\\n2011). Adaptations of conjugate gradients specifically for neural networks have\\nbeen proposed earlier, such as the scaled conjugate gradients algorithm (Moller,\\n1993).\\n8.6.3 BFGS\\nThe Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm attempts to\\nbring some of the advantages of Newton’s method without the computational\\nburden. In that respect, BFGS is similar to the conjugate gradient method.\\nHowever, BFGS takes a more direct approach to the approximation of Newton’s\\nupdate. Recall that Newton’s update is given by\\nθ = θ H 1 J(θ ), (8.32)\\n∗ 0 − θ 0\\n− ∇\\nwhere H is the Hessian of J with respect to θ evaluated at θ . The primary\\n0\\ncomputational difficulty in applying Newton’s update is the calculation of the\\ninverse Hessian H 1. The approach adopted by quasi-Newton methods (of which\\n−'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the BFGS algorithm is the most prominent) is to approximate the inverse with\\na matrix M that is iteratively refined by low rank updates to become a better\\nt\\napproximation of H 1.\\n−\\nThe specification and derivation of the BFGS approximation is given in many\\ntextbooks on optimization, including Luenberger (1984).\\nOnce the inverse Hessian approximation M is updated, the direction of descent\\nt\\nρ is determined by ρ = M g . A line search is performed in this direction to\\nt t t t\\ndetermine the size of the step, \\ue00f , taken in this direction. The final update to the\\n∗\\nparameters is given by:\\nθ = θ +\\ue00f ρ . (8.33)\\nt+1 t ∗ t\\nLike the method of conjugate gradients, the BFGS algorithm iterates a series of\\nline searches with the direction incorporating second-order information. However\\n316\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nunlike conjugate gradients, the success of the approach is not heavily dependent\\non the line search finding a point very close to the true minimum along the line.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Thus, relative to conjugate gradients, BFGS has the advantage that it can spend\\nless time refining each line search. On the other hand, the BFGS algorithm must\\nstore the inverse Hessian matrix, M, that requires O(n2) memory, making BFGS\\nimpractical for most modern deep learning models that typically have millions of\\nparameters.\\nLimited Memory BFGS (or L-BFGS) The memory costs of the BFGS\\nalgorithm can be significantly decreased by avoiding storing the complete inverse\\nHessianapproximationM. The L-BFGSalgorithmcomputes the approximation M\\nusing the same method as the BFGS algorithm, but beginning with the assumption\\nthat M(t 1) is the identity matrix, rather than storing the approximation from one\\n−\\nstep to the next. If used with exact line searches, the directions defined by L-BFGS\\nare mutually conjugate. However, unlike the method of conjugate gradients, this\\nprocedure remains well behaved when the minimum of the line search is reached'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='only approximately. The L-BFGS strategy with no storage described here can be\\ngeneralized to include more information about the Hessian by storing some of the\\nvectors used to update M at each time step, which costs only O(n) per step.\\n8.7 Optimization Strategies and Meta-Algorithms\\nMany optimization techniques are not exactly algorithms, but rather general\\ntemplates that can be specialized to yield algorithms, or subroutines that can be\\nincorporated into many different algorithms.\\n8.7.1 Batch Normalization\\nBatch normalization (Ioffe and Szegedy, 2015) is one of the most exciting recent\\ninnovationsin optimizing deep neural networksand it is actually notan optimization\\nalgorithm at all. Instead, it is a method of adaptive reparametrization, motivated\\nby the difficulty of training very deep models.\\nVery deep models involve the composition of several functions or layers. The\\ngradient tells how to update each parameter, under the assumption that the other'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='layers do not change. In practice, we update all of the layers simultaneously.\\nWhen we make the update, unexpected results can happen because many functions\\ncomposed together are changed simultaneously, using updates that were computed\\nunder the assumption that the other functions remain constant. As a simple\\n317\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nexample, suppose we have a deep neural network that has only one unit per layer\\nand does not use an activation function at each hidden layer: yˆ = xw w w ...w .\\n1 2 3 l\\nHere, w provides the weight used by layer i. The output of layer i is h = h w .\\ni i i 1 i\\n−\\nThe output yˆ is a linear function of the input x, but a nonlinear function of the\\nweights w . Suppose our cost function has put a gradient of 1 on yˆ, so we wish to\\ni\\ndecrease yˆ slightly. The back-propagation algorithm can then compute a gradient\\ng = yˆ. Consider what happens when we make an update w w \\ue00fg. The\\nw\\n∇ ← −'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='first-order Taylor series approximation ofyˆ predicts that the value ofyˆwill decrease\\nby \\ue00fg g. If we wanted to decrease yˆ by .1, this first-order information available in\\n\\ue03e\\nthe gradient suggests we could set the learning rate \\ue00f to .1 . However, the actual\\ng g\\n\\ue03e\\nupdate will include second-order and third-order effects, on up to effects of order l.\\nThe new value of yˆ is given by\\nx(w \\ue00fg )(w \\ue00fg )...(w \\ue00fg ). (8.34)\\n1 1 2 2 l l\\n− − −\\nAn example of one second-order term arising from this update is \\ue00f2g g l w .\\n1 2 i=3 i\\nl\\nThis term might be negligible if w is small, or might be exponentially large\\ni=3 i \\ue051\\nif the weights on layers 3 through l are greater than 1. This makes it very hard\\n\\ue051\\nto choose an appropriate learning rate, because the effects of an update to the\\nparameters for one layer depends so strongly on all of the other layers. Second-order\\noptimization algorithms address this issue by computing an update that takes these'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='second-order interactions into account, but we can see that in very deep networks,\\neven higher-order interactions can be significant. Even second-order optimization\\nalgorithms are expensive and usually require numerous approximations that prevent\\nthem from truly accounting for all significant second-order interactions. Building\\nan n-th order optimization algorithm for n > 2 thus seems hopeless. What can we\\ndo instead?\\nBatch normalization providesan elegantway of reparametrizing almost anydeep\\nnetwork. The reparametrization significantly reduces the problem of coordinating\\nupdates across many layers. Batch normalization can be applied to any input\\nor hidden layer in a network. Let H be a minibatch of activations of the layer\\nto normalize, arranged as a design matrix, with the activations for each example\\nappearing in a row of the matrix. To normalize H, we replace it with\\nH µ\\nH = − , (8.35)\\n\\ue030\\nσ\\nwhere µ is a vector containing the mean of each unit and σ is a vector containing'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the standard deviation of each unit. The arithmetic here is based on broadcasting\\nthe vector µ and the vector σ to be applied to every row of the matrix H. Within\\neach row, the arithmetic is element-wise, so H is normalized by subtracting µ\\ni,j j\\n318\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nand dividing by σ . The rest of the network then operates on H in exactly the\\nj \\ue030\\nsame way that the original network operated on H.\\nAt training time,\\n1\\nµ = H (8.36)\\ni,:\\nm\\ni\\n\\ue058\\nand\\n1\\n2\\nσ = δ + (H µ) , (8.37)\\nm − i\\n\\ue073\\ni\\n\\ue058\\nwhere δ is a small positive value such as 10 8 imposed to avoid encountering\\n−\\nthe undefined gradient of √z at z = 0. Crucially, we back-propagate through\\nthese operations for computing the mean and the standard deviation, and for\\napplying them to normalize H. This means that the gradient will never propose\\nan operation that acts simply to increase the standard deviation or mean of\\nh ; the normalization operations remove the effect of such an action and zero\\ni'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='out its component in the gradient. This was a major innovation of the batch\\nnormalization approach. Previous approaches had involved adding penalties to\\nthe cost function to encourage units to have normalized activation statistics or\\ninvolved intervening to renormalize unit statistics after each gradient descent step.\\nThe former approach usually resulted in imperfect normalization and the latter\\nusually resulted in significant wasted time as the learning algorithm repeatedly\\nproposed changing the mean and variance and the normalization step repeatedly\\nundid this change. Batch normalization reparametrizes the model to make some\\nunits always be standardized by definition, deftly sidestepping both problems.\\nAt test time, µ and σ may be replaced by running averages that were collected\\nduring training time. This allows the model to be evaluated on a single example,\\nwithout needing to use definitions of µ and σ that depend on an entire minibatch.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Revisiting the yˆ= xw w ...w example, we see that we can mostly resolve the\\n1 2 l\\ndifficulties in learning this model by normalizing h . Suppose that x is drawn\\nl 1\\n−\\nfrom a unit Gaussian. Then h will also come from a Gaussian, because the\\nl 1\\n−\\ntransformation from x to h is linear. However, h will no longer have zero mean\\nl l 1\\n−\\nand unit variance. After applying batch normalization, we obtain the normalized\\nˆ\\nh that restores the zero mean and unit variance properties. For almost any\\nl 1\\n− ˆ\\nupdate to the lower layers, h will remain a unit Gaussian. The output yˆ may\\nl 1\\n− ˆ\\nthen be learned as a simple linear function yˆ= wh . Learning in this model is\\nl l 1\\n−\\nnow very simple because the parameters at the lower layers simply do not have an\\neffect in most cases; their output is always renormalized to a unit Gaussian. In\\nsome corner cases, the lower layers can have an effect. Changing one of the lower\\nlayer weights to 0 can make the output become degenerate, and changing the sign\\n319'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nˆ\\nof one of the lower weights can flip the relationship between h and y. These\\nl 1\\n−\\nsituations are very rare. Without normalization, nearly every update would have\\nan extreme effect on the statistics of h . Batch normalization has thus made\\nl 1\\n−\\nthis model significantly easier to learn. In this example, the ease of learning of\\ncourse came at the cost of making the lower layers useless. In our linear example,\\nthe lower layers no longer have any harmful effect, but they also no longer have\\nany beneficial effect. This is because we have normalized out the first and second\\norder statistics, which is all that a linear network can influence. In a deep neural\\nnetwork with nonlinear activation functions, the lower layers can perform nonlinear\\ntransformations of the data, so they remain useful. Batch normalization acts to\\nstandardize only the mean and variance of each unit in order to stabilize learning,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='but allows the relationships between units and the nonlinear statistics of a single\\nunit to change.\\nBecause the final layer of the network is able to learn a linear transformation,\\nwe may actually wish to remove all linear relationships between units within a\\nlayer. Indeed, this is the approach taken by Desjardins et al. (2015), who provided\\nthe inspiration for batch normalization. Unfortunately, eliminating all linear\\ninteractions is much more expensive than standardizing the mean and standard\\ndeviation of each individual unit, and so far batch normalization remains the most\\npractical approach.\\nNormalizing the mean and standard deviation of a unit can reducethe expressive\\npower of the neural network containing that unit. In order to maintain the\\nexpressive power of the network, it is common to replace the batch of hidden unit\\nactivations H with γH +β rather than simply the normalized H . The variables\\n\\ue030 \\ue030\\nγ and β are learned parameters that allow the new variable to have any mean'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='and standard deviation. At first glance, this may seem useless—why did we set\\nthe mean to 0, and then introduce a parameter that allows it to be set back to\\nany arbitrary value β? The answer is that the new parametrization can represent\\nthe same family of functions of the input as the old parametrization, but the new\\nparametrization has different learning dynamics. In the old parametrization, the\\nmean of H was determined by a complicated interaction between the parameters\\nin the layers below H. In the new parametrization, the mean of γH + β is\\n\\ue030\\ndetermined solely by β. The new parametrization is much easier to learn with\\ngradient descent.\\nMost neural network layers take the form of φ(XW + b) where φ is some\\nfixed nonlinear activation function such as the rectified linear transformation. It\\nis natural to wonder whether we should apply batch normalization to the input\\nX, or to the transformed value XW +b. Ioffe and Szegedy (2015) recommend\\n320'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nthe latter. More specifically, XW +b should be replaced by a normalized version\\nof XW. The bias term should be omitted because it becomes redundant with\\nthe β parameter applied by the batch normalization reparametrization. The input\\nto a layer is usually the output of a nonlinear activation function such as the\\nrectified linear function in a previous layer. The statistics of the input are thus\\nmore non-Gaussian and less amenable to standardization by linear operations.\\nIn convolutional networks, described in chapter 9, it is important to apply the\\nsame normalizing µ and σ at every spatial location within a feature map, so that\\nthe statistics of the feature map remain the same regardless of spatial location.\\n8.7.2 Coordinate Descent\\nIn some cases, it may be possible to solve an optimization problem quickly by\\nbreaking it into separate pieces. If we minimize f(x) with respect to a single'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='variable x , then minimize it with respect to another variable x and so on,\\ni j\\nrepeatedly cycling through all variables, we are guaranteed to arrive at a (local)\\nminimum. This practice is known as coordinate descent, because we optimize\\none coordinate at a time. More generally, block coordinate descent refers to\\nminimizing with respect to a subset of the variables simultaneously. The term\\n“coordinate descent” is often used to refer to block coordinate descent as well as\\nthe strictly individual coordinate descent.\\nCoordinate descent makes the most sense when the different variables in the\\noptimization problem can be clearly separated into groups that play relatively\\nisolated roles, or when optimization with respect to one group of variables is\\nsignificantly more efficient than optimization with respect to all of the variables.\\nFor example, consider the cost function\\n2\\nJ(H,W) = H + X W H . (8.38)\\ni,j \\ue03e\\n| | − i,j\\n\\ue058i,j \\ue058i,j \\ue010 \\ue011'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='This function describes a learning problem called sparse coding, where the goal is\\nto find a weight matrix W that can linearly decode a matrix of activation values\\nH to reconstruct the training set X. Most applications of sparse coding also\\ninvolve weight decay or a constraint on the norms of the columns of W, in order\\nto prevent the pathological solution with extremely small H and large W.\\nThe function J is not convex. However, we can divide the inputs to the\\ntraining algorithm into two sets: the dictionary parameters W and the code\\nrepresentations H. Minimizing the objective function with respect to either one of\\nthese sets of variables is a convex problem. Block coordinate descent thus gives\\n321\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nus an optimization strategy that allows us to use efficient convex optimization\\nalgorithms, by alternating between optimizing W with H fixed, then optimizing\\nH with W fixed.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Coordinate descent is not a very good strategy when the value of one variable\\nstrongly influences the optimal value of another variable, as in the function f(x) =\\n(x x )2 +α x2 +x2 where α is a positive constant. The first term encourages\\n1 2 1 2\\n−\\nthe two variables to have similar value, while the second term encourages them\\n\\ue000 \\ue001\\nto be near zero. The solution is to set both to zero. Newton’s method can solve\\nthe problem in a single step because it is a positive definite quadratic problem.\\nHowever, for small α, coordinate descent will make very slow progress because the\\nfirst term does not allow a single variable to be changed to a value that differs\\nsignificantly from the current value of the other variable.\\n8.7.3 Polyak Averaging\\nPolyak averaging (Polyak and Juditsky, 1992) consists of averaging together several\\npoints in the trajectory through parameter space visited by an optimization\\nalgorithm. If t iterations of gradient descent visit points θ(1),...,θ(t), then the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='output of the Polyak averaging algorithm is θˆ(t) = 1 θ(i). On some problem\\nt i\\nclasses, such as gradient descent applied to convex problems, this approach has\\n\\ue050\\nstrong convergence guarantees. When applied to neural networks, its justification\\nis more heuristic, but it performs well in practice. The basic idea is that the\\noptimization algorithm may leap back and forth across a valley several times\\nwithout ever visiting a point near the bottom of the valley. The average of all of\\nthe locations on either side should be close to the bottom of the valley though.\\nIn non-convex problems, the path taken by the optimization trajectory can be\\nvery complicated and visit many different regions. Including points in parameter\\nspace from the distant past that may be separated from the current point by large\\nbarriers in the cost function does not seem like a useful behavior. As a result,\\nwhen applying Polyak averaging to non-convex problems, it is typical to use an'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='exponentially decaying running average:\\nθˆ(t) = αθˆ(t 1) +(1 α)θ(t). (8.39)\\n−\\n−\\nThe running average approach is used in numerous applications. See Szegedy\\net al. (2015) for a recent example.\\n322\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\n8.7.4 Supervised Pretraining\\nSometimes, directly training a model to solve a specific task can be too ambitious\\nif the model is complex and hard to optimize or if the task is very difficult. It is\\nsometimes more effective to train a simpler model to solve the task, then make\\nthe model more complex. It can also be more effective to train the model to solve\\na simpler task, then move on to confront the final task. These strategies that\\ninvolve training simple models on simple tasks before confronting the challenge of\\ntraining the desired model to perform the desired task are collectively known as\\npretraining.\\nGreedy algorithms break a problem into many components, then solve for'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the optimal version of each component in isolation. Unfortunately, combining the\\nindividually optimal components is not guaranteed to yield an optimal complete\\nsolution. However, greedy algorithms can be computationally much cheaper than\\nalgorithms that solve for the best joint solution, and the quality of a greedy solution\\nis often acceptable if not optimal. Greedy algorithms may also be followed by a\\nfine-tuning stage in which a joint optimization algorithm searches for an optimal\\nsolution to the full problem. Initializing the joint optimization algorithm with a\\ngreedy solution can greatly speed it up and improve the quality of the solution it\\nfinds.\\nPretraining, and especially greedy pretraining, algorithms are ubiquitous in\\ndeep learning. In this section, we describe specifically those pretraining algorithms\\nthat break supervised learning problems into other simpler supervised learning\\nproblems. This approach is known as greedy supervised pretraining.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In the original (Bengio et al., 2007) version of greedy supervised pretraining,\\neach stage consists of a supervised learning training task involving only a subset of\\nthe layers in the final neural network. An example of greedy supervised pretraining\\nis illustrated in figure 8.7, in which each added hidden layer is pretrained as part\\nof a shallow supervised MLP, taking as input the output of the previously trained\\nhidden layer. Instead of pretraining one layer at a time, Simonyan and Zisserman\\n(2015) pretrain a deep convolutional network (eleven weight layers) and then use\\nthe first four and last three layers from this network to initialize even deeper\\nnetworks (with up to nineteen layers of weights). The middle layers of the new,\\nvery deep network are initialized randomly. The new network is then jointly trained.\\nAnother option, explored by Yu et al. (2010) is to use the outputs of the previously\\ntrained MLPs, as well as the raw input, as inputs for each added stage.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Why would greedy supervised pretraining help? The hypothesis initially\\ndiscussed by Bengio et al. (2007) is that it helps to provide better guidance to the\\n323\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nyy\\nUU((11))\\nhh((11)) hh((11))\\nWW((11)) WW((11)) UU((11)) yy\\nxx xx\\n(a) (b)\\ny\\nUU((22))\\nhh((22)) hh((22))\\nWW((22)) UU((22)) yy WW((22))\\nhh((11)) hh((11))\\nWW((11)) UU((11)) yy WW((11)) UU((11)) yy\\nxx xx\\n(c) (d)\\nFigure 8.7: Illustration of one form of greedy supervised pretraining (Bengio et al., 2007).\\n(a)We start by training a sufficiently shallow architecture. (b)Another drawing of the\\nsame architecture. (c)We keep only the input-to-hidden layer of the original network and\\ndiscard the hidden-to-output layer. We send the output of the first hidden layer as input\\nto another supervised single hidden layer MLP that is trained with the same objective\\nas the first network was, thus adding a second hidden layer. This can be repeated for as'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='many layers as desired. (d)Another drawing of the result, viewed as a feedforward network.\\nTo further improve the optimization, we can jointly fine-tune all the layers, either only at\\nthe end or at each stage of this process.\\n324\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nintermediate levels of a deep hierarchy. In general, pretraining may help both in\\nterms of optimization and in terms of generalization.\\nAn approach related to supervised pretraining extends the idea to the context\\nof transfer learning: Yosinski et al. (2014) pretrain a deep convolutional net with 8\\nlayers of weights on a set of tasks (a subset of the 1000 ImageNet object categories)\\nand then initialize a same-size network with the first k layers of the first net. All\\nthe layers of the second network (with the upper layers initialized randomly) are\\nthen jointly trained to perform a different set of tasks (another subset of the 1000'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='ImageNet object categories), with fewer training examples than for the first set of\\ntasks. Other approaches to transfer learning with neural networks are discussed in\\nsection 15.2.\\nAnother related line of work is the FitNets (Romero et al., 2015) approach.\\nThis approach begins by training a network that has low enough depth and great\\nenough width (number of units per layer) to be easy to train. This network then\\nbecomes a teacher for a second network, designated the student. The student\\nnetwork is much deeper and thinner (eleven to nineteen layers) and would be\\ndifficult to train with SGD under normal circumstances. The training of the\\nstudent network is made easier by training the student network not only to predict\\nthe output for the original task, but also to predict the value of the middle layer\\nof the teacher network. This extra task provides a set of hints about how the\\nhidden layers should be used and can simplify the optimization problem. Additional'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='parameters are introduced to regress the middle layer of the 5-layer teacher network\\nfrom the middle layer of the deeper student network. However, instead of predicting\\nthe final classification target, the objective is to predict the middle hidden layer\\nof the teacher network. The lower layers of the student networks thus have two\\nobjectives: to help the outputs of the student network accomplish their task, as\\nwell as to predict the intermediate layer of the teacher network. Although a thin\\nand deep network appears to be more difficult to train than a wide and shallow\\nnetwork, the thin and deep network may generalize better and certainly has lower\\ncomputational cost if it is thin enough to have far fewer parameters. Without\\nthe hints on the hidden layer, the student network performs very poorly in the\\nexperiments, both on the training and test set. Hints on middle layers may thus\\nbe one of the tools to help train neural networks that otherwise seem difficult to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='train, but other optimization techniques or changes in the architecture may also\\nsolve the problem.\\n325\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\n8.7.5 Designing Models to Aid Optimization\\nTo improveoptimization, the best strategy is not always to improve the optimization\\nalgorithm. Instead, many improvements in the optimization of deep models have\\ncome from designing the models to be easier to optimize.\\nIn principle, we could use activation functions that increase and decrease in\\njagged non-monotonic patterns. However, this would make optimization extremely\\ndifficult. In practice, it is more important to choose a model family that is easy to\\noptimize than to use a powerful optimization algorithm. Most of the advances in\\nneural network learning over the past 30 years have been obtained by changing\\nthe model family rather than changing the optimization procedure. Stochastic\\ngradient descent with momentum, which was used to train neural networks in the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='1980s, remains in use in modern state of the art neural network applications.\\nSpecifically, modern neural networks reflect a design choice to use linear trans-\\nformations between layers and activation functions that are differentiable almost\\neverywhere and have significant slope in large portions of their domain. In par-\\nticular, model innovations like the LSTM, rectified linear units and maxout units\\nhave all moved toward using more linear functions than previous models like deep\\nnetworks based on sigmoidal units. These models have nice properties that make\\noptimization easier. The gradient flows through many layers provided that the\\nJacobian of the linear transformation has reasonable singular values. Moreover,\\nlinear functions consistently increase in a single direction, so even if the model’s\\noutput is very far from correct, it is clear simply from computing the gradient\\nwhich direction its output should move to reduce the loss function. In other words,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='modern neural nets have been designed so that their local gradient information\\ncorresponds reasonably well to moving toward a distant solution.\\nOther model design strategies can help to make optimization easier. For\\nexample, linear paths or skip connections between layers reduce the length of\\nthe shortest path from the lower layer’s parameters to the output, and thus\\nmitigate the vanishing gradient problem (Srivastava et al., 2015). A related idea\\nto skip connections is adding extra copies of the output that are attached to the\\nintermediate hidden layers of the network, as in GoogLeNet (Szegedy et al., 2014a)\\nand deeply-supervised nets (Lee et al., 2014). These “auxiliary heads” are trained\\nto perform the same task as the primary output at the top of the network in order\\nto ensure that the lower layers receive a large gradient. When training is complete\\nthe auxiliary heads may be discarded. This is an alternative to the pretraining'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='strategies, which were introduced in the previous section. In this way, one can\\ntrain jointly all the layers in a single phase but change the architecture, so that\\nintermediate layers (especially the lower ones) can get some hints about what they\\n326\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nshould do, via a shorter path. These hints provide an error signal to lower layers.\\n8.7.6 Continuation Methods and Curriculum Learning\\nAs argued in section 8.2.7, many of the challenges in optimization arise from the\\nglobal structure of the cost function and cannot be resolved merely by making better\\nestimates of local update directions. The predominant strategy for overcoming this\\nproblem is to attempt to initialize the parameters in a region that is connected\\nto the solution by a short path through parameter space that local descent can\\ndiscover.\\nContinuation methods are a family of strategies that can make optimization'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='easier by choosing initial points to ensure that local optimization spends most of\\nits time in well-behaved regions of space. The idea behind continuation methods is\\nto construct a series of objective functions over the same parameters. In order to\\nminimize a cost function J(θ), we will construct new cost functions J(0),...,J(n) .\\n{ }\\nThese cost functions are designed to be increasingly difficult, with J(0) being fairly\\neasy to minimize, and J(n), the most difficult, being J(θ), the true cost function\\nmotivating the entire process. When we say that J(i) is easier than J(i+1), we\\nmean that it is well behaved over more of θ space. A random initialization is more\\nlikely to land in the region where local descent can minimize the cost function\\nsuccessfully because this region is larger. The series of cost functions are designed\\nso that a solution to one is a good initial point of the next. We thus begin by\\nsolving an easy problem then refine the solution to solve incrementally harder'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='problems until we arrive at a solution to the true underlying problem.\\nTraditional continuation methods (predating the use of continuation methods\\nfor neural network training) are usually based on smoothing the objective function.\\nSee Wu (1997) for an example of such a method and a review of some related\\nmethods. Continuation methods are also closely related to simulated annealing,\\nwhich adds noise to the parameters (Kirkpatrick et al., 1983). Continuation\\nmethods have been extremely successful in recent years. See Mobahi and Fisher\\n(2015) for an overview of recent literature, especially for AI applications.\\nContinuation methods traditionally were mostly designed with the goal of\\novercoming the challenge of local minima. Specifically, they were designed to\\nreach a global minimum despite the presence of many local minima. To do so,\\nthese continuation methods would construct easier cost functions by “blurring” the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='original cost function. This blurring operation can be done by approximating\\nJ(i)(θ) = E J(θ ) (8.40)\\nθ (θ ;θ,σ(i)2) \\ue030\\n\\ue030∼N \\ue030\\nvia sampling. The intuition for this approach is that some non-convex functions\\n327\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nbecome approximately convex when blurred. In many cases, this blurring preserves\\nenough information about the location of a global minimum that we can find the\\nglobal minimum by solving progressively less blurred versions of the problem. This\\napproach can break down in three different ways. First, it might successfully define\\na series of cost functions where the first is convex and the optimum tracks from\\none function to the next arriving at the global minimum, but it might require so\\nmany incremental cost functions that the cost of the entire procedure remains high.\\nNP-hard optimization problems remain NP-hard, even when continuation methods\\nare applicable. The other two ways that continuation methods fail both correspond'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to the method not being applicable. First, the function might not become convex,\\nno matter how much it is blurred. Consider for example the function J(θ) = θ θ.\\n\\ue03e\\n−\\nSecond, the function may become convex as a result of blurring, but the minimum\\nof this blurred function may track to a local rather than a global minimum of the\\noriginal cost function.\\nThough continuation methods were mostly originally designed to deal with the\\nproblem of local minima, local minima are no longer believed to be the primary\\nproblem for neural network optimization. Fortunately, continuation methods can\\nstill help. The easier objective functions introduced by the continuation method can\\neliminate flat regions, decrease variance in gradient estimates, improve conditioning\\nof the Hessian matrix, or do anything else that will either make local updates\\neasier to compute or improve the correspondence between local update directions\\nand progress toward a global solution.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Bengio et al. (2009) observed that an approach called curriculum learning\\nor shaping can be interpreted as a continuation method. Curriculum learning is\\nbased on the idea of planning a learning process to begin by learning simple concepts\\nand progress to learning more complex concepts that depend on these simpler\\nconcepts. This basic strategy was previously known to accelerate progress in animal\\ntraining (Skinner, 1958; Peterson, 2004; Krueger and Dayan, 2009) and machine\\nlearning (Solomonoff, 1989; Elman, 1993; Sanger, 1994). Bengio et al. (2009)\\njustified this strategy as a continuation method, where earlier J(i) are made easier by\\nincreasing the influence of simpler examples (either by assigning their contributions\\nto the cost function larger coefficients, or by sampling them more frequently), and\\nexperimentally demonstrated that better results could be obtained by following a\\ncurriculum on a large-scale neural language modeling task. Curriculum learning'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='has been successful on a wide range of natural language (Spitkovsky et al., 2010;\\nCollobert et al., 2011a; Mikolov et al., 2011b; Tu and Honavar, 2011) and computer\\nvision (Kumar et al., 2010; Lee and Grauman, 2011; Supancic and Ramanan, 2013)\\ntasks. Curriculum learning was also verified as being consistent with the way in\\nwhich humans teach (Khan et al., 2011): teachers start by showing easier and\\n328\\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\\nmore prototypical examples and then help the learner refine the decision surface\\nwith the less obvious cases. Curriculum-based strategies are more effective for\\nteaching humans than strategies based on uniform sampling of examples, and can\\nalso increase the effectiveness of other teaching strategies (Basu and Christensen,\\n2013).\\nAnother important contribution to research on curriculum learning arose in the\\ncontext of training recurrent neural networks to capture long-term dependencies:'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Zaremba and Sutskever (2014) found that much better results were obtained with a\\nstochastic curriculum, in whicha randommix of easy and difficultexamples is always\\npresented to the learner, but where the average proportion of the more difficult\\nexamples (here, those with longer-term dependencies) is gradually increased. With\\na deterministic curriculum, no improvement over the baseline (ordinary training\\nfrom the full training set) was observed.\\nWe have now described the basic family of neural network models and how to\\nregularize and optimize them. In the chapters ahead, we turn to specializations of\\nthe neural network family, that allow neural networks to scale to very large sizes and\\nprocess input data that has special structure. The optimization methods discussed\\nin this chapter are often directly applicable to these specialized architectures with\\nlittle or no modification.\\n329\\nChapter 9\\nConvolutional Networks\\nConvolutional networks (LeCun, 1989), also known as convolutional neural'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='networks or CNNs, are a specialized kind of neural network for processing data\\nthat has a known, grid-like topology. Examples include time-series data, which can\\nbe thought of as a 1D grid taking samples at regular time intervals, and image data,\\nwhich can be thought of as a 2D grid of pixels. Convolutional networks have been\\ntremendously successful in practical applications. The name “convolutional neural\\nnetwork” indicates that the network employs a mathematical operation called\\nconvolution. Convolution is a specialized kind of linear operation. Convolutional\\nnetworks are simply neural networks that use convolution in place of general matrix\\nmultiplication in at least one of their layers.\\nIn this chapter, we will first describe what convolution is. Next, we will\\nexplain the motivation behind using convolution in a neural network. We will then\\ndescribe an operation called pooling, which almost all convolutional networks'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='employ. Usually, the operation used in a convolutional neural network does not\\ncorrespond precisely to the definition of convolution as used in other fields such\\nas engineering or pure mathematics. We will describe several variants on the\\nconvolution function that are widely used in practice for neural networks. We\\nwill also show how convolution may be applied to many kinds of data, with\\ndifferent numbers of dimensions. We then discuss means of making convolution\\nmore efficient. Convolutional networks stand out as an example of neuroscientific\\nprinciples influencing deep learning. We will discuss these neuroscientific principles,\\nthen conclude with comments about the role convolutional networks have played\\nin the history of deep learning. One topic this chapter does not address is how to\\nchoose the architecture of your convolutional network. The goal of this chapter is\\nto describe the kinds of tools that convolutional networks provide, while chapter 11\\n330'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 9. CONVOLUTIONAL NETWORKS\\ndescribes general guidelines for choosing which tools to use in which circumstances.\\nResearch into convolutional network architectures proceeds so rapidly that a new\\nbest architecture for a given benchmark is announced every few weeks to months,\\nrendering it impractical to describe the best architecture in print. However, the\\nbest architectures have consistently been composed of the building blocks described\\nhere.\\n9.1 The Convolution Operation\\nIn its most general form, convolution is an operation on two functions of a real-\\nvalued argument. To motivate the definition of convolution, we start with examples\\nof two functions we might use.\\nSuppose we are tracking the location of a spaceship with a laser sensor. Our\\nlaser sensor provides a single output x(t), the position of the spaceship at time\\nt. Both x and t are real-valued, i.e., we can get a different reading from the laser\\nsensor at any instant in time.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Now suppose that our laser sensor is somewhat noisy. To obtain a less noisy\\nestimate of the spaceship’s position, we would like to average together several\\nmeasurements. Of course, more recent measurements are more relevant, so we will\\nwant this to be a weighted average that gives more weight to recent measurements.\\nWe can do this with a weighting function w(a), where a is the age of a measurement.\\nIf we apply such a weighted average operation at every moment, we obtain a new\\nfunction s providing a smoothed estimate of the position of the spaceship:\\ns(t) = x(a)w(t a)da (9.1)\\n−\\n\\ue05a\\nThis operation is called convolution. The convolution operation is typically\\ndenoted with an asterisk:\\ns(t) = (x w)(t) (9.2)\\n∗\\nIn our example, w needs to be a valid probability density function, or the\\noutput is not a weighted average. Also, w needs to be 0 for all negative arguments,\\nor it will look into the future, which is presumably beyond our capabilities. These'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='limitations are particular to our example though. In general, convolution is defined\\nfor any functions for which the above integral is defined, and may be used for other\\npurposes besides taking weighted averages.\\nIn convolutional network terminology, the first argument (in this example, the\\nfunction x) to the convolution is often referred to as the input and the second\\n331\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nargument (in this example, the function w) as the kernel. The output is sometimes\\nreferred to as the feature map.\\nIn our example, the idea of a laser sensor that can provide measurements\\nat every instant in time is not realistic. Usually, when we work with data on a\\ncomputer, time will be discretized, and our sensor will provide data at regular\\nintervals. In our example, it might be more realistic to assume that our laser\\nprovides a measurement once per second. The time index t can then take on only\\ninteger values. If we now assume that x and w are defined only on integer t, we'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='can define the discrete convolution:\\n∞\\ns(t) = (x w)(t) = x(a)w(t a) (9.3)\\n∗ −\\na=\\n\\ue058−∞\\nIn machine learning applications, the input is usually a multidimensional array\\nof data and the kernel is usually a multidimensional array of parameters that are\\nadapted by the learning algorithm. We will refer to these multidimensional arrays\\nas tensors. Because each element of the input and kernel must be explicitly stored\\nseparately, we usually assume that these functions are zero everywhere but the\\nfinite set of points for which we store the values. This means that in practice we\\ncan implement the infinite summation as a summation over a finite number of\\narray elements.\\nFinally, we often use convolutions over more than one axis at a time. For\\nexample, if we use a two-dimensional image I as our input, we probably also want\\nto use a two-dimensional kernel K:\\nS(i,j) = (I K)(i,j) = I(m,n)K(i m,j n). (9.4)\\n∗ − −\\nm n\\n\\ue058\\ue058\\nConvolution is commutative, meaning we can equivalently write:'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='S(i,j) = (K I)(i,j) = I(i m,j n)K(m,n). (9.5)\\n∗ − −\\nm n\\n\\ue058\\ue058\\nUsually the latter formula is more straightforward to implement in a machine\\nlearning library, because there is less variation in the range of valid values of m\\nand n.\\nThe commutative property of convolution arises because we have flipped the\\nkernel relative to the input, in the sense that as m increases, the index into the\\ninput increases, but the index into the kernel decreases. The only reason to flip\\nthe kernel is to obtain the commutative property. While the commutative property\\n332\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nis useful for writing proofs, it is not usually an important property of a neural\\nnetwork implementation. Instead, many neural network libraries implement a\\nrelated function called the cross-correlation, which is the same as convolution\\nbut without flipping the kernel:\\nS(i,j) = (I K)(i,j) = I(i +m,j +n)K(m,n). (9.6)\\n∗\\nm n\\n\\ue058\\ue058\\nMany machine learning libraries implement cross-correlation but call it convolution.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In this text we will follow this convention of calling both operations convolution,\\nand specify whether we mean to flip the kernel or not in contexts where kernel\\nflipping is relevant. In the context of machine learning, the learning algorithm will\\nlearn the appropriate values of the kernel in the appropriate place, so an algorithm\\nbased on convolution with kernel flipping will learn a kernel that is flipped relative\\nto the kernel learned by an algorithm without the flipping. It is also rare for\\nconvolution to be used alone in machine learning; instead convolution is used\\nsimultaneously with other functions, and the combination of these functions does\\nnot commute regardless of whether the convolution operation flips its kernel or\\nnot.\\nSee figure 9.1 for an example of convolution (without kernel flipping) applied\\nto a 2-D tensor.\\nDiscrete convolution can be viewed as multiplication by a matrix. However, the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='matrix has several entries constrained to be equal to other entries. For example,\\nfor univariate discrete convolution, each row of the matrix is constrained to be\\nequal to the row above shifted by one element. This is known as a Toeplitz\\nmatrix. In two dimensions, a doubly block circulant matrix corresponds to\\nconvolution. In addition to these constraints that several elements be equal to\\neach other, convolution usually corresponds to a very sparse matrix (a matrix\\nwhose entries are mostly equal to zero). This is because the kernel is usually much\\nsmaller than the input image. Any neural network algorithm that works with\\nmatrix multiplication and does not depend on specific properties of the matrix\\nstructure should work with convolution, without requiring any further changes\\nto the neural network. Typical convolutional neural networks do make use of\\nfurther specializations in order to deal with large inputs efficiently, but these are'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='not strictly necessary from a theoretical perspective.\\n333\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nInput\\nKernel\\na b c d\\nw x\\ne f g h\\ny z\\ni j k l\\nOutput\\naaww ++ bbxx ++ bbww ++ ccxx ++ ccww ++ ddxx ++\\neeyy ++ ffzz ffyy ++ ggzz ggyy ++ hhzz\\neeww ++ ffxx ++ ffww ++ ggxx ++ ggww ++ hhxx ++\\niiyy ++ jjzz jjyy ++ kkzz kkyy ++ llzz\\nFigure 9.1: An example of 2-D convolution without kernel-flipping. In this case we restrict\\nthe output to only positions where the kernel lies entirely within the image, called “valid”\\nconvolution in some contexts. We draw boxes with arrows to indicate how the upper-left\\nelement of the output tensor is formed by applying the kernel to the corresponding\\nupper-left region of the input tensor.\\n334\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\n9.2 Motivation\\nConvolution leverages three important ideas that can help improve a machine\\nlearning system: sparse interactions, parameter sharing and equivariant\\nrepresentations. Moreover, convolution provides a means for working with'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='inputs of variable size. We now describe each of these ideas in turn.\\nTraditional neural network layers use matrix multiplication by a matrix of\\nparameters with a separate parameter describing the interaction between each input\\nunit and each output unit. This means every output unit interacts with every input\\nunit. Convolutional networks, however, typically have sparse interactions (also\\nreferred to as sparse connectivity or sparse weights). This is accomplished by\\nmaking the kernel smaller than the input. For example, when processing an image,\\nthe input image might have thousands or millions of pixels, but we can detect small,\\nmeaningful features such as edges with kernels that occupy only tens or hundreds of\\npixels. This means that we need to store fewer parameters, which both reduces the\\nmemory requirements of the model and improves its statistical efficiency. It also\\nmeans that computing the output requires fewer operations. These improvements'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='in efficiency are usually quite large. If there are m inputs and n outputs, then\\nmatrix multiplication requires m nparameters and the algorithms used in practice\\n×\\nhave O(m n) runtime (per example). If we limit the number of connections\\n×\\neach output may have to k, then the sparsely connected approach requires only\\nk n parameters and O(k n) runtime. For many practical applications, it is\\n× ×\\npossible to obtain good performance on the machine learning task while keeping\\nk several orders of magnitude smaller than m. For graphical demonstrations of\\nsparse connectivity, see figure 9.2 and figure 9.3. In a deep convolutional network,\\nunits in the deeper layers may indirectly interact with a larger portion of the input,\\nas shown in figure 9.4. This allows the network to efficiently describe complicated\\ninteractions between many variables by constructing such interactions from simple\\nbuilding blocks that each describe only sparse interactions.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Parameter sharing refers to using the same parameter for more than one\\nfunction in a model. In a traditional neural net, each element of the weight matrix\\nis used exactly once when computing the output of a layer. It is multiplied by\\none element of the input and then never revisited. As a synonym for parameter\\nsharing, one can say that a network has tied weights, because the value of the\\nweight applied to one input is tied to the value of a weight applied elsewhere. In\\na convolutional neural net, each member of the kernel is used at every position\\nof the input (except perhaps some of the boundary pixels, depending on the\\ndesign decisions regarding the boundary). The parameter sharing used by the\\nconvolution operation means that rather than learning a separate set of parameters\\n335\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nss ss ss ss ss\\n11 22 33 44 55\\nxx xx xx xx xx\\n11 22 33 44 55\\nss ss ss ss ss\\n11 22 33 44 55\\nxx xx xx xx xx\\n11 22 33 44 55'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Figure 9.2: Sparse connectivity, viewed from below: We highlight one input unit, x ,\\n3\\nand also highlight the output units in s that are affected by this unit. (Top)When s is\\nformed by convolution with a kernel of width 3, only three outputs are affected by x.\\n(Bottom)When s is formed by matrix multiplication, connectivity is no longer sparse, so\\nall of the outputs are affected by x .\\n3\\n336\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nss ss ss ss ss\\n11 22 33 44 55\\nxx xx xx xx xx\\n11 22 33 44 55\\nss ss ss ss ss\\n11 22 33 44 55\\nxx xx xx xx xx\\n11 22 33 44 55\\nFigure 9.3: Sparse connectivity, viewed from above: We highlight one output unit, s ,\\n3\\nand also highlight the input units in x that affect this unit. These units are known\\nas the receptive field of s . (Top)When s is formed by convolution with a kernel of\\n3\\nwidth 3, only three inputs affect s . (Bottom)When s is formed by matrix multiplication,\\n3\\nconnectivity is no longer sparse, so all of the inputs affect s .\\n3\\ngg gg gg gg gg\\n11 22 33 44 55'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='hh hh hh hh hh\\n11 22 33 44 55\\nxx xx xx xx xx\\n11 22 33 44 55\\nFigure 9.4: The receptive field of the units in the deeper layers of a convolutional network\\nis larger than the receptive field of the units in the shallow layers. This effect increases if\\nthe network includes architectural features like strided convolution (figure 9.12) or pooling\\n(section 9.3). This means that even though direct connections in a convolutional net are\\nvery sparse, units in the deeper layers can be indirectly connected to all or most of the\\ninput image.\\n337\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nss ss ss ss ss\\n11 22 33 44 55\\nxx xx xx xx xx\\n11 22 33 44 55\\nss ss ss ss ss\\n11 22 33 44 55\\nxx xx xx xx xx\\n11 22 33 44 55\\nFigure 9.5: Parameter sharing: Black arrows indicate the connections that use a particular\\nparameter in two different models. (Top)The black arrows indicate uses of the central\\nelement of a 3-element kernel in a convolutional model. Due to parameter sharing, this'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='single parameter is used at all input locations. (Bottom)The single black arrow indicates\\nthe use of the central element of the weight matrix in a fully connected model. This model\\nhas no parameter sharing so the parameter is used only once.\\nfor every location, we learn only one set. This does not affect the runtime of\\nforward propagation—it is still O(k n)—but it does further reduce the storage\\n×\\nrequirements of the model to k parameters. Recall that k is usually several orders\\nof magnitude less than m. Since m and n are usually roughly the same size, k is\\npractically insignificant compared to m n. Convolution is thus dramatically more\\n×\\nefficient than dense matrix multiplication in terms of the memory requirements\\nand statistical efficiency. For a graphical depiction of how parameter sharing works,\\nsee figure 9.5.\\nAs an example of both of these first two principles in action, figure 9.6 shows\\nhow sparse connectivity and parameter sharing can dramatically improve the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='efficiency of a linear function for detecting edges in an image.\\nIn the case of convolution, the particular form of parameter sharing causes the\\nlayer to have a property called equivariance to translation. To say a function is\\nequivariant means that if the input changes, the output changes in the same way.\\nSpecifically, a function f(x) is equivariant to a function g if f(g(x)) = g(f(x)).\\nIn the case of convolution, if we let g be any function that translates the input,\\ni.e., shifts it, then the convolution function is equivariant to g. For example, let I\\nbe a function giving image brightness at integer coordinates. Let g be a function\\n338\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nmapping one image function to another image function, such that I = g(I) is\\n\\ue030\\nthe image function with I (x,y) = I(x 1,y). This shifts every pixel of I one\\n\\ue030\\n−\\nunit to the right. If we apply this transformation to I, then apply convolution,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the result will be the same as if we applied convolution to I , then applied the\\n\\ue030\\ntransformation g to the output. When processing time series data, this means\\nthat convolution produces a sort of timeline that shows when different features\\nappear in the input. If we move an event later in time in the input, the exact\\nsame representation of it will appear in the output, just later in time. Similarly\\nwith images, convolution creates a 2-D map of where certain features appear in\\nthe input. If we move the object in the input, its representation will move the\\nsame amount in the output. This is useful for when we know that some function\\nof a small number of neighboring pixels is useful when applied to multiple input\\nlocations. For example, when processing images, it is useful to detect edges in\\nthe first layer of a convolutional network. The same edges appear more or less\\neverywhere in the image, so it is practical to share parameters across the entire'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='image. In some cases, we may not wish to share parameters across the entire\\nimage. For example, if we are processing images that are cropped to be centered\\non an individual’s face, we probably want to extract different features at different\\nlocations—the part of the network processing the top of the face needs to look for\\neyebrows, while the part of the network processing the bottom of the face needs to\\nlook for a chin.\\nConvolution is not naturally equivariant to some other transformations, such\\nas changes in the scale or rotation of an image. Other mechanisms are necessary\\nfor handling these kinds of transformations.\\nFinally, some kinds of data cannot be processed by neural networks defined by\\nmatrix multiplication with a fixed-shape matrix. Convolution enables processing\\nof some of these kinds of data. We discuss this further in section 9.7.\\n9.3 Pooling\\nA typical layer of a convolutional network consists of three stages (see figure 9.7).'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In the first stage, the layer performs several convolutions in parallel to produce a\\nset of linear activations. In the second stage, each linear activation is run through\\na nonlinear activation function, such as the rectified linear activation function.\\nThis stage is sometimes called the detector stage. In the third stage, we use a\\npooling function to modify the output of the layer further.\\nA pooling function replaces the output of the net at a certain location with a\\nsummary statistic of the nearby outputs. For example, the max pooling (Zhou\\n339\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nFigure 9.6: Efficiency of edge detection. The image on the right was formed by taking\\neach pixel in the original image and subtracting the value of its neighboring pixel on the\\nleft. This shows the strength of all of the vertically oriented edges in the input image,\\nwhich can be a useful operation for object detection. Both images are 280 pixels tall.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The input image is 320 pixels wide while the output image is 319 pixels wide. This\\ntransformation can be described by a convolution kernel containing two elements, and\\nrequires 319 280 3 = 267,960 floating point operations (two multiplications and\\n× ×\\none addition per output pixel) to compute using convolution. To describe the same\\ntransformation with a matrix multiplication would take 320 280 319 280, or over\\n× × ×\\neight billion, entries in the matrix, making convolution four billion times more efficient for\\nrepresenting this transformation. The straightforward matrix multiplication algorithm\\nperforms over sixteen billion floating point operations, making convolution roughly 60,000\\ntimes more efficient computationally. Of course, most of the entries of the matrix would be\\nzero. If we stored only the nonzero entries of the matrix, then both matrix multiplication\\nand convolution would require the same number of floating point operations to compute.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The matrix would still need to contain 2 319 280 = 178,640 entries. Convolution\\n× ×\\nis an extremely efficient way of describing transformations that apply the same linear\\ntransformation of a small, local region across the entire input. (Photo credit: Paula\\nGoodfellow)\\n340\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nComplex layer terminology Simple layer terminology\\nNext layer Next layer\\nConvolutional Layer\\nPooling stage Pooling layer\\nDetector stage:\\nDetector layer: Nonlinearity\\nNonlinearity\\ne.g., rectified linear\\ne.g., rectified linear\\nConvolution stage: Convolution layer:\\nAffine transform Affine transform\\nInput to layer Input to layers\\nFigure 9.7: The components of a typical convolutional neural network layer. There are two\\ncommonly used sets of terminology for describing these layers. (Left)In this terminology,\\nthe convolutional net is viewed as a small number of relatively complex layers, with\\neach layer having many “stages.” In this terminology, there is a one-to-one mapping'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='between kernel tensors and network layers. In this book we generally use this terminology.\\n(Right)In this terminology, the convolutional net is viewed as a larger number of simple\\nlayers; every step of processing is regarded as a layer in its own right. This means that\\nnot every “layer” has parameters.\\n341\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nand Chellappa, 1988) operation reports the maximum output within a rectangular\\nneighborhood. Other popular pooling functions include the average of a rectangular\\nneighborhood, the L2 norm of a rectangular neighborhood, or a weighted average\\nbased on the distance from the central pixel.\\nIn all cases, pooling helps to make the representation become approximately\\ninvariant to small translations of the input. Invariance to translation means that\\nif we translate the input by a small amount, the values of most of the pooled\\noutputs do not change. See figure 9.8 for an example of how this works. Invariance'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to local translation can be a very useful property if we care more about whether\\nsome feature is present than exactly where it is. For example, when determining\\nwhether an image contains a face, we need not know the location of the eyes with\\npixel-perfect accuracy, we just need to know that there is an eye on the left side\\nof the face and an eye on the right side of the face. In other contexts, it is more\\nimportant to preserve the location of a feature. For example, if we want to find a\\ncorner defined by two edges meeting at a specific orientation, we need to preserve\\nthe location of the edges well enough to test whether they meet.\\nThe use of pooling can be viewed as adding an infinitely strong prior that\\nthe function the layer learns must be invariant to small translations. When this\\nassumption is correct, it can greatly improve the statistical efficiency of the network.\\nPooling over spatial regions produces invariance to translation, but if we pool'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='over the outputs of separately parametrized convolutions, the features can learn\\nwhich transformations to become invariant to (see figure 9.9).\\nBecause pooling summarizes the responses over a whole neighborhood, it is\\npossible to use fewer pooling units than detector units, by reporting summary\\nstatistics for pooling regions spaced k pixels apart rather than 1 pixel apart. See\\nfigure 9.10 for an example. This improves the computational efficiency of the\\nnetwork because the next layer has roughly k times fewer inputs to process. When\\nthe number of parameters in the next layer is a function of its input size (such as\\nwhen the next layer is fully connected and based on matrix multiplication) this\\nreduction in the input size can also result in improved statistical efficiency and\\nreduced memory requirements for storing the parameters.\\nFor many tasks, pooling is essential for handling inputs of varying size. For'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='example, if we want to classify images of variable size, the input to the classification\\nlayer must have a fixed size. This is usually accomplished by varying the size of an\\noffset between pooling regions so that the classification layer always receives the\\nsame number of summary statistics regardless of the input size. For example, the\\nfinal pooling layer of the network may be defined to output four sets of summary\\nstatistics, one for each quadrant of an image, regardless of the image size.\\n342\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nPOOLING STAGE\\n1. 1. 1. 0.2\\n... ...\\n0.1 1. 0.2 0.1\\n... ...\\nDETECTOR STAGE\\nPOOLING STAGE\\n0.3 1. 1. 1.\\n... ...\\n0.3 0.1 1. 0.2\\n... ...\\nDETECTOR STAGE\\nFigure 9.8: Max pooling introduces invariance. (Top)A view of the middle of the output\\nof a convolutional layer. The bottom row shows outputs of the nonlinearity. The top\\nrow shows the outputs of max pooling, with a stride of one pixel between pooling regions'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='and a pooling region width of three pixels. (Bottom)A view of the same network, after\\nthe input has been shifted to the right by one pixel. Every value in the bottom row has\\nchanged, but only half of the values in the top row have changed, because the max pooling\\nunits are only sensitive to the maximum value in the neighborhood, not its exact location.\\n343\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nLarge response Large response\\nin pooling unit in pooling unit\\nLarge Large\\nresponse response\\nin detector in detector\\nunit 1 unit 3\\nFigure9.9: Example of learned invariances: Apoolingunit thatpools overmultiplefeatures\\nthat are learned with separate parameters can learn to be invariant to transformations of\\nthe input. Here we show how a set of three learned filters and a max pooling unit can learn\\nto become invariant to rotation. All three filters are intended to detect a hand-written 5.\\nEach filter attempts to match a slightly different orientation of the 5. When a 5 appears in'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the input, the corresponding filter will match it and cause a large activation in a detector\\nunit. The max pooling unit then has a large activation regardless of which detector unit\\nwas activated. We show here how the network processes two different inputs, resulting\\nin two different detector units being activated. The effect on the pooling unit is roughly\\nthe same either way. This principle is leveraged by maxout networks (Goodfellow et al.,\\n2013a) and other convolutional networks. Max pooling over spatial positions is naturally\\ninvariant to translation; this multi-channel approach is only necessary for learning other\\ntransformations.\\n1. 0.2 0.1\\n0.1 1. 0.2 0.1 0.0 0.1\\nFigure 9.10: Pooling with downsampling. Here we use max-pooling with a pool width of\\nthree and a stride between pools of two. This reduces the representation size by a factor\\nof two, which reduces the computational and statistical burden on the next layer. Note'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='that the rightmost pooling region has a smaller size, but must be included if we do not\\nwant to ignore some of the detector units.\\n344\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nSome theoretical work gives guidance as to which kinds of pooling one should\\nuse in various situations (Boureau et al., 2010). It is also possible to dynamically\\npool features together, for example, by running a clustering algorithm on the\\nlocations of interesting features (Boureau et al., 2011). This approach yields a\\ndifferent set of pooling regions for each image. Another approach is to learn a\\nsingle pooling structure that is then applied to all images (Jia et al., 2012).\\nPooling can complicate some kinds of neural network architectures that use\\ntop-down information, such as Boltzmann machines and autoencoders. These\\nissues will be discussed further when we present these types of networks in part III.\\nPooling in convolutional Boltzmann machines is presented in section 20.6. The'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='inverse-like operations on pooling units needed in some differentiable networks will\\nbe covered in section 20.10.6.\\nSome examples of complete convolutionalnetwork architectures for classification\\nusing convolution and pooling are shown in figure 9.11.\\n9.4 Convolution and Pooling as an Infinitely Strong\\nPrior\\nRecall the concept of a prior probability distribution from section 5.2. This is\\na probability distribution over the parameters of a model that encodes our beliefs\\nabout what models are reasonable, before we have seen any data.\\nPriors can be considered weak or strong depending on how concentrated the\\nprobability density in the prior is. A weak prior is a prior distribution with high\\nentropy, such as a Gaussian distribution with high variance. Such a prior allows\\nthe data to move the parameters more or less freely. A strong prior has very low\\nentropy, such as a Gaussian distribution with low variance. Such a prior plays a\\nmore active role in determining where the parameters end up.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='An infinitely strong prior places zero probability on some parameters and says\\nthat these parameter values are completely forbidden, regardless of how much\\nsupport the data gives to those values.\\nWe can imagine a convolutional net as being similar to a fully connected net,\\nbut with an infinitely strong prior over its weights. This infinitely strong prior\\nsays that the weights for one hidden unit must be identical to the weights of its\\nneighbor, but shifted in space. The prior also says that the weights must be zero,\\nexcept for in the small, spatially contiguous receptive field assigned to that hidden\\nunit. Overall, we can think of the use of convolution as introducing an infinitely\\nstrong prior probability distribution over the parameters of a layer. This prior\\n345\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nOutput of softmax: Output of softmax: Output of softmax:\\n1,000 class 1,000 class 1,000 class\\nprobabilities probabilities probabilities\\nOutput of matrix Output of matrix Output of average'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='multiply: 1,000 units multiply: 1,000 units pooling: 1x1x1,000\\nOutput of reshape to Output of reshape to Output of\\nvector: vector: convolution:\\n16,384 units 576 units 16x16x1,000\\nOutput of pooling Output of pooling\\nOutput of pooling to\\nwith stride 4: with stride 4:\\n3x3 grid: 3x3x64\\n16x16x64 16x16x64\\nOutput of Output of Output of\\nconvolution + convolution + convolution +\\nReLU: 64x64x64 ReLU: 64x64x64 ReLU: 64x64x64\\nOutput of pooling Output of pooling Output of pooling\\nwith stride 4: with stride 4: with stride 4:\\n64x64x64 64x64x64 64x64x64\\nOutput of Output of Output of\\nconvolution + convolution + convolution +\\nReLU: 256x256x64 ReLU: 256x256x64 ReLU: 256x256x64\\nInput image: Input image: Input image:\\n256x256x3 256x256x3 256x256x3\\nFigure 9.11: Examples of architectures for classification with convolutional networks. The\\nspecific strides and depths used in this figure are not advisable for real use; they are'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='designed to be very shallow in order to fit onto the page. Real convolutional networks\\nalso often involve significant amounts of branching, unlike the chain structures used\\nhere for simplicity. (Left)A convolutional network that processes a fixed image size.\\nAfter alternating between convolution and pooling for a few layers, the tensor for the\\nconvolutional feature map is reshaped to flatten out the spatial dimensions. The rest\\nof the network is an ordinary feedforward network classifier, as described in chapter 6.\\n(Center)A convolutional network that processes a variable-sized image, but still maintains\\na fully connected section. This network uses a pooling operation with variably-sized pools\\nbut a fixed number of pools, in order to provide a fixed-size vector of 576 units to the\\nfully connected portion of the network. (Right)A convolutional network that does not\\nhave any fully connected weight layer. Instead, the last convolutional layer outputs one'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='feature map per class. The model presumably learns a map of how likely each class is to\\noccur at each spatial location. Averaging a feature map down to a single value provides\\nthe argument to the softmax classifier at the top.\\n346\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nsays that the function the layer should learn contains only local interactions and is\\nequivariant to translation. Likewise, the use of pooling is an infinitely strong prior\\nthat each unit should be invariant to small translations.\\nOf course, implementing a convolutional net as a fully connected net with an\\ninfinitely strong prior would be extremely computationally wasteful. But thinking\\nof a convolutional net as a fully connected net with an infinitely strong prior can\\ngive us some insights into how convolutional nets work.\\nOne key insight is that convolution and pooling can cause underfitting. Like\\nany prior, convolution and pooling are only useful when the assumptions made'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='by the prior are reasonably accurate. If a task relies on preserving precise spatial\\ninformation, then using pooling on all features can increase the training error.\\nSome convolutional network architectures (Szegedy et al., 2014a) are designed to\\nuse pooling on some channels but not on other channels, in order to get both\\nhighly invariant features and features that will not underfit when the translation\\ninvariance prior is incorrect. When a task involves incorporating information from\\nvery distant locations in the input, then the prior imposed by convolution may be\\ninappropriate.\\nAnother key insight from this view is that we should only compare convolu-\\ntional models to other convolutional models in benchmarks of statistical learning\\nperformance. Models that do not use convolution would be able to learn even\\nif we permuted all of the pixels in the image. For many image datasets, there\\nare separate benchmarks for models that are permutation invariant and must'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='discover the concept of topology via learning, and models that have the knowledge\\nof spatial relationships hard-coded into them by their designer.\\n9.5 Variants of the Basic Convolution Function\\nWhen discussing convolution in the context of neural networks, we usually do\\nnot refer exactly to the standard discrete convolution operation as it is usually\\nunderstood in the mathematical literature. The functions used in practice differ\\nslightly. Here we describe these differences in detail, and highlight some useful\\nproperties of the functions used in neural networks.\\nFirst, when we refer to convolution in the context of neural networks, we usually\\nactually mean an operation that consists of many applications of convolution in\\nparallel. This is because convolution with a single kernel can only extract one kind\\nof feature, albeit at many spatial locations. Usually we want each layer of our\\nnetwork to extract many kinds of features, at many locations.\\n347\\nCHAPTER 9. CONVOLUTIONAL NETWORKS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Additionally, the input is usually not just a grid of real values. Rather, it is a\\ngrid of vector-valued observations. For example, a color image has a red, green\\nand blue intensity at each pixel. In a multilayer convolutional network, the input\\nto the second layer is the output of the first layer, which usually has the output\\nof many different convolutions at each position. When working with images, we\\nusually think of the input and output of the convolution as being 3-D tensors, with\\none index into the different channels and two indices into the spatial coordinates\\nof each channel. Software implementations usually work in batch mode, so they\\nwill actually use 4-D tensors, with the fourth axis indexing different examples in\\nthe batch, but we will omit the batch axis in our description here for simplicity.\\nBecause convolutional networks usually use multi-channel convolution, the\\nlinear operations they are based on are not guaranteed to be commutative, even if'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='kernel-flipping is used. These multi-channel operations are only commutative if\\neach operation has the same number of output channels as input channels.\\nK K\\nAssume we have a 4-D kernel tensor with element giving the connection\\ni,j,k,l\\nstrength between a unit in channel i of the output and a unit in channel j of the\\ninput, with an offset of k rows and l columns between the output unit and the\\ninput unit. Assume our input consists of observed data V with element V giving\\ni,j,k\\nthe value of the input unit within channel i at row j and column k. Assume our\\nZ V Z K\\noutput consists of with the same format as . If is produced by convolving\\nV K\\nacross without flipping , then\\nZ = V K (9.7)\\ni,j,k l,j+m 1,k+n 1 i,l,m,n\\n− −\\nl,m,n\\n\\ue058\\nwhere the summation over l, m and n is over all values for which the tensor indexing\\noperations inside the summation is valid. In linear algebra notation, we index into\\narrays using a 1 for the first entry. This necessitates the 1 in the above formula.\\n−'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Programming languages such as C and Python index starting from 0, rendering\\nthe above expression even simpler.\\nWe may want to skip over some positions of the kernel in order to reduce the\\ncomputational cost (at the expense of not extracting our features as finely). We\\ncan think of this as downsampling the output of the full convolution function. If\\nwe want to sample only every s pixels in each direction in the output, then we can\\ndefine a downsampled convolution function c such that\\nZ K V V K\\n= c( , ,s) = . (9.8)\\ni,j,k i,j,k l,(j 1) s+m,(k 1) s+n i,l,m,n\\n− × − ×\\nl,m,n\\nWe refer to s as the stride of this downsampled convolution. It is also possible\\n348\\n\\ue058 \\ue002 \\ue003\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nto define a separate stride for each direction of motion. See figure 9.12 for an\\nillustration.\\nOne essential feature of any convolutional network implementation is the ability\\nV\\nto implicitly zero-pad the input in order to make it wider. Without this feature,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the width of the representation shrinks by one pixel less than the kernel width\\nat each layer. Zero padding the input allows us to control the kernel width and\\nthe size of the output independently. Without zero padding, we are forced to\\nchoose between shrinking the spatial extent of the network rapidly and using small\\nkernels—both scenarios that significantly limit the expressive power of the network.\\nSee figure 9.13 for an example.\\nThree special cases of the zero-padding setting are worth mentioning. One is\\nthe extreme case in which no zero-padding is used whatsoever, and the convolution\\nkernel is only allowed to visit positions where the entire kernel is contained entirely\\nwithin the image. In MATLAB terminology, this is called valid convolution. In\\nthis case, all pixels in the output are a function of the same number of pixels in\\nthe input, so the behavior of an output pixel is somewhat more regular. However,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the size of the output shrinks at each layer. If the input image has width m and\\nthe kernel has width k, the output will be of width m k +1. The rate of this\\n−\\nshrinkage can be dramatic if the kernels used are large. Since the shrinkage is\\ngreater than 0, it limits the number of convolutional layers that can be included\\nin the network. As layers are added, the spatial dimension of the network will\\neventually drop to 1 1, at which point additional layers cannot meaningfully\\n×\\nbe considered convolutional. Another special case of the zero-padding setting is\\nwhen just enough zero-padding is added to keep the size of the output equal to\\nthe size of the input. MATLAB calls this same convolution. In this case, the\\nnetwork can contain as many convolutional layers as the available hardware can\\nsupport, since the operation of convolution does not modify the architectural\\npossibilities available to the next layer. However, the input pixels near the border'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='influence fewer output pixels than the input pixels near the center. This can make\\nthe border pixels somewhat underrepresented in the model. This motivates the\\nother extreme case, which MATLAB refers to as full convolution, in which enough\\nzeroes are added for every pixel to be visited k times in each direction, resulting\\nin an output image of width m+ k 1. In this case, the output pixels near the\\n−\\nborder are a function of fewer pixels than the output pixels near the center. This\\ncan make it difficult to learn a single kernel that performs well at all positions in\\nthe convolutional feature map. Usually the optimal amount of zero padding (in\\nterms of test set classification accuracy) lies somewhere between “valid” and “same”\\nconvolution.\\n349\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nss ss ss\\n11 22 33\\nStrided\\nconvolution\\nxx xx xx xx xx\\n11 22 33 44 55\\nss ss ss\\n11 22 33\\nDownsampling\\nzz zz zz zz zz\\n11 22 33 44 55\\nConvolution\\nxx xx xx xx xx\\n11 22 33 44 55'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Figure 9.12: Convolution with a stride. In this example, we use a stride of two.\\n(Top)Convolution with a stride length of two implemented in a single operation. (Bot-\\ntom)Convolution with a stride greater than one pixel is mathematically equivalent to\\nconvolution with unit stride followed by downsampling. Obviously, the two-step approach\\ninvolving downsampling is computationally wasteful, because it computes many values\\nthat are then discarded.\\n350\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\n...\\n... ...\\n...\\n...\\n...\\n...\\n...\\n...\\nFigure 9.13: The effect of zero padding on network size: Consider a convolutional network\\nwith a kernel of width six at every layer. In this example, we do not use any pooling, so\\nonly the convolution operation itself shrinks the network size. (Top)In this convolutional\\nnetwork, we do not use any implicit zero padding. This causes the representation to\\nshrink by five pixels at each layer. Starting from an input of sixteen pixels, we are only'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='able to have three convolutional layers, and the last layer does not ever move the kernel,\\nso arguably only two of the layers are truly convolutional. The rate of shrinking can\\nbe mitigated by using smaller kernels, but smaller kernels are less expressive and some\\nshrinking is inevitable in this kind of architecture. (Bottom)By adding five implicit zeroes\\nto each layer, we prevent the representation from shrinking with depth. This allows us to\\nmake an arbitrarily deep convolutional network.\\n351\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nIn some cases, we do not actually want to use convolution, but rather locally\\nconnected layers (LeCun, 1986, 1989). In this case, the adjacency matrix in the\\ngraph of our MLP is the same, but every connection has its own weight, specified\\nW W\\nby a 6-D tensor . The indices into are respectively: i, the output channel,\\nj, the output row, k, the output column, l, the input channel, m, the row offset'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='within the input, and n, the column offset within the input. The linear part of a\\nlocally connected layer is then given by\\nZ V\\n= [ w ]. (9.9)\\ni,j,k l,j+m 1,k+n 1 i,j,k,l,m,n\\n− −\\nl,m,n\\n\\ue058\\nThis is sometimes also called unshared convolution, because it is a similar oper-\\nation to discrete convolution with a small kernel, but without sharing parameters\\nacross locations. Figure 9.14 compares local connections, convolution, and full\\nconnections.\\nLocally connected layers are useful when we know that each feature should be\\na function of a small part of space, but there is no reason to think that the same\\nfeature should occur across all of space. For example, if we want to tell if an image\\nis a picture of a face, we only need to look for the mouth in the bottom half of the\\nimage.\\nIt can also be useful to make versions of convolution or locally connected layers\\nin which the connectivity is further restricted, for example to constrain each output'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='channel i to be a function of only a subset of the input channels l. A common\\nway to do this is to make the first m output channels connect to only the first\\nn input channels, the second m output channels connect to only the second n\\ninput channels, and so on. See figure 9.15 for an example. Modeling interactions\\nbetween few channels allows the network to have fewer parameters in order to\\nreduce memory consumption and increase statistical efficiency, and also reduces\\nthe amount of computation needed to perform forward and back-propagation. It\\naccomplishes these goals without reducing the number of hidden units.\\nTiled convolution (Gregor and LeCun, 2010a; Le et al., 2010) offers a com-\\npromise between a convolutional layer and a locally connected layer. Rather than\\nlearning a separate set of weights at every spatial location, we learn a set of kernels\\nthat we rotate through as we move through space. This means that immediately'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='neighboring locations will have different filters, like in a locally connected layer,\\nbut the memory requirements for storing the parameters will increase only by a\\nfactor of the size of this set of kernels, rather than the size of the entire output\\nfeature map. See figure 9.16 for a comparison of locally connected layers, tiled\\nconvolution, and standard convolution.\\n352\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nss ss ss ss ss\\n11 22 33 44 55\\na b c d e f g h i\\nxx xx xx xx xx\\n11 22 33 44 55\\nss ss ss ss ss\\n11 22 33 44 55\\na b a b a b a b a\\nxx xx xx xx xx\\n11 22 33 44 55\\nss ss ss ss ss\\n11 22 33 44 55\\nxx xx xx xx xx\\n11 22 33 44 55\\nFigure 9.14: Comparison of local connections, convolution, and full connections.\\n(Top)A locally connected layer with a patch size of two pixels. Each edge is labeled with\\na unique letter to show that each edge is associated with its own weight parameter.\\n(Center)A convolutional layer with a kernel width of two pixels. This model has exactly'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the same connectivity as the locally connected layer. The difference lies not in which units\\ninteract with eachother, but in howthe parameters are shared. The locally connected layer\\nhas no parameter sharing. The convolutional layer uses the same two weights repeatedly\\nacross the entire input, as indicated by the repetition of the letters labeling each edge.\\n(Bottom)A fully connected layer resembles a locally connected layer in the sense that each\\nedge has its own parameter (there are too many to label explicitly with letters in this\\ndiagram). However, it does not have the restricted connectivity of the locally connected\\nlayer.\\n353\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nOutput Tensor\\nInput Tensor\\nSpatial coordinates\\nsetanidrooc\\nlennahC\\nFigure 9.15: A convolutional network with the first two output channels connected to\\nonly the first two input channels, and the second two output channels connected to only\\nthe second two input channels.\\n354\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nss ss ss ss ss'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='11 22 33 44 55\\na b c d e f g h i\\nxx xx xx xx xx\\n11 22 33 44 55\\nss ss ss ss ss\\n11 22 33 44 55\\na b c d a b c d a\\nxx xx xx xx xx\\n11 22 33 44 55\\nss ss ss ss ss\\n11 22 33 44 55\\na b a b a b a b a\\nxx xx xx xx xx\\n11 22 33 44 55\\nFigure 9.16: A comparison of locally connected layers, tiled convolution, and standard\\nconvolution. All three have the same sets of connections between units, when the same\\nsize of kernel is used. This diagram illustrates the use of a kernel that is two pixels wide.\\nThe differences between the methods lies in how they share parameters. (Top)A locally\\nconnected layer has no sharing at all. We indicate that each connection has its own weight\\nby labeling each connection with a unique letter. (Center)Tiled convolution has a set of\\nt different kernels. Here we illustrate the case of t = 2. One of these kernels has edges\\nlabeled “a” and “b,” while the other has edges labeled “c” and “d.” Each time we move one'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='pixel to the right in the output, we move on to using a different kernel. This means that,\\nlike the locally connected layer, neighboring units in the output have different parameters.\\nUnlike the locally connected layer, after we have gone through all t available kernels,\\nwe cycle back to the first kernel. If two output units are separated by a multiple of t\\nsteps, then they share parameters. (Bottom)Traditional convolution is equivalent to tiled\\nconvolution with t = 1. There is only one kernel and it is applied everywhere, as indicated\\nin the diagram by using the kernel with weights labeled “a” and “b” everywhere.\\n355\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nTo define tiled convolution algebraically, let k be a 6-D tensor, where two of\\nthe dimensions correspond to different locations in the output map. Rather than\\nhaving a separate index for each location in the output map, output locations cycle\\nthrough a set of t different choices of kernel stack in each direction. If t is equal to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the output width, this is the same as a locally connected layer.\\nZ V K\\n= , (9.10)\\ni,j,k l,j+m 1,k+n 1 i,l,m,n,j%t+1,k%t+1\\n− −\\nl,m,n\\n\\ue058\\nwhere % is the modulo operation, with t%t = 0, (t + 1)%t = 1, etc. It is\\nstraightforward to generalize this equation to use a different tiling range for each\\ndimension.\\nBoth locally connected layers and tiled convolutional layers have an interesting\\ninteraction with max-pooling: the detector units of these layers are driven by\\ndifferent filters. If these filters learn to detect different transformed versions of\\nthe same underlying features, then the max-pooled units become invariant to the\\nlearned transformation (see figure 9.9). Convolutional layers are hard-coded to be\\ninvariant specifically to translation.\\nOther operations besides convolution are usually necessary to implement a\\nconvolutional network. To perform learning, one must be able to compute the\\ngradient with respect to the kernel, given the gradient with respect to the outputs.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In some simple cases, this operation can be performed using the convolution\\noperation, but many cases of interest, including the case of stride greater than 1,\\ndo not have this property.\\nRecall that convolution is a linear operation and can thus be described as a\\nmatrix multiplication (if we first reshape the input tensor into a flat vector). The\\nmatrix involved is a function of the convolution kernel. The matrix is sparse and\\neach element of the kernel is copied to several elements of the matrix. This view\\nhelps us to derive some of the other operations needed to implement a convolutional\\nnetwork.\\nMultiplication by the transpose of the matrix defined by convolution is one\\nsuch operation. This is the operation needed to back-propagate error derivatives\\nthrough a convolutional layer, so it is needed to train convolutional networks\\nthat have more than one hidden layer. This same operation is also needed if we'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='wish to reconstruct the visible units from the hidden units (Simard et al., 1992).\\nReconstructing the visible units is an operation commonly used in the models\\ndescribed in part III of this book, such as autoencoders, RBMs, and sparse coding.\\nTranspose convolution is necessary to construct convolutional versions of those\\nmodels. Like the kernel gradient operation, this input gradient operation can be\\n356\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nimplemented using a convolution in some cases, but in the general case requires\\na third operation to be implemented. Care must be taken to coordinate this\\ntranspose operation with the forward propagation. The size of the output that the\\ntranspose operation should return depends on the zero padding policy and stride of\\nthe forward propagation operation, as well as the size of the forward propagation’s\\noutput map. In some cases, multiple sizes of input to forward propagation can'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='result in the same size of output map, so the transpose operation must be explicitly\\ntold what the size of the original input was.\\nThese three operations—convolution, backprop from output to weights, and\\nbackprop from output to inputs—are sufficient to compute all of the gradients\\nneeded to train any depth of feedforward convolutional network, as well as to train\\nconvolutional networks with reconstruction functions based on the transpose of\\nconvolution. See Goodfellow (2010) for a full derivation of the equations in the\\nfully general multi-dimensional, multi-example case. To give a sense of how these\\nequations work, we present the two dimensional, single example version here.\\nSuppose we want to train a convolutional network that incorporates strided\\nK V\\nconvolution of kernel stack applied to multi-channel image with stride s as\\nK V\\ndefined by c( , ,s) as in equation 9.8. Suppose we want to minimize some loss\\nfunction J(V ,K). During forward propagation, we will need to use c itself to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Z\\noutput , which is then propagated through the rest of the network and used to\\nG\\ncompute the cost function J. During back-propagation, we will receive a tensor\\nsuch that G = ∂ J(V ,K ).\\ni,j,k ∂Z\\ni,j,k\\nTo train the network, we need to compute the derivatives with respect to the\\nweights in the kernel. To do so, we can use a function\\n∂\\nG V V K G V\\ng( , ,s) = J( , ) = . (9.11)\\ni,j,k,l K i,m,n j,(m 1) s+k,(n 1) s+l\\n∂ i,j,k,l − × − ×\\nm,n\\n\\ue058\\nIf this layer is not the bottom layer of the network, we will need to compute\\nV\\nthe gradient with respect to in order to back-propagate the error farther down.\\nTo do so, we can use a function\\n∂\\nK G V K\\nh( , ,s) = J( , ) (9.12)\\ni,j,k V\\n∂\\ni,j,k\\nK G\\n= . (9.13)\\nq,i,m,p q,l,n\\nl,m n,p q\\n\\ue058\\ns.t.\\n\\ue058s.t. \\ue058\\n(l 1) s+m=j (n 1) s+p=k\\n− × − ×\\nAutoencoder networks, described in chapter 14, are feedforward networks\\ntrained to copy their input to their output. A simple example is the PCA algorithm,\\n357\\nCHAPTER 9. CONVOLUTIONAL NETWORKS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='that copies its input x to an approximate reconstruction r using the function\\nW Wx. It is common for more general autoencoders to use multiplication\\n\\ue03e\\nby the transpose of the weight matrix just as PCA does. To make such models\\nconvolutional, we can use the function h to perform the transpose of the convolution\\nH Z\\noperation. Suppose we have hidden units in the same format as and we define\\na reconstruction\\nR K H\\n= h( , ,s). (9.14)\\nIn order to train the autoencoder, we will receive the gradient with respect\\nR E\\nto as a tensor . To train the decoder, we need to obtain the gradient with\\nK H E\\nrespect to . This is given by g( , ,s). To train the encoder, we need to obtain\\nH K E\\nthe gradient with respect to . This is given by c( , ,s). It is also possible to\\ndifferentiate through g using c and h, but these operations are not needed for the\\nback-propagation algorithm on any standard network architectures.\\nGenerally, we do not use only a linear operation in order to transform from'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the inputs to the outputs in a convolutional layer. We generally also add some\\nbias term to each output before applying the nonlinearity. This raises the question\\nof how to share parameters among the biases. For locally connected layers it is\\nnatural to give each unit its own bias, and for tiled convolution, it is natural to\\nshare the biases with the same tiling pattern as the kernels. For convolutional\\nlayers, it is typical to have one bias per channel of the output and share it across\\nall locations within each convolution map. However, if the input is of known, fixed\\nsize, it is also possible to learn a separate bias at each location of the output map.\\nSeparating the biases may slightly reduce the statistical efficiency of the model, but\\nalso allows the model to correct for differences in the image statistics at different\\nlocations. For example, when using implicit zero padding, detector units at the\\nedge of the image receive less total input and may need larger biases.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='9.6 Structured Outputs\\nConvolutional networks can be used to output a high-dimensional, structured\\nobject, rather than just predicting a class label for a classification task or a real\\nvalue for a regression task. Typically this object is just a tensor, emitted by a\\nS\\nstandard convolutional layer. For example, the model might emit a tensor , where\\nS is the probability that pixel (j,k) of the input to the network belongs to class\\ni,j,k\\ni. This allows the model to label every pixel in an image and draw precise masks\\nthat follow the outlines of individual objects.\\nOne issue that often comes up is that the output plane can be smaller than the\\n358\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nYYˆˆ((11)) YYˆˆ((22)) YYˆˆ((33))\\nV W V W V\\nHH((11)) HH((22)) HH((33))\\nU U U\\nXX\\nFigure 9.17: An example of a recurrent convolutional network for pixel labeling. The\\ninput is an image tensor X, with axes corresponding to image rows, image columns, and'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='channels (red, green, blue). The goal is to output a tensor of labels Yˆ, with a probability\\ndistribution over labels for each pixel. This tensor has axes corresponding to image rows,\\nˆ\\nimage columns, and the different classes. Rather than outputting Y in a single shot, the\\nˆ ˆ\\nrecurrent network iteratively refines its estimate Y by using a previous estimate of Y\\nas input for creating a new estimate. The same parameters are used for each updated\\nestimate, and the estimate can be refined as many times as we wish. The tensor of\\nconvolution kernels U is used on each step to compute the hidden representation given the\\ninput image. The kernel tensor V is used to produce an estimate of the labels given the\\nhidden values. On all but the first step, the kernels W are convolved over Yˆ to provide\\ninput to the hidden layer. On the first time step, this term is replaced by zero. Because\\nthe same parameters are used on each step, this is an example of a recurrent network, as'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='described in chapter 10.\\ninput plane, as shown in figure 9.13. In the kinds of architectures typically used for\\nclassification of a single object in an image, the greatest reduction in the spatial\\ndimensions of the network comes from using pooling layers with large stride. In\\norder to produce an output map of similar size as the input, one can avoid pooling\\naltogether (Jain et al., 2007). Another strategy is to simply emit a lower-resolution\\ngrid of labels (Pinheiro and Collobert, 2014, 2015). Finally, in principle, one could\\nuse a pooling operator with unit stride.\\nOne strategy for pixel-wise labeling of images is to produce an initial guess\\nof the image labels, then refine this initial guess using the interactions between\\nneighboring pixels. Repeating this refinement step several times corresponds to\\nusing the same convolutions at each stage, sharing weights between the last layers of\\nthe deep net (Jain et al., 2007). This makes the sequence of computations performed'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='by the successive convolutional layers with weights shared across layers a particular\\nkind of recurrent network (Pinheiro and Collobert, 2014, 2015). Figure 9.17 shows\\nthe architecture of such a recurrent convolutional network.\\n359\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nOnce a prediction for each pixel is made, various methods can be used to\\nfurther process these predictions in order to obtain a segmentation of the image\\ninto regions (Briggman et al., 2009; Turaga et al., 2010; Farabet et al., 2013).\\nThe general idea is to assume that large groups of contiguous pixels tend to be\\nassociated with the same label. Graphical models can describe the probabilistic\\nrelationships between neighboring pixels. Alternatively, the convolutional network\\ncan be trained to maximize an approximation of the graphical model training\\nobjective (Ning et al., 2005; Thompson et al., 2014).\\n9.7 Data Types\\nThe data used with a convolutional network usually consists of several channels,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='each channel being the observation of a different quantity at some point in space\\nor time. See table 9.1 for examples of data types with different dimensionalities\\nand number of channels.\\nFor an example of convolutional networks applied to video, see Chen et al.\\n(2010).\\nSo far we have discussed only the case where every example in the train and test\\ndata has the same spatial dimensions. One advantage to convolutional networks\\nis that they can also process inputs with varying spatial extents. These kinds of\\ninput simply cannot be represented by traditional, matrix multiplication-based\\nneural networks. This provides a compelling reason to use convolutional networks\\neven when computational cost and overfitting are not significant issues.\\nFor example, consider a collection of images, where each image has a different\\nwidth and height. It is unclear how to model such inputs with a weight matrix of\\nfixed size. Convolution is straightforward to apply; the kernel is simply applied a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='different number of times depending on the size of the input, and the output of the\\nconvolution operation scales accordingly. Convolution may be viewed as matrix\\nmultiplication; the same convolution kernel induces a different size of doubly block\\ncirculant matrix for each size of input. Sometimes the output of the network is\\nallowed to have variable size as well as the input, for example if we want to assign\\na class label to each pixel of the input. In this case, no further design work is\\nnecessary. In other cases, the network must produce some fixed-size output, for\\nexample if we want to assign a single class label to the entire image. In this case\\nwe must make some additional design steps, like inserting a pooling layer whose\\npooling regions scale in size proportional to the size of the input, in order to\\nmaintain a fixed number of pooled outputs. Some examples of this kind of strategy\\nare shown in figure 9.11.\\n360\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nSingle channel Multi-channel'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='1-D Audio waveform: The axis we Skeleton animation data: Anima-\\nconvolve over corresponds to tions of 3-D computer-rendered\\ntime. We discretize time and characters are generated by alter-\\nmeasure the amplitude of the ing the pose of a “skeleton” over\\nwaveform once per time step. time. At each point in time, the\\npose of the character is described\\nby a specification of the angles of\\neach of the joints in the charac-\\nter’s skeleton. Each channel in\\nthe data we feed to the convolu-\\ntional model represents the angle\\nabout one axis of one joint.\\n2-D Audio data that has been prepro- Color image data: One channel\\ncessed with a Fourier transform: contains the red pixels, one the\\nWe can transform the audio wave- green pixels, and one the blue\\nform into a 2D tensor with dif- pixels. The convolution kernel\\nferent rows corresponding to dif- moves over both the horizontal\\nferent frequencies and different and vertical axes of the image,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='columns corresponding to differ- conferring translation equivari-\\nent points in time. Using convolu- ance in both directions.\\ntion in the time makes the model\\nequivariant to shifts in time. Us-\\ning convolution across the fre-\\nquency axis makes the model\\nequivariant to frequency, so that\\nthe same melody played in a dif-\\nferent octave produces the same\\nrepresentation but at a different\\nheight in the network’s output.\\n3-D Volumetric data: A common Color video data: One axis corre-\\nsource of this kind of data is med- sponds to time, one to the height\\nical imaging technology, such as of the video frame, and one to\\nCT scans. the width of the video frame.\\nTable 9.1: Examples of different formats of data that can be used with convolutional\\nnetworks.\\n361\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nNote that the use of convolution for processing variable sized inputs only makes\\nsense for inputs that have variable size because they contain varying amounts'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of observation of the same kind of thing—different lengths of recordings over\\ntime, different widths of observations over space, etc. Convolution does not make\\nsense if the input has variable size because it can optionally include different\\nkinds of observations. For example, if we are processing college applications, and\\nour features consist of both grades and standardized test scores, but not every\\napplicant took the standardized test, then it does not make sense to convolve the\\nsame weights over both the features corresponding to the grades and the features\\ncorresponding to the test scores.\\n9.8 Efficient Convolution Algorithms\\nModern convolutional network applications often involve networks containing more\\nthan one million units. Powerful implementations exploiting parallel computation\\nresources, as discussed in section 12.1, are essential. However, in many cases it\\nis also possible to speed up convolution by selecting an appropriate convolution\\nalgorithm.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Convolution is equivalent to converting both the input and the kernel to the\\nfrequency domain using a Fourier transform, performing point-wise multiplication\\nof the two signals, and converting back to the time domain using an inverse\\nFourier transform. For some problem sizes, this can be faster than the naive\\nimplementation of discrete convolution.\\nWhen a d-dimensional kernel can be expressed as the outer product of d\\nvectors, one vector per dimension, the kernel is called separable. When the\\nkernel is separable, naive convolution is inefficient. It is equivalent to compose d\\none-dimensional convolutions with each of these vectors. The composed approach\\nis significantly faster than performing one d-dimensional convolution with their\\nouter product. The kernel also takes fewer parameters to represent as vectors.\\nIf the kernel is w elements wide in each dimension, then naive multidimensional\\nconvolution requires O(wd) runtime and parameter storage space, while separable'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='convolution requires O(w d) runtime and parameter storage space. Of course,\\n×\\nnot every convolution can be represented in this way.\\nDevising faster ways of performing convolution or approximate convolution\\nwithout harming the accuracy of the model is an active area of research. Even tech-\\nniques that improve the efficiency of only forward propagation are useful because\\nin the commercial setting, it is typical to devote more resources to deployment of\\na network than to its training.\\n362\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\n9.9 Random or Unsupervised Features\\nTypically, the most expensive part of convolutional network training is learning the\\nfeatures. The output layer is usually relatively inexpensive due to the small number\\nof features provided as input to this layer after passing through several layers of\\npooling. When performing supervised training with gradient descent, every gradient\\nstep requires a complete run of forward propagation and backward propagation'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='through the entire network. One way to reduce the cost of convolutional network\\ntraining is to use features that are not trained in a supervised fashion.\\nThere are three basic strategies for obtaining convolution kernels without\\nsupervised training. One is to simply initialize them randomly. Another is to\\ndesign them by hand, for example by setting each kernel to detect edges at a\\ncertain orientation or scale. Finally, one can learn the kernels with an unsupervised\\ncriterion. For example, Coates et al. (2011) apply k-means clustering to small\\nimage patches, then use each learned centroid as a convolution kernel. Part III\\ndescribes many more unsupervised learning approaches. Learning the features\\nwith an unsupervised criterion allows them to be determined separately from the\\nclassifier layer at the top of the architecture. One can then extract the features for\\nthe entire training set just once, essentially constructing a new training set for the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='last layer. Learning the last layer is then typically a convex optimization problem,\\nassuming the last layer is something like logistic regression or an SVM.\\nRandom filters often work surprisingly well in convolutional networks (Jarrett\\net al., 2009; Saxe et al., 2011; Pinto et al., 2011; Cox and Pinto, 2011). Saxe et al.\\n(2011) showed that layers consisting of convolution following by pooling naturally\\nbecome frequency selective and translation invariant when assigned random weights.\\nThey argue that this provides an inexpensive way to choose the architecture of\\na convolutional network: first evaluate the performance of several convolutional\\nnetwork architectures by training only the last layer, then take the best of these\\narchitectures and train the entire architecture using a more expensive approach.\\nAn intermediate approach is to learn the features, but using methods that do\\nnot require full forward and back-propagation at every gradient step. As with'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='multilayer perceptrons, we use greedy layer-wise pretraining, to train the first layer\\nin isolation, then extract all features from the first layer only once, then train the\\nsecond layer in isolation given those features, and so on. Chapter 8 has described\\nhow to perform supervised greedy layer-wise pretraining, and part III extends this\\nto greedy layer-wise pretraining using an unsupervised criterion at each layer. The\\ncanonical example of greedy layer-wise pretraining of a convolutional model is the\\nconvolutional deep belief network (Lee et al., 2009). Convolutional networks offer\\n363\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nus the opportunity to take the pretraining strategy one step further than is possible\\nwith multilayer perceptrons. Instead of training an entire convolutional layer at a\\ntime, we can train a model of a small patch, as Coates et al. (2011) do with k-means.\\nWe can then use the parameters from this patch-based model to define the kernels'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of a convolutional layer. This means that it is possible to use unsupervised learning\\nto train a convolutional network without ever using convolution during the training\\nprocess. Using this approach, we can train very large models and incur a high\\ncomputational cost only at inference time (Ranzato et al., 2007b; Jarrett et al.,\\n2009; Kavukcuoglu et al., 2010; Coates et al., 2013). This approach was popular\\nfrom roughly 2007–2013, when labeled datasets were small and computational\\npower was more limited. Today, most convolutional networks are trained in a\\npurely supervised fashion, using full forward and back-propagation through the\\nentire network on each training iteration.\\nAs with other approaches to unsupervised pretraining, it remains difficult to\\ntease apart the cause of some of the benefits seen with this approach. Unsupervised\\npretraining may offer some regularization relative to supervised training, or it may'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='simply allow us to train much larger architectures due to the reduced computational\\ncost of the learning rule.\\n9.10 The Neuroscientific Basis for Convolutional Net-\\nworks\\nConvolutional networks are perhaps the greatest success story of biologically\\ninspired artificial intelligence. Though convolutional networks have been guided\\nby many other fields, some of the key design principles of neural networks were\\ndrawn from neuroscience.\\nThe history of convolutional networks begins with neuroscientific experiments\\nlong before the relevant computational models were developed. Neurophysiologists\\nDavid Hubel and Torsten Wiesel collaborated for several years to determine many\\nof the most basic facts about how the mammalian vision system works (Hubel and\\nWiesel, 1959, 1962, 1968). Their accomplishments were eventually recognized with\\na Nobel prize. Their findings that have had the greatest influence on contemporary\\ndeep learning models were based on recording the activity of individual neurons in'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='cats. They observed how neurons in the cat’s brain responded to images projected\\nin precise locations on a screen in front of the cat. Their great discovery was\\nthat neurons in the early visual system responded most strongly to very specific\\npatterns of light, such as precisely oriented bars, but responded hardly at all to\\nother patterns.\\n364\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nTheir work helped to characterize many aspects of brain function that are\\nbeyond the scope of this book. From the point of view of deep learning, we can\\nfocus on a simplified, cartoon view of brain function.\\nIn this simplified view, we focus on a part of the brain called V1, also known\\nas the primary visual cortex. V1 is the first area of the brain that begins to\\nperform significantly advanced processing of visual input. In this cartoon view,\\nimages are formed by light arriving in the eye and stimulating the retina, the\\nlight-sensitive tissue in the back of the eye. The neurons in the retina perform'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='some simple preprocessing of the image but do not substantially alter the way it is\\nrepresented. The image then passes through the optic nerve and a brain region\\ncalled the lateral geniculate nucleus. The main role, as far as we are concerned\\nhere, of both of these anatomical regions is primarily just to carry the signal from\\nthe eye to V1, which is located at the back of the head.\\nA convolutional network layer is designed to capture three properties of V1:\\n1. V1 is arranged in a spatial map. It actually has a two-dimensional structure\\nmirroring the structure of the image in the retina. For example, light\\narriving at the lower half of the retina affects only the corresponding half of\\nV1. Convolutional networks capture this property by having their features\\ndefined in terms of two dimensional maps.\\n2. V1 contains many simple cells. A simple cell’s activity can to some extent\\nbe characterized by a linear function of the image in a small, spatially'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='localized receptive field. The detector units of a convolutional network are\\ndesigned to emulate these properties of simple cells.\\n3. V1 also contains many complex cells. These cells respond to features that\\nare similar to those detected by simple cells, but complex cells are invariant\\nto small shifts in the position of the feature. This inspires the pooling units\\nof convolutional networks. Complex cells are also invariant to some changes\\nin lighting that cannot be captured simply by pooling over spatial locations.\\nThese invariances have inspired some of the cross-channel pooling strategies\\nin convolutional networks, such as maxout units (Goodfellow et al., 2013a).\\nThough we know the most about V1, it is generally believed that the same\\nbasic principles apply to other areas of the visual system. In our cartoon view of\\nthe visual system, the basic strategy of detection followed by pooling is repeatedly\\napplied as we move deeper into the brain. As we pass through multiple anatomical'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='layers of the brain, we eventually find cells that respond to some specific concept\\nand are invariant to many transformations of the input. These cells have been\\n365\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nnicknamed “grandmother cells”—the idea is that a person could have a neuron that\\nactivates when seeing an image of their grandmother, regardless of whether she\\nappears in the left or right side of the image, whether the image is a close-up of\\nher face or zoomed out shot of her entire body, whether she is brightly lit, or in\\nshadow, etc.\\nThese grandmother cells have been shown to actually exist in the human brain,\\nin a region called the medial temporal lobe (Quiroga et al., 2005). Researchers\\ntested whether individual neurons would respond to photos of famous individuals.\\nThey found what has come to be called the “Halle Berry neuron”: an individual\\nneuron that is activated by the concept of Halle Berry. This neuron fires when a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='person sees a photo of Halle Berry, a drawing of Halle Berry, or even text containing\\nthe words “Halle Berry.” Of course, this has nothing to do with Halle Berry herself;\\nother neurons responded to the presence of Bill Clinton, Jennifer Aniston, etc.\\nThese medial temporal lobe neurons are somewhat more general than modern\\nconvolutional networks, which would not automatically generalize to identifying\\na person or object when reading its name. The closest analog to a convolutional\\nnetwork’s last layer of features is a brain area called the inferotemporal cortex\\n(IT). When viewing an object, information flows from the retina, through the\\nLGN, to V1, then onward to V2, then V4, then IT. This happens within the first\\n100ms of glimpsing an object. If a person is allowed to continue looking at the\\nobject for more time, then information will begin to flow backwards as the brain\\nuses top-down feedback to update the activations in the lower level brain areas.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='However, if we interrupt the person’s gaze, and observe only the firing rates that\\nresult from the first 100ms of mostly feedforward activation, then IT proves to be\\nvery similar to a convolutional network. Convolutional networks can predict IT\\nfiring rates, and also perform very similarly to (time limited) humans on object\\nrecognition tasks (DiCarlo, 2013).\\nThat being said, there are many differences between convolutional networks\\nand the mammalian vision system. Some of these differences are well known\\nto computational neuroscientists, but outside the scope of this book. Some of\\nthese differences are not yet known, because many basic questions about how the\\nmammalian vision system works remain unanswered. As a brief list:\\nThe humaneye is mostly very low resolution, except for a tiny patchcalled the\\n•\\nfovea. The fovea only observes an area about the size of a thumbnail held at\\narms length. Though we feel as if we can see an entire scene in high resolution,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='this is an illusion created by the subconscious part of our brain, as it stitches\\ntogether several glimpses of small areas. Most convolutional networks actually\\nreceive large full resolution photographs as input. The human brain makes\\n366\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nseveral eye movements called saccades to glimpse the most visually salient\\nor task-relevant parts of a scene. Incorporating similar attention mechanisms\\ninto deep learning models is an active research direction. In the context of\\ndeep learning, attention mechanisms have been most successful for natural\\nlanguage processing, as described in section 12.4.5.1. Several visual models\\nwith foveation mechanisms have been developed but so far have not become\\nthe dominant approach (Larochelle and Hinton, 2010; Denil et al., 2012).\\nThe human visual system is integrated with many other senses, such as\\n•\\nhearing, and factors like our moods and thoughts. Convolutional networks\\nso far are purely visual.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The human visual system does much more than just recognize objects. It is\\n•\\nable to understand entire scenes including many objects and relationships\\nbetween objects, and processes rich 3-D geometric information needed for\\nour bodies to interface with the world. Convolutional networks have been\\napplied to some of these problems but these applications are in their infancy.\\nEven simple brain areas like V1 are heavily impacted by feedback from higher\\n•\\nlevels. Feedback has been explored extensively in neural network models but\\nhas not yet been shown to offer a compelling improvement.\\nWhile feedforward IT firing rates capture much of the same information as\\n•\\nconvolutional network features, it is not clear how similar the intermediate\\ncomputations are. The brain probably uses very different activation and\\npooling functions. An individual neuron’s activation probably is not well-\\ncharacterized by a single linear filter response. A recent model of V1 involves'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='multiple quadratic filters for each neuron (Rust et al., 2005). Indeed our\\ncartoon picture of “simple cells” and “complex cells” might create a non-\\nexistent distinction; simple cells and complex cells might both be the same\\nkind of cell but with their “parameters” enabling a continuum of behaviors\\nranging from what we call “simple” to what we call “complex.”\\nIt is also worth mentioning that neuroscience has told us relatively little\\nabout how to train convolutional networks. Model structures with parameter\\nsharing across multiple spatial locations date back to early connectionist models\\nof vision (Marr and Poggio, 1976), but these models did not use the modern\\nback-propagation algorithm and gradient descent. For example, the Neocognitron\\n(Fukushima, 1980) incorporated most of the model architecture design elements of\\nthe modern convolutional network but relied on a layer-wise unsupervised clustering\\nalgorithm.\\n367\\nCHAPTER 9. CONVOLUTIONAL NETWORKS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Lang and Hinton (1988) introduced the use of back-propagation to train\\ntime-delay neural networks (TDNNs). To use contemporary terminology,\\nTDNNs are one-dimensional convolutional networks applied to time series. Back-\\npropagation applied to these models was not inspired by any neuroscientific observa-\\ntion and is considered by some to be biologically implausible. Following the success\\nof back-propagation-based training of TDNNs, (LeCun et al., 1989) developed\\nthe modern convolutional network by applying the same training algorithm to 2-D\\nconvolution applied to images.\\nSo far we have described how simple cells are roughly linear and selective for\\ncertain features, complex cells are more nonlinear and become invariant to some\\ntransformations of these simple cell features, and stacks of layers that alternate\\nbetween selectivity and invariance can yield grandmother cells for very specific\\nphenomena. We have not yet described precisely what these individual cells detect.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In a deep, nonlinear network, it can be difficult to understand the function of\\nindividual cells. Simple cells in the first layer are easier to analyze, because their\\nresponses are driven by a linear function. In an artificial neural network, we can\\njust display an image of the convolution kernel to see what the corresponding\\nchannel of a convolutional layer responds to. In a biological neural network, we\\ndo not have access to the weights themselves. Instead, we put an electrode in the\\nneuron itself, display several samples of white noise images in front of the animal’s\\nretina, and record how each of these samples causes the neuron to activate. We can\\nthen fit a linear model to these responses in order to obtain an approximation of\\nthe neuron’s weights. This approach is known as reverse correlation (Ringach\\nand Shapley, 2004).\\nReverse correlation shows us that most V1 cells have weights that are described\\nby Gabor functions. The Gabor function describes the weight at a 2-D point'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='in the image. We can think of an image as being a function of 2-D coordinates,\\nI(x,y). Likewise, we can think of a simple cell as sampling the image at a set of\\nX Y\\nlocations, defined by a set of x coordinates and a set of y coordinates, , and\\napplying weights that are also a function of the location, w(x,y). From this point\\nof view, the response of a simple cell to an image is given by\\ns(I) = w(x,y)I(x,y). (9.15)\\nx Xy Y\\n\\ue058∈ \\ue058∈\\nSpecifically, w(x,y) takes the form of a Gabor function:\\nw(x,y;α,β ,β ,f,φ,x ,y ,τ) = αexp β x 2 β y 2 cos(fx +φ), (9.16)\\nx y 0 0 x \\ue030 y \\ue030 \\ue030\\n− −\\nwhere\\nx = (x x )cos(τ)+ (y y )sin(τ) (9.17)\\n\\ue030 0 0\\n− −\\n368\\n\\ue000 \\ue001\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nand\\ny = (x x )sin(τ)+ (y y )cos(τ). (9.18)\\n\\ue030 0 0\\n− − −\\nHere, α, β , β , f, φ, x , y , and τ are parameters that control the properties\\nx y 0 0\\nof the Gabor function. Figure 9.18 shows some examples of Gabor functions with\\ndifferent settings of these parameters.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The parameters x , y , and τ define a coordinate system. We translate and\\n0 0\\nrotate x and y to form x and y . Specifically, the simple cell will respond to image\\n\\ue030 \\ue030\\nfeatures centered at the point (x , y ), and it will respond to changes in brightness\\n0 0\\nas we move along a line rotated τ radians from the horizontal.\\nViewed as a function of x and y , the function w then responds to changes in\\n\\ue030 \\ue030\\nbrightness as we move along the x axis. It has two important factors: one is a\\n\\ue030\\nGaussian function and the other is a cosine function.\\nThe Gaussian factor αexp β x2 β y 2 can be seen as a gating term that\\nx \\ue030 y \\ue030\\n− −\\nensures the simple cell will only respond to values near where x and y are both\\n\\ue030 \\ue030\\n\\ue000 \\ue001\\nzero, in other words, near the center of the cell’s receptive field. The scaling factor\\nα adjusts the total magnitude of the simple cell’s response, while β and β control\\nx y\\nhow quickly its receptive field falls off.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The cosine factor cos(fx +φ) controls how the simple cell responds to changing\\n\\ue030\\nbrightness along the x axis. The parameter f controls the frequency of the cosine\\n\\ue030\\nand φ controls its phase offset.\\nAltogether, this cartoon view of simple cells means that a simple cell responds\\nto a specific spatial frequency of brightness in a specific direction at a specific\\nlocation. Simple cells are most excited when the wave of brightness in the image\\nhas the same phase as the weights. This occurs when the image is bright where the\\nweights are positive and dark where the weights are negative. Simple cells are most\\ninhibited when the wave of brightness is fully out of phase with the weights—when\\nthe image is dark where the weights are positive and bright where the weights are\\nnegative.\\nThe cartoon view of a complex cell is that it computes the L2 norm of the\\n2-D vector containing two simple cells’ responses: c(I) = s (I)2 +s (I)2. An\\n0 1'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='important special case occurs when s has all of the same parameters as s except\\n1 0\\n\\ue070\\nfor φ, and φ is set such that s is one quarter cycle out of phase with s . In this\\n1 0\\ncase, s and s form a quadrature pair. A complex cell defined in this way\\n0 1\\nresponds when the Gaussian reweighted image I(x,y)exp( β x2 β y 2) contains\\nx \\ue030 y \\ue030\\n− −\\na high amplitude sinusoidal wave with frequency f in direction τ near (x ,y ),\\n0 0\\nregardless of the phase offset of this wave. In other words, the complex cell is\\ninvariant to small translations of the image in direction τ, or to negating the image\\n369\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nFigure 9.18: Gabor functions with a variety of parameter settings. White indicates\\nlarge positive weight, black indicates large negative weight, and the background gray\\ncorresponds to zero weight. (Left)Gabor functions with different values of the parameters\\nthat control the coordinate system: x , y , and τ. Each Gabor function in this grid is\\n0 0'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='assigned a value of x and y proportional to its position in its grid, and τ is chosen so\\n0 0\\nthat each Gabor filter is sensitive to the direction radiating out from the center of the grid.\\nFor the other two plots, x , y , and τ are fixed to zero. (Center)Gabor functions with\\n0 0\\ndifferent Gaussian scale parameters β and β . Gabor functions are arranged in increasing\\nx y\\nwidth (decreasing β ) as we move left to right through the grid, and increasing height\\nx\\n(decreasing β ) as we move top to bottom. For the other two plots, the β values are fixed\\ny\\nto 1.5 the image width. (Right)Gabor functions with different sinusoid parameters f\\n×\\nand φ. As we move top to bottom, f increases, and as we move left to right, φ increases.\\nFor the other two plots, φ is fixed to 0 and f is fixed to 5 the image width.\\n×\\n(replacing black with white and vice versa).\\nSome of the most striking correspondences between neuroscience and machine'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='learning come from visually comparing the features learned by machine learning\\nmodels with those employed by V1. Olshausen and Field (1996) showed that\\na simple unsupervised learning algorithm, sparse coding, learns features with\\nreceptive fields similar to those of simple cells. Since then, we have found that\\nan extremely wide variety of statistical learning algorithms learn features with\\nGabor-like functions when applied to natural images. This includes most deep\\nlearning algorithms, which learn these features in their first layer. Figure 9.19\\nshows some examples. Because so many different learning algorithms learn edge\\ndetectors, it is difficult to conclude that any specific learning algorithm is the\\n“right” model of the brain just based on the features that it learns (though it can\\ncertainly be a bad sign if an algorithm does not learn some sort of edge detector\\nwhen applied to natural images). These features are an important part of the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='statistical structure of natural images and can be recovered by many different\\napproaches to statistical modeling. See Hyvärinen et al. (2009) for a review of the\\nfield of natural image statistics.\\n370\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nFigure 9.19: Many machine learning algorithms learn features that detect edges or specific\\ncolors of edges when applied to natural images. These feature detectors are reminiscent of\\nthe Gabor functions known to be present in primary visual cortex. (Left)Weights learned\\nby an unsupervised learning algorithm (spike and slab sparse coding) applied to small\\nimage patches. (Right)Convolution kernels learned by the first layer of a fully supervised\\nconvolutional maxout network. Neighboring pairs of filters drive the same maxout unit.\\n9.11 Convolutional Networks and the History of Deep\\nLearning\\nConvolutional networks have played an important role in the history of deep\\nlearning. They are a key example of a successful application of insights obtained'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='by studying the brain to machine learning applications. They were also some of\\nthe first deep models to perform well, long before arbitrary deep models were\\nconsidered viable. Convolutional networks were also some of the first neural\\nnetworks to solve important commercial applications and remain at the forefront\\nof commercial applications of deep learning today. For example, in the 1990s, the\\nneural network research group at AT&T developed a convolutional network for\\nreading checks (LeCun et al., 1998b). By the end of the 1990s, this system deployed\\nby NEC was reading over 10% of all the checks in the US. Later, several OCR\\nand handwriting recognition systems based on convolutional nets were deployed by\\nMicrosoft (Simard et al., 2003). See chapter 12 for more details on such applications\\nand more modern applications of convolutional networks. See LeCun et al. (2010)\\nfor a more in-depth history of convolutional networks up to 2010.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Convolutional networks were also used to win many contests. The current\\nintensity of commercial interest in deep learning began when Krizhevsky et al.\\n(2012) won the ImageNet object recognition challenge, but convolutional networks\\n371\\nCHAPTER 9. CONVOLUTIONAL NETWORKS\\nhad been used to win other machine learning and computer vision contests with\\nless impact for years earlier.\\nConvolutional nets were some of the first working deep networks trained with\\nback-propagation. It is not entirely clear why convolutional networks succeeded\\nwhen general back-propagation networks were considered to have failed. It may\\nsimply be that convolutional networks were more computationally efficient than\\nfully connected networks, so it was easier to run multiple experiments with them\\nand tune their implementation and hyperparameters. Larger networks also seem\\nto be easier to train. With modern hardware, large fully connected networks'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='appear to perform reasonably on many tasks, even when using datasets that were\\navailable and activation functions that were popular during the times when fully\\nconnected networks were believed not to work well. It may be that the primary\\nbarriers to the success of neural networks were psychological (practitioners did\\nnot expect neural networks to work, so they did not make a serious effort to use\\nneural networks). Whatever the case, it is fortunate that convolutional networks\\nperformed well decades ago. In many ways, they carried the torch for the rest of\\ndeep learning and paved the way to the acceptance of neural networks in general.\\nConvolutional networks provide a way to specialize neural networks to work\\nwith data that has a clear grid-structured topology and to scale such models to\\nvery large size. This approach has been the most successful on a two-dimensional,\\nimage topology. To process one-dimensional, sequential data, we turn next to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='another powerful specialization of the neural networks framework: recurrent neural\\nnetworks.\\n372\\nChapter 10\\nSequence Modeling: Recurrent\\nand Recursive Nets\\nRecurrent neural networks or RNNs (Rumelhart et al., 1986a) are a family of\\nneural networks for processing sequential data. Much as a convolutional network\\nX\\nis a neural network that is specialized for processing a grid of values such as\\nan image, a recurrent neural network is a neural network that is specialized for\\nprocessing a sequence of values x(1),...,x(τ). Just as convolutional networks\\ncan readily scale to images with large width and height, and some convolutional\\nnetworks can process images of variable size, recurrent networks can scale to much\\nlonger sequences than would be practical for networks without sequence-based\\nspecialization. Most recurrent networks can also process sequences of variable\\nlength.\\nTo go from multi-layer networks to recurrent networks, we need to take advan-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='tage of one of the early ideas found in machine learning and statistical models of\\nthe 1980s: sharing parameters across different parts of a model. Parameter sharing\\nmakes it possible to extend and apply the model to examples of different forms\\n(different lengths, here) and generalize across them. If we had separate parameters\\nfor each value of the time index, we could not generalize to sequence lengths not\\nseen during training, nor share statistical strength across different sequence lengths\\nand across different positions in time. Such sharing is particularly important when\\na specific piece of information can occur at multiple positions within the sequence.\\nFor example, consider the two sentences “I went to Nepal in 2009” and “In 2009,\\nI went to Nepal.” If we ask a machine learning model to read each sentence and\\nextract the year in which the narrator went to Nepal, we would like it to recognize\\nthe year 2009 as the relevant piece of information, whether it appears in the sixth\\n373'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nword or the second word of the sentence. Suppose that we trained a feedforward\\nnetwork that processes sentences of fixed length. A traditional fully connected\\nfeedforward network would have separate parameters for each input feature, so it\\nwould need to learn all of the rules of the language separately at each position in\\nthe sentence. By comparison, a recurrent neural network shares the same weights\\nacross several time steps.\\nA related idea is the use of convolution across a 1-D temporal sequence. This\\nconvolutional approach is the basis for time-delay neural networks (Lang and\\nHinton, 1988; Waibel et al., 1989; Lang et al., 1990). The convolution operation\\nallows a network to share parameters across time, but is shallow. The output\\nof convolution is a sequence where each member of the output is a function of\\na small number of neighboring members of the input. The idea of parameter'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='sharing manifests in the application of the same convolution kernel at each time\\nstep. Recurrent networks share parameters in a different way. Each member of the\\noutput is a function of the previous members of the output. Each member of the\\noutput is produced using the same update rule applied to the previous outputs.\\nThis recurrent formulation results in the sharing of parameters through a very\\ndeep computational graph.\\nFor the simplicity of exposition, we refer to RNNs as operating on a sequence\\nthat contains vectors x(t) with the time step index t ranging from 1 to τ. In\\npractice, recurrent networks usually operate on minibatches of such sequences,\\nwith a different sequence length τ for each member of the minibatch. We have\\nomitted the minibatch indices to simplify notation. Moreover, the time step index\\nneed not literally refer to the passage of time in the real world. Sometimes it refers\\nonly to the position in the sequence. RNNs may also be applied in two dimensions'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='across spatial data such as images, and even when applied to data involving time,\\nthe network may have connections that go backwards in time, provided that the\\nentire sequence is observed before it is provided to the network.\\nThis chapter extends the idea of a computational graph to include cycles. These\\ncycles represent the influence of the present value of a variable on its own value\\nat a future time step. Such computational graphs allow us to define recurrent\\nneural networks. We then describe many different ways to construct, train, and\\nuse recurrent neural networks.\\nFor more information on recurrent neural networks than is available in this\\nchapter, we refer the reader to the textbook of Graves (2012).\\n374\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\n10.1 Unfolding Computational Graphs\\nA computational graph is a way to formalize the structure of a set of computations,\\nsuch as those involved in mapping inputs and parameters to outputs and loss.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Please refer to section 6.5.1 for a general introduction. In this section we explain\\nthe idea of unfolding a recursive or recurrent computation into a computational\\ngraph that has a repetitive structure, typically corresponding to a chain of events.\\nUnfolding this graph results in the sharing of parameters across a deep network\\nstructure.\\nFor example, consider the classical form of a dynamical system:\\ns(t) = f(s(t 1);θ), (10.1)\\n−\\nwhere s(t) is called the state of the system.\\nEquation 10.1 is recurrent because the definition of s at time t refers back to\\nthe same definition at time t 1.\\n−\\nFor a finite number of time steps τ, the graph can be unfolded by applying\\nthe definition τ 1 times. For example, if we unfold equation 10.1 for τ = 3 time\\n−\\nsteps, we obtain\\ns(3) =f(s(2);θ) (10.2)\\n=f(f(s(1);θ);θ) (10.3)\\nUnfolding the equation by repeatedly applying the definition in this way has\\nyielded an expression that does not involve recurrence. Such an expression can'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='now be represented by a traditional directed acyclic computational graph. The\\nunfolded computational graph of equation 10.1 and equation 10.3 is illustrated in\\nfigure 10.1.\\nss((......)) ss((tt 11)) ss((tt)) ss((tt++11)) ss((......))\\n−−\\nff ff ff ff\\nFigure 10.1: The classical dynamical system described by equation 10.1, illustrated as an\\nunfolded computational graph. Each node represents the state at some time t and the\\nfunction f maps the state at t to the state at t+1. The same parameters (the same value\\nof θ used to parametrize f) are used for all time steps.\\nAs another example, let us consider a dynamical system driven by an external\\nsignal x(t),\\ns(t) = f(s(t 1),x(t);θ), (10.4)\\n−\\n375\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nwhere we seethat the state now containsinformation about the whole pastsequence.\\nRecurrent neural networks can be built in many different ways. Much as\\nalmost any function can be considered a feedforward neural network, essentially'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='any function involving recurrence can be considered a recurrent neural network.\\nMany recurrent neural networks use equation 10.5 or a similar equation to\\ndefine the values of their hidden units. To indicate that the state is the hidden\\nunits of the network, we now rewrite equation 10.4 using the variable h to represent\\nthe state:\\nh(t) = f(h(t 1),x(t);θ), (10.5)\\n−\\nillustrated in figure 10.2, typical RNNs will add extra architectural features such\\nas output layers that read information out of the state h to make predictions.\\nWhen the recurrentnetwork is trainedto perform a task that requirespredicting\\nthe future from the past, the network typically learns to use h(t) as a kind of lossy\\nsummary of the task-relevant aspects of the past sequence of inputs up to t. This\\nsummary is in general necessarily lossy, since it maps an arbitrary length sequence\\n(x(t),x(t 1),x(t 2),...,x(2),x(1)) to a fixed length vector h(t). Depending on the\\n− −'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='training criterion, this summary might selectively keep some aspects of the past\\nsequence with more precision than other aspects. For example, if the RNN is used\\nin statistical language modeling, typically to predict the next word given previous\\nwords, it may not be necessary to store all of the information in the input sequence\\nup to time t, but rather only enough information to predict the rest of the sentence.\\nThe most demanding situation is when we ask h(t) to be rich enough to allow\\none to approximately recover the input sequence, as in autoencoder frameworks\\n(chapter 14).\\nhh hh((......)) hh((tt −−11)) hh((tt)) hh((tt++11)) hh((......))\\nff ff ff f\\nff Unfold\\nxx xx((tt −−11)) xx((tt)) xx((tt++11))\\nFigure 10.2: A recurrent network with no outputs. This recurrent network just processes\\ninformation from the input x by incorporating it into the state h that is passed forward\\nthrough time. (Left)Circuit diagram. The black square indicates a delay of a single time'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='step. (Right)The same network seen as an unfolded computational graph, where each\\nnode is now associated with one particular time instance.\\nEquation 10.5 can be drawn in two different ways. One way to draw the RNN\\nis with a diagram containing one node for every component that might exist in a\\n376\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nphysical implementation of the model, such as a biological neural network. In this\\nview, the network defines a circuit that operates in real time, with physical parts\\nwhose current state can influence their future state, as in the left of figure 10.2.\\nThroughout this chapter, we use a black square in a circuit diagram to indicate\\nthat an interaction takes place with a delay of a single time step, from the state\\nat time t to the state at time t+ 1. The other way to draw the RNN is as an\\nunfolded computational graph, in which each component is represented by many'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='different variables, with one variable per time step, representing the state of the\\ncomponent at that point in time. Each variable for each time step is drawn as a\\nseparate node of the computational graph, as in the right of figure 10.2. What we\\ncall unfolding is the operation that maps a circuit as in the left side of the figure\\nto a computational graph with repeated pieces as in the right side. The unfolded\\ngraph now has a size that depends on the sequence length.\\nWe can represent the unfolded recurrence after t steps with a function g(t):\\nh(t) =g(t)(x(t),x(t 1),x(t 2),...,x(2),x(1)) (10.6)\\n− −\\n=f(h(t 1),x(t);θ) (10.7)\\n−\\nThe function g(t) takes the whole past sequence (x(t),x(t 1),x(t 2),...,x(2),x(1))\\n− −\\nas input and produces the current state, but the unfolded recurrent structure\\nallows us to factorize g(t) into repeated application of a function f. The unfolding\\nprocess thus introduces two major advantages:'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='1. Regardless of the sequence length, the learned model always has the same\\ninput size, because it is specified in terms of transition from one state to\\nanother state, rather than specified in terms of a variable-length history of\\nstates.\\n2. It is possible to use the same transition function f with the same parameters\\nat every time step.\\nThese two factors make it possible to learn a single model f that operates on\\nall time steps and all sequence lengths, rather than needing to learn a separate\\nmodel g(t) for all possible time steps. Learning a single, shared model allows\\ngeneralization to sequence lengths that did not appear in the training set, and\\nallows the model to be estimated with far fewer training examples than would be\\nrequired without parameter sharing.\\nBoth the recurrent graph and the unrolled graph have their uses. The recurrent\\ngraph is succinct. The unfolded graph provides an explicit description of which'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='computations to perform. The unfolded graph also helps to illustrate the idea of\\n377\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\ninformation flow forward in time (computing outputs and losses) and backward\\nin time (computing gradients) by explicitly showing the path along which this\\ninformation flows.\\n10.2 Recurrent Neural Networks\\nArmed with the graph unrolling and parameter sharing ideas of section 10.1, we\\ncan design a wide variety of recurrent neural networks.\\nyy yy((tt 11)) yy((tt)) yy((tt++11))\\n−−\\nLL LL((tt −−11)) LL((tt)) LL((tt++11))\\noo oo((tt −−11)) oo((tt)) oo((tt++11))\\nVV Unfold\\nVV VV VV\\nWW WW WW WW WW\\nhh hh((......)) hh((tt −−11)) hh((tt)) hh((tt++11)) hh((......))\\nUU UU UU UU\\nxx xx((tt −−11)) xx((tt)) xx((tt++11))\\nFigure 10.3: The computational graph to compute the training loss of a recurrent network\\nthat maps an input sequence of x values to a corresponding sequence of output o values.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='A loss L measures how far each ois from the corresponding training targety. When using\\nsoftmax outputs, we assume o is the unnormalized log probabilities. The loss L internally\\ncomputes yˆ = softmax(o) andcomparesthistothetarget y. TheRNNhas inputtohidden\\nconnections parametrized by a weight matrix U, hidden-to-hidden recurrent connections\\nparametrized by a weight matrix W, and hidden-to-output connections parametrized by\\na weight matrix V. Equation 10.8 defines forward propagation in this model. (Left)The\\nRNN and its loss drawn with recurrent connections. (Right)The same seen as an time-\\nunfolded computational graph, where each node is now associated with one particular\\ntime instance.\\nSome examples of important design patterns for recurrent neural networks\\ninclude the following:\\n378\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nRecurrent networks that produce an output at each time step and have\\n•\\nrecurrent connections between hidden units, illustrated in figure 10.3.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Recurrent networks that produce an output at each time step and have\\n•\\nrecurrent connections only from the output at one time step to the hidden\\nunits at the next time step, illustrated in figure 10.4\\nRecurrent networks with recurrent connections between hidden units, that\\n•\\nread an entire sequence and then produce a single output, illustrated in\\nfigure 10.5.\\nfigure 10.3 is a reasonably representative example that we return to throughout\\nmost of the chapter.\\nThe recurrent neural network of figure 10.3 and equation 10.8 is universal in the\\nsense that any function computable by a Turing machine can be computed by such\\na recurrent network of a finite size. The output can be read from the RNN after\\na number of time steps that is asymptotically linear in the number of time steps\\nused by the Turing machine and asymptotically linear in the length of the input\\n(Siegelmann and Sontag, 1991; Siegelmann, 1995; Siegelmann and Sontag, 1995;'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Hyotyniemi, 1996). The functions computable by a Turing machine are discrete,\\nso these results regard exact implementation of the function, not approximations.\\nThe RNN, when used as a Turing machine, takes a binary sequence as input and its\\noutputs must be discretized to provide a binary output. It is possible to compute all\\nfunctions in this setting using a single specific RNN of finite size (Siegelmann and\\nSontag (1995) use 886 units). The “input” of the Turing machine is a specification\\nof the function to be computed, so the same network that simulates this Turing\\nmachine is sufficient for all problems. The theoretical RNN used for the proof\\ncan simulate an unbounded stack by representing its activations and weights with\\nrational numbers of unbounded precision.\\nWe now develop the forward propagation equations for the RNN depicted in\\nfigure 10.3. The figure does not specify the choice of activation function for the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='hidden units. Here we assume the hyperbolic tangent activation function. Also,\\nthe figure does not specify exactly what form the output and loss function take.\\nHere we assume that the output is discrete, as if the RNN is used to predict words\\nor characters. A natural way to represent discrete variables is to regard the output\\no as giving the unnormalized log probabilities of each possible value of the discrete\\nvariable. We can then apply the softmax operation as a post-processing step to\\nobtain a vector yˆ of normalized probabilities over the output. Forward propagation\\nbegins with a specification of the initial state h(0). Then, for each time step from\\n379\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nyy yy((tt 11)) yy((tt)) yy((tt++11))\\n−−\\nLL LL((tt −−11)) LL((tt)) LL((tt++11))\\noo oo((......)) oo((tt −−11)) oo((tt)) oo((tt++11))\\nW W W W\\nV\\nV V V\\nW\\nUnfold\\nhh hh((tt −−11)) hh((tt)) hh((tt++11)) hh((......))\\nU U U U\\nxx xx((tt −−11)) xx((tt)) xx((tt++11))'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Figure 10.4: An RNN whose only recurrence is the feedback connection from the output\\nto the hidden layer. At each time step t, the input is x , the hidden layer activations are\\nt\\nh(t), the outputs are o(t), the targets are y(t) and the loss is L(t). (Left)Circuit diagram.\\n(Right)Unfolded computational graph. Such an RNN is less powerful (can express a\\nsmaller set of functions) than those in the family represented by figure 10.3. The RNN\\nin figure 10.3 can choose to put any information it wants about the past into its hidden\\nrepresentation h and transmit h to the future. The RNN in this figure is trained to\\nput a specific output value into o, and o is the only information it is allowed to send\\nto the future. There are no direct connections from h going forward. The previous h\\nis connected to the present only indirectly, via the predictions it was used to produce.\\nUnless o is very high-dimensional and rich, it will usually lack important information'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='from the past. This makes the RNN in this figure less powerful, but it may be easier to\\ntrain because each time step can be trained in isolation from the others, allowing greater\\nparallelization during training, as described in section 10.2.1.\\n380\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nt = 1 to t = τ, we apply the following update equations:\\na(t) = b+Wh(t 1) +Ux(t) (10.8)\\n−\\nh(t) = tanh(a(t)) (10.9)\\no(t) = c+V h(t) (10.10)\\nyˆ(t) = softmax(o(t)) (10.11)\\nwhere the parameters are the bias vectors b and c along with the weight matrices\\nU, V and W, respectively for input-to-hidden, hidden-to-output and hidden-to-\\nhidden connections. This is an example of a recurrent network that maps an\\ninput sequence to an output sequence of the same length. The total loss for a\\ngiven sequence of x values paired with a sequence of y values would then be just\\nthe sum of the losses over all the time steps. For example, if L(t) is the negative'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='log-likelihood of y(t) given x(1),...,x(t), then\\nL x(1),...,x(τ) , y(1),...,y(τ) (10.12)\\n{ } { }\\n= \\ue010 L(t) \\ue011 (10.13)\\nt\\n\\ue058\\n= logp y(t) x(1),...,x(t) , (10.14)\\nmodel\\n− | { }\\nt\\n\\ue058 \\ue010 \\ue011\\nwhere p y(t) x(1),...,x(t) is given by reading the entry for y(t) from the\\nmodel\\n| { }\\nmodel’soutputvectoryˆ(t). Computingthegradientofthislossfunctionwithrespect\\n\\ue000 \\ue001\\nto the parameters is an expensive operation. The gradient computation involves\\nperforming a forward propagation pass moving left to right through our illustration\\nof the unrolled graph in figure 10.3, followed by a backward propagation pass\\nmoving right to left through the graph. The runtime is O(τ) and cannot be reduced\\nby parallelization because the forward propagation graph is inherently sequential;\\neach time step may only be computed after the previous one. States computed\\nin the forward pass must be stored until they are reused during the backward\\npass, so the memory cost is also O(τ). The back-propagation algorithm applied'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to the unrolled graph with O(τ) cost is called back-propagation through time\\nor BPTT and is discussed further in section 10.2.2. The network with recurrence\\nbetween hidden units is thus very powerful but also expensive to train. Is there an\\nalternative?\\n10.2.1 Teacher Forcing and Networks with Output Recurrence\\nThe network with recurrent connections only from the output at one time step to\\nthe hidden units at the next time step (shown in figure 10.4) is strictly less powerful\\n381\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nbecause it lacks hidden-to-hidden recurrent connections. For example, it cannot\\nsimulate a universal Turing machine. Because this network lacks hidden-to-hidden\\nrecurrence, it requires that the output units capture all of the information about\\nthe past that the network will use to predict the future. Because the output units\\nare explicitly trained to match the training set targets, they are unlikely to capture'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the necessary information about the past history of the input, unless the user\\nknows how to describe the full state of the system and provides it as part of the\\ntraining set targets. The advantage of eliminating hidden-to-hidden recurrence\\nis that, for any loss function based on comparing the prediction at time t to the\\ntraining target at time t, all the time steps are decoupled. Training can thus be\\nparallelized, with the gradient for each step t computed in isolation. There is no\\nneed to compute the output for the previous time step first, because the training\\nset provides the ideal value of that output.\\nLL((ττ))\\nyy((ττ)) oo((ττ))\\nV\\n......\\nW\\nhh((tt −−11))\\nW\\nhh((tt))\\nW\\n......\\nW\\nhh((ττ))\\nU U U U\\nxx((tt −−11)) xx((tt)) xx((......)) xx((ττ))\\nFigure 10.5: Time-unfolded recurrent neural network with a single output at the end\\nof the sequence. Such a network can be used to summarize a sequence and produce a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='fixed-size representation used as input for further processing. There might be a target\\nright at the end (as depicted here) or the gradient on the output o(t) can be obtained by\\nback-propagating from further downstream modules.\\nModels that have recurrent connections from their outputs leading back into\\nthe model may be trained with teacher forcing. Teacher forcing is a procedure\\nthat emerges from the maximum likelihood criterion, in which during training the\\nmodel receives the ground truth output y(t) as input at time t+ 1. We can see\\nthis by examining a sequence with two time steps. The conditional maximum\\n382\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nyy((tt 11)) yy((tt))\\n−−\\nLL((tt −−11)) LL((tt))\\nW\\noo((tt −−11)) oo((tt)) oo((tt −−11)) oo((tt))\\nW\\nV V V V\\nhh((tt −−11)) hh((tt)) hh((tt −−11)) hh((tt))\\nU U U U\\nxx((tt −−11)) xx((tt)) xx((tt −−11)) xx((tt))\\nTrain time Test time\\nFigure 10.6: Illustration of teacher forcing. Teacher forcing is a training technique that is'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='applicable to RNNs that have connections from their output to their hidden states at the\\nnext time step. (Left)At train time, we feed the correct output y(t) drawn from the train\\nset as input to h(t+1). (Right)When the model is deployed, the true output is generally\\nnot known. In this case, we approximate the correct output y(t) with the model’s output\\no(t), and feed the output back into the model.\\n383\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nlikelihood criterion is\\nlogp y(1),y(2) x(1),x(2) (10.15)\\n|\\n\\ue010 \\ue011\\n=logp y(2) y(1),x(1),x(2) +logp y(1) x(1),x(2) (10.16)\\n| |\\n\\ue010 \\ue011 \\ue010 \\ue011\\nIn this example, we see that at time t = 2, the model is trained to maximize the\\nconditional probability of y(2) given both the x sequence so far and the previous y\\nvalue from the training set. Maximum likelihood thus specifies that during training,\\nrather than feeding the model’s own output back into itself, these connections'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='should be fed with the target values specifying what the correct output should be.\\nThis is illustrated in figure 10.6.\\nWe originallymotivatedteacherforcing asallowingus toavoid back-propagation\\nthrough time in models that lack hidden-to-hidden connections. Teacher forcing\\nmay still be applied to models that have hidden-to-hidden connections so long as\\nthey have connections from the output at one time step to values computed in the\\nnext time step. However, as soon as the hidden units become a function of earlier\\ntime steps, the BPTT algorithm is necessary. Some models may thus be trained\\nwith both teacher forcing and BPTT.\\nThe disadvantage of strict teacher forcing arises if the network is going to be\\nlater used in an open-loop mode, with the network outputs (or samples from\\nthe output distribution) fed back as input. In this case, the kind of inputs that\\nthe network sees during training could be quite different from the kind of inputs'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='that it will see at test time. One way to mitigate this problem is to train with\\nboth teacher-forced inputs and with free-running inputs, for example by predicting\\nthe correct target a number of steps in the future through the unfolded recurrent\\noutput-to-input paths. In this way, the network can learn to take into account\\ninput conditions (such as those it generates itself in the free-running mode) not\\nseen during training and how to map the state back towards one that will make\\nthe network generate proper outputs after a few steps. Another approach (Bengio\\net al., 2015b) to mitigate the gap between the inputs seen at train time and the\\ninputs seen at test time randomly chooses to use generated values or actual data\\nvalues as input. This approach exploits a curriculum learning strategy to gradually\\nuse more of the generated values as input.\\n10.2.2 Computing the Gradient in a Recurrent Neural Network\\nComputing the gradient through a recurrent neural network is straightforward.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='One simply applies the generalized back-propagation algorithm of section 6.5.6\\n384\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nto the unrolled computational graph. No specialized algorithms are necessary.\\nGradientsobtained byback-propagationmaythen be usedwith anygeneral-purpose\\ngradient-based techniques to train an RNN.\\nTo gain some intuition for how the BPTT algorithm behaves, we provide an\\nexample of how to compute gradients by BPTT for the RNN equations above\\n(equation 10.8 and equation 10.12). The nodes of our computational graph include\\nthe parameters U, V , W , b and c as well as the sequence of nodes indexed by\\nt for x(t), h(t), o(t) and L(t). For each node N we need to compute the gradient\\nNL recursively, based on the gradient computed at nodes that follow it in the\\n∇\\ngraph. We start the recursion with the nodes immediately preceding the final loss\\n∂L\\n= 1. (10.17)\\n∂L(t)\\nIn this derivation we assume that the outputs o(t) are used as the argument to the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='softmax function to obtain the vector yˆ of probabilities over the output. We also\\nassume that the loss is the negative log-likelihood of the true target y(t) given the\\ninput so far. The gradient L on the outputs at time step t, for all i,t, is as\\no(t)\\n∇\\nfollows:\\n∂L ∂L ∂L(t)\\n(t)\\n( L) = = = yˆ 1 . (10.18)\\n∇o(t) i (t) ∂L(t) (t) i − i,y(t)\\n∂o ∂o\\ni i\\nWe work our way backwards, starting from the end of the sequence. At the final\\ntime step τ, h(τ) only has o(τ) as a descendent, so its gradient is simple:\\nL = V L. (10.19)\\nh(τ) \\ue03e o(τ)\\n∇ ∇\\nWe can then iterate backwards in time to back-propagate gradients through time,\\nfrom t =τ 1 down to t = 1, noting that h(t) (for t < τ) has as descendents both\\n−\\no(t) and h(t+1). Its gradient is thus given by\\n∂h(t+1) \\ue03e ∂o(t) \\ue03e\\nL = ( L)+ ( L) (10.20)\\n∇h(t)\\n∂h(t)\\n∇h(t+1)\\n∂h(t)\\n∇o(t)\\n\\ue020 \\ue021 \\ue020 \\ue021\\n2\\n= W ( L)diag 1 h(t+1) +V ( L) (10.21)\\n\\ue03e h(t+1) \\ue03e o(t)\\n∇ − ∇\\n\\ue012 \\ue013\\n\\ue010 \\ue011\\n2\\nwhere diag 1 h(t+1) indicates the diagonal matrix containing the elements\\n−\\n1\\n(h(t+1) )\\ue0102.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Th\\ue000is is th\\ue001e\\n\\ue011\\nJacobian of the hyperbolic tangent associated with the\\ni\\n−\\nhidden unit i at time t+1.\\n385\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nOnce the gradients on the internal nodes of the computational graph are\\nobtained, we can obtain the gradients on the parameter nodes. Because the\\nparameters are shared across many time steps, we must take some care when\\ndenoting calculus operations involving these variables. The equations we wish to\\nimplement use the bprop method of section 6.5.6, that computes the contribution\\nof a single edge in the computational graph to the gradient. However, the f\\nW\\n∇\\noperator used in calculus takes into account the contribution of W to the value\\nof f due to all edges in the computational graph. To resolve this ambiguity, we\\nintroduce dummy variables W(t) that are defined to be copies of W but with each\\nW(t) used only at time step t. We may then use to denote the contribution\\nW(t)\\n∇\\nof the weights at time step t to the gradient.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Using this notation, the gradient on the remaining parameters is given by:\\n∂o(t) \\ue03e\\nL = L = L (10.22)\\nc o(t) o(t)\\n∇ ∂c ∇ ∇\\n\\ue020 \\ue021\\nt t\\n\\ue058 \\ue058\\n∂h(t) \\ue03e 2\\nL = L = diag 1 h(t) L(10.23)\\n∇b\\n∂b(t)\\n∇h(t)\\n−\\n∇h(t)\\n\\ue020 \\ue021\\nt t \\ue012 \\ue013\\n\\ue058 \\ue058 \\ue010 \\ue011\\n∂L\\nL = o(t) = ( L)h(t) \\ue03e (10.24)\\n∇V (t) ∇V i ∇o(t)\\n\\ue020∂o \\ue021\\nt i i t\\n\\ue058\\ue058 \\ue058\\n∂L\\n(t)\\nL = h (10.25)\\n∇W (t) ∇W(t) i\\n\\ue020∂h \\ue021\\nt i i\\n\\ue058\\ue058\\n2\\n= diag 1 h(t) ( L)h(t 1) \\ue03e (10.26)\\nh(t) −\\n− ∇\\n\\ue058t \\ue012 \\ue010 \\ue011 \\ue013\\n∂L\\n(t)\\nL = h (10.27)\\n∇U (t) ∇U(t) i\\n\\ue020∂h \\ue021\\nt i i\\n\\ue058\\ue058\\n2\\n= diag 1 h(t) ( L)x(t) \\ue03e (10.28)\\nh(t)\\n− ∇\\n\\ue058t \\ue012 \\ue010 \\ue011 \\ue013\\nWe do not need to compute the gradient with respect to x(t) for training because\\nit does not have any parameters as ancestors in the computational graph defining\\nthe loss.\\n386\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\n10.2.3 Recurrent Networks as Directed Graphical Models\\nIn the example recurrent network we have developed so far, the losses L(t) were\\ncross-entropiesbetweentrainingtargetsy(t) andoutputs o(t). Aswithafeedforward'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='network, it is in principle possible to use almost any loss with a recurrent network.\\nThe loss should be chosen based on the task. As with a feedforward network, we\\nusually wish to interpret the output of the RNN as a probability distribution, and\\nwe usually use the cross-entropy associated with that distribution to define the loss.\\nMean squared error is the cross-entropy loss associated with an output distribution\\nthat is a unit Gaussian, for example, just as with a feedforward network.\\nWhen we use a predictive log-likelihood training objective, such as equa-\\ntion 10.12, we train the RNN to estimate the conditional distribution of the next\\nsequence element y(t) given the past inputs. This may mean that we maximize\\nthe log-likelihood\\nlogp(y(t) x(1),...,x(t)), (10.29)\\n|\\nor, if the model includes connections from the output at one time step to the next\\ntime step,\\nlogp(y(t) x(1),...,x(t),y(1),...,y(t 1)). (10.30)\\n−\\n|'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Decomposing the joint probability over the sequence of y values as a series of\\none-step probabilistic predictions is one way to capture the full joint distribution\\nacross the whole sequence. When we do not feed past y values as inputs that\\ncondition the next step prediction, the directed graphical model contains no edges\\nfrom any y(i) in the past to the current y(t). In this case, the outputs y are\\nconditionally independent given the sequence of x values. When we do feed the\\nactual y values (not their prediction, but the actual observed or generated values)\\nback into the network, the directed graphical model contains edges from all y(i)\\nvalues in the past to the current y(t) value.\\nAs a simple example, let us consider the case where the RNN models only a\\nsequence of scalar random variables Y = y(1),...,y(τ) , with no additional inputs\\n{ }\\nx. The input at time step t is simply the output at time step t 1. The RNN then\\n−'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='defines a directed graphical model over the y variables. We parametrize the joint\\ndistribution of these observations using the chain rule (equation 3.6) for conditional\\nprobabilities:\\nτ\\nP(Y ) = P(y(1),...,y(τ)) = P(y(t) y(t 1),y(t 2),...,y(1)) (10.31)\\n− −\\n|\\nt=1\\n\\ue059\\nwhere the right-hand side of the bar is empty for t = 1, of course. Hence the\\nnegative log-likelihood of a set of values y(1),...,y(τ) according to such a model\\n{ }\\n387\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nyy((11)) yy((22)) yy((33)) yy((44)) yy((55)) yy((......))\\nFigure 10.7: Fully connected graphical model for a sequence y(1),y(2),...,y(t),...: every\\npast observation y(i) may influence the conditional distribution of some y(t) (for t > i),\\ngiven the previous values. Parametrizing the graphical model directly according to this\\ngraph (as in equation 10.6) might be very inefficient, with an ever growing number of\\ninputs and parameters for each element of the sequence. RNNs obtain the same full'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='connectivity but efficient parametrization, as illustrated in figure 10.8.\\nis\\nL = L(t) (10.32)\\nt\\n\\ue058\\nwhere\\nL(t) = logP(y(t) = y(t) y(t 1),y(t 2),...,y(1)). (10.33)\\n− −\\n− |\\nhh((11)) hh((22)) hh((33)) hh((44)) hh((55)) hh((......))\\nyy((11)) yy((22)) yy((33)) yy((44)) yy((55)) yy((......))\\nFigure 10.8: Introducing the state variable in the graphical model of the RNN, even\\nthough it is a deterministic function of its inputs, helps to see how we can obtain a very\\nefficient parametrization, based on equation 10.5. Every stage in the sequence (for h(t)\\nand y(t)) involves the same structure (the same number of inputs for each node) and can\\nshare the same parameters with the other stages.\\nThe edgesin agraphical model indicate which variables depend directlyon other\\nvariables. Many graphical models aim to achieve statistical and computational\\nefficiency by omitting edges that do not correspond to strong interactions. For\\n388\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='example, it is common to make the Markov assumption that the graphical model\\nshould only contain edges from y(t k),...,y(t 1) to y(t), rather than containing\\n− −\\n{ }\\nedges from the entire past history. However, in some cases, we believe that all past\\ninputs should have an influence on the next element of the sequence. RNNs are\\nuseful when we believe that the distribution over y(t) may depend on a value of y(i)\\nfrom the distant past in a way that is not captured by the effect of y(i) on y(t 1).\\n−\\nOne way to interpret an RNN as a graphical model is to view the RNN as\\ndefining a graphical model whose structure is the complete graph, able to represent\\ndirect dependencies between any pair of y values. The graphical model over the y\\nvalues with the complete graph structure is shown in figure 10.7. The complete\\ngraph interpretation of the RNN is based on ignoring the hidden units h(t) by\\nmarginalizing them out of the model.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='It is more interesting to consider the graphical model structure of RNNs that\\nresults from regarding the hidden units h(t) as random variables.1 Including the\\nhidden units in the graphical model reveals that the RNN provides a very efficient\\nparametrization of the joint distribution over the observations. Suppose that we\\nrepresented an arbitrary joint distribution over discrete values with a tabular\\nrepresentation—an array containing a separate entry for each possible assignment\\nof values, with the value of that entry giving the probability of that assignment\\noccurring. If y can take onk different values, the tabular representation would\\nhave O(kτ) parameters. By comparison, due to parameter sharing, the number of\\nparameters in the RNN is O(1) as a function of sequence length. The number of\\nparameters in the RNN may be adjusted to control model capacity but is not forced\\nto scale with sequence length. Equation 10.5 shows that the RNN parametrizes'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='long-term relationships between variables efficiently, using recurrent applications\\nof the same function f and same parameters θ at each time step. Figure 10.8\\nillustrates the graphical model interpretation. Incorporating the h(t) nodes in\\nthe graphical model decouples the past and the future, acting as an intermediate\\nquantity between them. A variable y(i) in the distant past may influence a variable\\ny(t) via its effect on h. The structure of this graph shows that the model can be\\nefficiently parametrized by using the same conditional probability distributions at\\neach time step, and that when the variables are all observed, the probability of the\\njoint assignment of all variables can be evaluated efficiently.\\nEven with the efficient parametrization of the graphical model, some operations\\nremain computationally challenging. For example, it is difficult to predict missing\\n1The conditional distribution over these variables given their parents is deterministic. This is'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='perfectlylegitimate,thoughitissomewhatraretodesignagraphicalmodelwithsuchdeterministic\\nhidden units.\\n389\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nvalues in the middle of the sequence.\\nThe price recurrent networks pay for their reduced number of parameters is\\nthat optimizing the parameters may be difficult.\\nThe parameter sharing used in recurrent networks relies on the assumption\\nthat the same parameters can be used for different time steps. Equivalently, the\\nassumption is that the conditional probability distribution over the variables at\\ntime t+1 given the variables at time t isstationary, meaning that the relationship\\nbetween the previous time step and the next time step does not depend on t. In\\nprinciple, it would be possible to use t as an extra input at each time step and let\\nthe learner discover any time-dependence while sharing as much as it can between\\ndifferent time steps. This would already be much better than using a different'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='conditional probability distribution for each t, but the network would then have to\\nextrapolate when faced with new values of t.\\nTo complete our view of an RNN as a graphical model, we must describe how\\nto draw samples from the model. The main operation that we need to perform is\\nsimply to sample from the conditional distribution at each time step. However,\\nthere is one additional complication. The RNN must have some mechanism for\\ndetermining the length of the sequence. This can be achieved in various ways.\\nIn the case when the output is a symbol taken from a vocabulary, one can\\nadd a special symbol corresponding to the end of a sequence (Schmidhuber, 2012).\\nWhen that symbol is generated, the sampling process stops. In the training set,\\nwe insert this symbol as an extra member of the sequence, immediately after x(τ)\\nin each training example.\\nAnother option is to introduce an extra Bernoulli output to the model that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='represents the decision to either continue generation or halt generation at each\\ntime step. This approach is more general than the approach of adding an extra\\nsymbol to the vocabulary, because it may be applied to any RNN, rather than\\nonly RNNs that output a sequence of symbols. For example, it may be applied to\\nan RNN that emits a sequence of real numbers. The new output unit is usually a\\nsigmoid unit trained with the cross-entropy loss. In this approach the sigmoid is\\ntrained to maximize the log-probability of the correct prediction as to whether the\\nsequence ends or continues at each time step.\\nAnother way to determine the sequence length τ is to add an extra output to\\nthe model that predicts the integer τ itself. The model can sample a value of τ\\nand then sample τ steps worth of data. This approach requires adding an extra\\ninput to the recurrent update at each time step so that the recurrent update is\\naware of whether it is near the end of the generated sequence. This extra input'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='can either consist of the value of τ or can consist of τ t, the number of remaining\\n−\\n390\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\ntime steps. Without this extra input, the RNN might generate sequences that\\nend abruptly, such as a sentence that ends before it is complete. This approach is\\nbased on the decomposition\\nP(x(1),...,x(τ)) = P(τ)P(x(1),...,x(τ) τ). (10.34)\\n|\\nThe strategy of predicting τ directly is used for example by Goodfellow et al.\\n(2014d).\\n10.2.4 Modeling Sequences Conditioned on Context with RNNs\\nIn the previous section we described how an RNN could correspond to a directed\\ngraphical model over a sequence of random variables y(t) with no inputs x. Of\\ncourse, our development of RNNs as in equation 10.8 included a sequence of\\ninputs x(1),x(2),...,x(τ). In general, RNNs allow the extension of the graphical\\nmodel view to represent not only a joint distribution over the y variables but'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='also a conditional distribution over y given x. As discussed in the context of\\nfeedforward networks in section 6.2.1.1, any model representing a variable P(y; θ)\\ncan be reinterpreted as a model representing a conditional distribution P(y ω)\\n|\\nwith ω = θ. We can extend such a model to represent a distribution P(y x) by\\n|\\nusing the same P(y ω) as before, but making ω a function of x. In the case of\\n|\\nan RNN, this can be achieved in different ways. We review here the most common\\nand obvious choices.\\nPreviously, we have discussed RNNs that take a sequence of vectors x(t) for\\nt = 1,...,τ as input. Another option is to take only a single vector x as input.\\nWhen x is a fixed-size vector, we can simply make it an extra input of the RNN\\nthat generates the y sequence. Some common ways of providing an extra input to\\nan RNN are:\\n1. as an extra input at each time step, or\\n2. as the initial state h(0), or\\n3. both.\\nThe first and most common approach is illustrated in figure 10.9. The interaction'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='between the input xand each hidden unit vector h(t) is parametrized by a newly\\nintroduced weight matrix R that was absent from the model of only the sequence\\nof y values. The same product x R is added as additional input to the hidden\\n\\ue03e\\nunits at every time step. We can think of the choice of x as determining the value\\n391\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nof x R that is effectively a new bias parameter used for each of the hidden units.\\n\\ue03e\\nThe weights remain independent of the input. We can think of this model as taking\\nthe parameters θ of the non-conditional model and turning them into ω, where\\nthe bias parameters within ω are now a function of the input.\\nyy((tt −−11)) yy((tt)) yy((tt++11)) yy((......))\\nU LL((tt −−11)) U LL((tt)) U LL((tt++11))\\noo((tt −−11)) oo((tt)) oo((tt++11))\\nV V V\\nW W W W\\nss((......)) hh((tt −−11)) hh((tt)) hh((tt++11)) hh((......))\\nR R R R R\\nxx\\nFigure 10.9: An RNN that maps a fixed-length vector x into a distribution over sequences'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Y. This RNN is appropriate for tasks such as image captioning, where a single image is\\nused as input to a model that then produces a sequence of words describing the image.\\nEach element y(t) of the observed output sequence serves both as input (for the current\\ntime step) and, during training, as target (for the previous time step).\\nRather than receiving only a single vector x as input, the RNN may receive\\na sequence of vectors x(t) as input. The RNN described in equation 10.8 corre-\\nsponds to a conditional distribution P(y(1),...,y(τ) x(1),...,x(τ)) that makes a\\n|\\nconditional independence assumption that this distribution factorizes as\\nP(y(t) x(1),...,x(t)). (10.35)\\n|\\nt\\n\\ue059\\nTo remove the conditional independence assumption, we can add connections from\\nthe output at time t to the hidden unit at time t+1, as shown in figure 10.10. The\\nmodel can then represent arbitrary probability distributions over the y sequence.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='This kind of model representing a distribution over a sequence given another\\n392\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nyy((tt 11)) yy((tt)) yy((tt++11))\\n−−\\nLL((tt −−11)) LL((tt)) LL((tt++11))\\nR R R\\noo((tt −−11)) oo((tt)) oo((tt++11))\\nV V V\\nW W W W\\nhh((......)) hh((tt −−11)) hh((tt)) hh((tt++11)) hh((......))\\nU U U\\nxx((tt −−11)) xx((tt)) xx((tt++11))\\nFigure 10.10: A conditional recurrent neural network mapping a variable-length sequence\\nofx values into a distribution over sequences of y values of the same length. Compared to\\nfigure 10.3, this RNN contains connections from the previous output to the current state.\\nThese connections allow this RNN to model an arbitrary distribution over sequences of y\\ngiven sequences of x of the same length. The RNN of figure 10.3 is only able to represent\\ndistributions in which the y values are conditionally independent from each other given\\nthe x values.\\n393\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='sequence still has one restriction, which is that the length of both sequences must\\nbe the same. We describe how to remove this restriction in section 10.4.\\nyy((tt −−11)) yy((tt)) yy((tt++11))\\nLL((tt −−11)) LL((tt)) LL((tt++11))\\noo((tt −−11)) oo((tt)) oo((tt++11))\\ngg((tt −−11)) gg((tt)) gg((tt++11))\\nhh((tt −−11)) hh((tt)) hh((tt++11))\\nxx((tt 11)) xx((tt)) xx((tt++11))\\n−−\\nFigure 10.11: Computation of a typical bidirectional recurrent neural network, meant\\nto learn to map input sequences x to target sequences y, with loss L(t) at each step t.\\nThe h recurrence propagates information forward in time (towards the right) while the\\ng recurrence propagates information backward in time (towards the left). Thus at each\\npoint t, the output units o(t) can benefit from a relevant summary of the past in its h(t)\\ninput and from a relevant summary of the future in its g(t) input.\\n10.3 Bidirectional RNNs\\nAll of the recurrent networks we have considered up to now have a “causal” struc-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='ture, meaning that the state at time t only captures information from the past,\\nx(1),...,x(t 1), and the present input x(t). Some of the models we have discussed\\n−\\nalso allow information from past y values to affect the current state when the y\\nvalues are available.\\nHowever, in many applications we want to output a prediction of y(t) which may\\n394\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\ndepend on the whole input sequence. For example, in speech recognition, the correct\\ninterpretation of the current sound as a phoneme may depend on the next few\\nphonemes because of co-articulation and potentially may even depend on the next\\nfew words because of the linguistic dependencies between nearby words: if there\\nare two interpretations of the current word that are both acoustically plausible, we\\nmay have to look far into the future (and the past) to disambiguate them. This is\\nalso true of handwriting recognition and many other sequence-to-sequence learning'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='tasks, described in the next section.\\nBidirectional recurrent neural networks (or bidirectional RNNs) were invented\\nto address that need (Schuster and Paliwal, 1997). They have been extremely suc-\\ncessful (Graves, 2012) in applications where that need arises, such as handwriting\\nrecognition (Graves et al., 2008; Graves and Schmidhuber, 2009), speech recogni-\\ntion (Graves and Schmidhuber, 2005; Graves et al., 2013) and bioinformatics (Baldi\\net al., 1999).\\nAs the name suggests, bidirectional RNNs combine an RNN that moves forward\\nthrough time beginning from the start of the sequence with another RNN that\\nmoves backward through time beginning from the end of the sequence. Figure 10.11\\nillustrates the typical bidirectional RNN, with h(t) standing for the state of the\\nsub-RNN that moves forward through time and g(t) standing for the state of the\\nsub-RNN that moves backward through time. This allows the output units o(t)'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='to compute a representation that depends on both the past and the future but\\nis most sensitive to the input values around time t, without having to specify a\\nfixed-size window around t (as one would have to do with a feedforward network,\\na convolutional network, or a regular RNN with a fixed-size look-ahead buffer).\\nThis idea can be naturally extended to 2-dimensional input, such as images, by\\nhaving four RNNs, each one going in one of the four directions: up, down, left,\\nright. At each point (i,j) of a 2-D grid, an output O could then compute a\\ni,j\\nrepresentation that would capture mostly local information but could also depend\\non long-range inputs, if the RNN is able to learn to carry that information.\\nCompared to a convolutional network, RNNs applied to images are typically more\\nexpensive but allow for long-range lateral interactions between features in the\\nsame feature map (Visin et al., 2015; Kalchbrenner et al., 2015). Indeed, the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='forward propagation equations for such RNNs may be written in a form that shows\\nthey use a convolution that computes the bottom-up input to each layer, prior\\nto the recurrent propagation across the feature map that incorporates the lateral\\ninteractions.\\n395\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\n10.4 Encoder-Decoder Sequence-to-Sequence Architec-\\ntures\\nWe have seen in figure 10.5 how an RNN can map an input sequence to a fixed-size\\nvector. We have seen in figure 10.9 how an RNN can map a fixed-size vector to a\\nsequence. We have seen in figures 10.3, 10.4, 10.10 and 10.11 how an RNN can\\nmap an input sequence to an output sequence of the same length.\\nEncoder\\n…\\nxx((11)) xx((22)) xx((......)) xx((nnxx))\\nCC\\nDecoder\\n…\\nyy((11)) yy((22)) yy((......)) yy((nnyy))\\nFigure 10.12: Example of an encoder-decoder or sequence-to-sequence RNN architecture,\\nfor learning to generate an output sequence (y(1),...,y(n y)) given an input sequence'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='(x(1),x(2),...,x(nx)). It is composed of an encoder RNN that reads the input sequence\\nand a decoder RNN that generates the output sequence (or computes the probability of a\\ngiven output sequence). The final hidden state of the encoder RNN is used to compute a\\ngenerally fixed-size context variable C which represents a semantic summary of the input\\nsequence and is given as input to the decoder RNN.\\nHere we discuss how an RNN can be trained to map an input sequence to an\\noutput sequence which is not necessarily of the same length. This comes up in\\nmany applications, such as speech recognition, machine translation or question\\n396\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nanswering, where the input and output sequences in the training set are generally\\nnot of the same length (although their lengths might be related).\\nWe often call the input to the RNN the “context.” We want to produce a\\nrepresentation of this context, C. The context C might be a vector or sequence of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='vectors that summarize the input sequence X = (x(1),...,x(n x)).\\nThe simplest RNN architecture for mapping a variable-length sequence to\\nanother variable-length sequence was first proposed by Cho et al. (2014a) and\\nshortly after by Sutskever et al. (2014), who independently developed that archi-\\ntecture and were the first to obtain state-of-the-art translation using this approach.\\nThe former system is based on scoring proposals generated by another machine\\ntranslation system, while the latter uses a standalone recurrent network to generate\\nthe translations. These authors respectively called this architecture, illustrated\\nin figure 10.12, the encoder-decoder or sequence-to-sequence architecture. The\\nidea is very simple: (1) an encoder or reader or input RNN processes the input\\nsequence. The encoder emits the context C, usually as a simple function of its\\nfinal hidden state. (2) a decoder or writer or output RNN is conditioned on'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='that fixed-length vector (just like in figure 10.9) to generate the output sequence\\nY = (y(1),...,y(ny)). The innovation of this kind of architecture over those\\npresented in earlier sections of this chapter is that the lengths n and n can\\nx y\\nvary from each other, while previous architectures constrained n = n = τ. In a\\nx y\\nsequence-to-sequence architecture, the two RNNs are trained jointly to maximize\\nthe average of logP(y(1),...,y(ny) x(1),...,x(nx)) over all the pairs of x and y\\n|\\nsequences in the training set. The last state h of the encoder RNN is typically\\nnx\\nused as a representation C of the input sequence that is provided as input to the\\ndecoder RNN.\\nIf the context C is a vector, then the decoder RNN is simply a vector-to-\\nsequence RNN as described in section 10.2.4. As we have seen, there are at least\\ntwo ways for a vector-to-sequence RNN to receive input. The input can be provided\\nas the initial state of the RNN, or the input can be connected to the hidden units'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='at each time step. These two ways can also be combined.\\nThere is no constraint that the encoder must have the same size of hidden layer\\nas the decoder.\\nOne clear limitation of this architecture is when the context C output by the\\nencoder RNN has a dimension that is too small to properly summarize a long\\nsequence. This phenomenon was observed by Bahdanau et al. (2015) in the context\\nof machine translation. They proposed to make C a variable-length sequence rather\\nthan a fixed-size vector. Additionally, they introduced an attention mechanism\\nthat learns to associate elements of the sequence C to elements of the output\\n397\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nsequence. See section 12.4.5.1 for more details.\\n10.5 Deep Recurrent Networks\\nThe computation in most RNNs can be decomposed into three blocks of parameters\\nand associated transformations:\\n1. from the input to the hidden state,\\n2. from the previous hidden state to the next hidden state, and'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='3. from the hidden state to the output.\\nWith the RNN architecture of figure 10.3, each of these three blocks is associated\\nwith a single weight matrix. In other words, when the network is unfolded, each\\nof these corresponds to a shallow transformation. By a shallow transformation,\\nwe mean a transformation that would be represented by a single layer within\\na deep MLP. Typically this is a transformation represented by a learned affine\\ntransformation followed by a fixed nonlinearity.\\nWould it be advantageous to introduce depth in each of these operations?\\nExperimental evidence (Graves et al., 2013; Pascanu et al., 2014a) strongly suggests\\nso. The experimental evidence is in agreement with the idea that we need enough\\ndepth in order to perform the required mappings. See also Schmidhuber (1992),\\nEl Hihi and Bengio (1996), or Jaeger (2007a) for earlier work on deep RNNs.\\nGraves et al. (2013) were the first to show a significant benefit of decomposing'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the state of an RNN into multiple layers as in figure 10.13 (left). We can think\\nof the lower layers in the hierarchy depicted in figure 10.13a as playing a role\\nin transforming the raw input into a representation that is more appropriate, at\\nthe higher levels of the hidden state. Pascanu et al. (2014a) go a step further\\nand propose to have a separate MLP (possibly deep) for each of the three blocks\\nenumeratedabove, asillustratedin figure10.13b. Considerationsof representational\\ncapacity suggest to allocate enough capacity in each of these three steps, but doing\\nso by adding depth may hurt learning by making optimization difficult. In general,\\nit is easier to optimize shallower architectures, and adding the extra depth of\\nfigure 10.13b makes the shortest path from a variable in time step t to a variable\\nin time step t+ 1 become longer. For example, if an MLP with a single hidden\\nlayer is used for the state-to-state transition, we have doubled the length of the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='shortest path between variables in any two different time steps, compared with the\\nordinary RNN of figure 10.3. However, as argued by Pascanu et al. (2014a), this\\n398\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\ny y\\ny\\nz h h\\nh\\nx x x\\n(a) (b) (c)\\nFigure 10.13: A recurrent neural network can be made deep in many ways (Pascanu\\net al., 2014a). (a)The hidden recurrent state can be broken down into groups organized\\nhierarchically. (b)Deeper computation (e.g., an MLP) can be introduced in the input-to-\\nhidden, hidden-to-hidden and hidden-to-output parts. This may lengthen the shortest\\npath linking different time steps. (c)The path-lengthening effect can be mitigated by\\nintroducing skip connections.\\n399\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\ncan be mitigated by introducing skip connections in the hidden-to-hidden path, as\\nillustrated in figure 10.13c.\\n10.6 Recursive Neural Networks\\nLL\\noo yy\\nU\\nW\\nU W U W\\nV V V V\\nxx((11)) xx((22)) xx((33)) xx((44))'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Figure 10.14: A recursive network has a computational graph that generalizes that of the\\nrecurrent network from a chain to a tree. A variable-size sequence x(1),x(2),...,x(t) can\\nbe mapped to a fixed-size representation (the output o), with a fixed set of parameters\\n(the weight matrices U, V, W). The figure illustrates a supervised learning case in which\\nsome target y is provided which is associated with the whole sequence.\\nRecursive neural networks2 represent yet another generalization of recurrent\\nnetworks, with a different kind of computational graph, which is structured as a\\ndeep tree, rather than the chain-like structure of RNNs. The typical computational\\ngraph for a recursive network is illustrated in figure 10.14. Recursive neural\\n2We suggest to not abbreviate “recursive neural network” as “RNN” to avoid confusion with\\n“recurrent neural network.”\\n400\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='networks were introduced by Pollack (1990) and their potential use for learning to\\nreason was described by Bottou (2011). Recursive networks have been successfully\\napplied to processing data structures as input to neural nets (Frasconi et al., 1997,\\n1998), in natural language processing (Socher et al., 2011a,c, 2013a) as well as in\\ncomputer vision (Socher et al., 2011b).\\nOne clear advantage of recursive nets over recurrent nets is that for a sequence\\nof the same length τ, the depth (measured as the number of compositions of\\nnonlinear operations) can be drastically reduced from τ to O(logτ), which might\\nhelp deal with long-term dependencies. An open question is how to best structure\\nthe tree. One option is to have a tree structure which does not depend on the data,\\nsuch as a balanced binary tree. In some application domains, external methods\\ncan suggest the appropriate tree structure. For example, when processing natural'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='language sentences, the tree structure for the recursive network can be fixed to\\nthe structure of the parse tree of the sentence provided by a natural language\\nparser (Socher et al., 2011a, 2013a). Ideally, one would like the learner itself to\\ndiscover and infer the tree structure that is appropriate for any given input, as\\nsuggested by Bottou (2011).\\nMany variants of the recursive net idea are possible. For example, Frasconi\\net al. (1997) and Frasconi et al. (1998) associate the data with a tree structure,\\nand associate the inputs and targets with individual nodes of the tree. The\\ncomputation performed by each node does not have to be the traditional artificial\\nneuron computation (affine transformation of all inputs followed by a monotone\\nnonlinearity). For example, Socher et al. (2013a) propose using tensor operations\\nand bilinear forms, which have previously been found useful to model relationships\\nbetween concepts (Weston et al., 2010; Bordes et al., 2012) when the concepts are'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='represented by continuous vectors (embeddings).\\n10.7 The Challenge of Long-Term Dependencies\\nThe mathematical challenge of learning long-term dependencies in recurrent net-\\nworks was introduced in section 8.2.5. The basic problem is that gradients prop-\\nagated over many stages tend to either vanish (most of the time) or explode\\n(rarely, but with much damage to the optimization). Even if we assume that the\\nparameters are such that the recurrent network is stable (can store memories,\\nwith gradients not exploding), the difficulty with long-term dependencies arises\\nfrom the exponentially smaller weights given to long-term interactions (involving\\nthe multiplication of many Jacobians) compared to short-term ones. Many other\\nsources provide a deeper treatment (Hochreiter, 1991; Doya, 1993; Bengio et al.,\\n401\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\n4\\n3\\n2\\n1\\n0\\n1\\n−\\n2\\n−\\n3\\n−\\n4\\n− 60 40 20 0 20 40 60\\n− − −\\nInput coordinate\\ntuptuo\\nfo\\nnoitcejorP\\n0\\n1\\n2\\n3\\n4\\n5'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Figure 10.15: When composing many nonlinear functions (like the linear-tanh layer shown\\nhere), the resultis highly nonlinear, typically withmost of thevaluesassociatedwith a tiny\\nderivative, some values with a large derivative, and many alternations between increasing\\nand decreasing. In this plot, we plot a linear projection of a 100-dimensional hidden state\\ndown to a single dimension, plotted on the y-axis. The x-axis is the coordinate of the\\ninitial state along a random direction in the 100-dimensional space. We can thus view this\\nplot as a linear cross-section of a high-dimensional function. The plots show the function\\nafter each time step, or equivalently, after each number of times the transition function\\nhas been composed.\\n1994; Pascanu et al., 2013) . In this section, we describe the problem in more\\ndetail. The remaining sections describe approaches to overcoming the problem.\\nRecurrent networks involve the composition of the same function multiple'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='times, once per time step. These compositions can result in extremely nonlinear\\nbehavior, as illustrated in figure 10.15.\\nIn particular, the function composition employed by recurrent neural networks\\nsomewhat resembles matrix multiplication. We can think of the recurrence relation\\nh(t) = W h(t 1) (10.36)\\n\\ue03e −\\nas a very simple recurrent neural network lacking a nonlinear activation function,\\nand lacking inputs x. As described in section 8.2.5, this recurrence relation\\nessentially describes the power method. It may be simplified to\\nh(t) = Wt \\ue03e h(0), (10.37)\\n\\ue000 \\ue001\\nand if W admits an eigendecomposition of the form\\nW = QΛQ , (10.38)\\n\\ue03e\\n402\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nwith orthogonal Q, the recurrence may be simplified further to\\nh(t) = Q ΛtQh(0). (10.39)\\n\\ue03e\\nThe eigenvalues are raised to the power of t causing eigenvalues with magnitude\\nless than one to decay to zero and eigenvalues with magnitude greater than one to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='explode. Any component of h(0) that is not aligned with the largest eigenvector\\nwill eventually be discarded.\\nThis problem is particular to recurrent networks. In the scalar case, imagine\\nmultiplying a weight w by itself many times. The product wt will either vanish or\\nexplode depending on the magnitude of w. However, if we make a non-recurrent\\nnetwork that has a different weight w(t) at each time step, the situation is different.\\nIf the initial state is given by1, then the state at time tis given by w(t). Suppose\\nt\\nthat the w(t) values are generated randomly, independently from one another, with\\n\\ue051\\nzero mean and variance v. The variance of the product is O(vn). To obtain some\\ndesired variance v we may choose the individual weights with variance v = √n v .\\n∗ ∗\\nVery deep feedforward networks with carefully chosen scaling can thus avoid the\\nvanishing and exploding gradient problem, as argued by Sussillo (2014).\\nThe vanishing and exploding gradient problem for RNNs was independently'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='discovered by separate researchers (Hochreiter, 1991; Bengio et al., 1993, 1994).\\nOne may hope that the problem can be avoided simply by staying in a region of\\nparameter space where the gradients do not vanish or explode. Unfortunately, in\\norder to store memories in a way that is robust to small perturbations, the RNN\\nmust enter a region of parameter space where gradients vanish (Bengio et al., 1993,\\n1994). Specifically, whenever the model is able to represent long term dependencies,\\nthe gradient of a long term interaction has exponentially smaller magnitude than\\nthe gradient of a short term interaction. It does not mean that it is impossible\\nto learn, but that it might take a very long time to learn long-term dependencies,\\nbecause the signal about these dependencies will tend to be hidden by the smallest\\nfluctuations arising from short-term dependencies. In practice, the experiments\\nin Bengio et al. (1994) show that as we increase the span of the dependencies that'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='need to be captured, gradient-based optimization becomes increasingly difficult,\\nwith the probability of successful training of a traditional RNN via SGD rapidly\\nreaching 0 for sequences of only length 10 or 20.\\nFor a deeper treatment of recurrent networks as dynamical systems, see Doya\\n(1993), Bengio et al. (1994) and Siegelmann and Sontag (1995), with a review\\nin Pascanu et al. (2013). The remaining sections of this chapter discuss various\\napproaches that have been proposed to reduce the difficulty of learning long-\\nterm dependencies (in some cases allowing an RNN to learn dependencies across\\n403\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nhundreds of steps), but the problem of learning long-term dependencies remains\\none of the main challenges in deep learning.\\n10.8 Echo State Networks\\nThe recurrent weights mapping from h(t 1) to h(t) and the input weights mapping\\n−\\nfrom x(t) to h(t) are some of the most difficult parameters to learn in a recurrent'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='network. One proposed (Jaeger, 2003; Maass et al., 2002; Jaeger and Haas, 2004;\\nJaeger, 2007b) approach to avoiding this difficulty is to set the recurrent weights\\nsuch that the recurrent hidden units do a good job of capturing the history of past\\ninputs, and learn only the output weights. This is the idea that was independently\\nproposed forecho state networks or ESNs(Jaeger and Haas,2004; Jaeger, 2007b)\\nand liquid state machines (Maass et al., 2002). The latter is similar, except\\nthat it uses spiking neurons (with binary outputs) instead of the continuous-valued\\nhidden units used for ESNs. Both ESNs and liquid state machines are termed\\nreservoir computing (Lukoševičius and Jaeger, 2009) to denote the fact that\\nthe hidden units form of reservoir of temporal features which may capture different\\naspects of the history of inputs.\\nOne way to think about these reservoir computing recurrent networks is that\\nthey are similar to kernel machines: they map an arbitrary length sequence (the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='history of inputs up to time t) into a fixed-length vector (the recurrent state h(t)),\\non which a linear predictor (typically a linear regression) can be applied to solve\\nthe problem of interest. The training criterion may then be easily designed to be\\nconvex as a function of the output weights. For example, if the output consists\\nof linear regression from the hidden units to the output targets, and the training\\ncriterion is mean squared error, then it is convex and may be solved reliably with\\nsimple learning algorithms (Jaeger, 2003).\\nThe important question is therefore: how do we set the input and recurrent\\nweights so that a rich set of histories can be represented in the recurrent neural\\nnetwork state? The answer proposed in the reservoir computing literature is to\\nview the recurrent net as a dynamical system, and set the input and recurrent\\nweights such that the dynamical system is near the edge of stability.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The original idea was to make the eigenvalues of the Jacobian of the state-to-\\nstate transition function be close to 1. As explained in section 8.2.5, an important\\ncharacteristic of a recurrent network is the eigenvalue spectrum of the Jacobians\\nJ(t) = ∂s(t) . Of particular importance is the spectral radius of J(t), defined to\\n∂s(t 1)\\n−\\nbe the maximum of the absolute values of its eigenvalues.\\n404\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nTo understand the effect of the spectral radius, consider the simple case of\\nback-propagation with a Jacobian matrix J that does not change with t. This\\ncase happens, for example, when the network is purely linear. Suppose that J has\\nan eigenvector v with corresponding eigenvalue λ. Consider what happens as we\\npropagate a gradient vector backwards through time. If we begin with a gradient\\nvector g, then after one step of back-propagation, we will have Jg, and after n'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='steps we will have Jng. Now consider what happens if we instead back-propagate\\na perturbed version of g. If we begin with g + δv, then after one step, we will\\nhave J(g + δv). After n steps, we will have Jn(g + δv). From this we can see\\nthat back-propagation starting from g and back-propagation starting from g+ δv\\ndiverge by δJnv after n steps of back-propagation. If v is chosen to be a unit\\neigenvector of J with eigenvalue λ, then multiplication by the Jacobian simply\\nscales the difference at each step. The two executions of back-propagation are\\nseparated by a distance of δ λ n. When v corresponds to the largest value of λ ,\\n| | | |\\nthis perturbation achieves the widest possible separation of an initial perturbation\\nof size δ.\\nWhen λ > 1, the deviation size δ λ n grows exponentially large. When λ < 1,\\n| | | | | |\\nthe deviation size becomes exponentially small.\\nOf course, this example assumed that the Jacobian was the same at every'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='time step, corresponding to a recurrent network with no nonlinearity. When a\\nnonlinearity is present, the derivative of the nonlinearity will approach zero on\\nmany time steps, and help to prevent the explosion resulting from a large spectral\\nradius. Indeed, the most recent work on echo state networks advocates using a\\nspectral radius much larger than unity (Yildiz et al., 2012; Jaeger, 2012).\\nEverything we have said about back-propagation via repeated matrix multipli-\\ncation applies equally to forward propagation in a network with no nonlinearity,\\nwhere the state h(t+1) = h(t) W.\\n\\ue03e\\nWhen a linear map W always shrinks h as measured by the L2 norm, then\\n\\ue03e\\nwe say that the map is contractive. When the spectral radius is less than one,\\nthe mapping from h(t) to h(t+1) is contractive, so a small change becomes smaller\\nafter each time step. This necessarily makes the network forget information about\\nthe past when we use a finite level of precision (such as 32 bit integers) to store'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the state vector.\\nThe Jacobian matrix tells us how a small change of h(t) propagates one step\\nforward, or equivalently, how the gradient on h(t+1) propagates one step backward,\\nduring back-propagation. Note that neither W nor J need to be symmetric (al-\\nthough they are square and real), so they can have complex-valued eigenvalues and\\neigenvectors, with imaginary components corresponding to potentially oscillatory\\n405\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nbehavior (if the same Jacobian was applied iteratively). Even though h(t) or a\\nsmall variation of h(t) of interest in back-propagation are real-valued, they can\\nbe expressed in such a complex-valued basis. What matters is what happens to\\nthe magnitude (complex absolute value) of these possibly complex-valued basis\\ncoefficients, when we multiply the matrix by the vector. An eigenvalue with\\nmagnitude greater than one corresponds to magnification (exponential growth, if'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='applied iteratively) or shrinking (exponential decay, if applied iteratively).\\nWith a nonlinear map, the Jacobian is free to change at each step. The\\ndynamics therefore become more complicated. However, it remains true that a\\nsmall initial variation can turn into a large variation after several steps. One\\ndifference between the purely linear case and the nonlinear case is that the use of\\na squashing nonlinearity such as tanh can cause the recurrent dynamics to become\\nbounded. Note that it is possible for back-propagation to retain unbounded\\ndynamics even when forward propagation has bounded dynamics, for example,\\nwhen a sequence of tanh units are all in the middle of their linear regime and are\\nconnected by weight matrices with spectral radius greater than 1. However, it is\\nrare for all of the tanh units to simultaneously lie at their linear activation point.\\nThe strategy of echo state networks is simply to fix the weights to have some'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='spectral radius such as 3, where information is carried forward through time but\\ndoes not explode due to the stabilizing effect of saturating nonlinearities like tanh.\\nMore recently, it has been shown that the techniques used to set the weights\\nin ESNs could be used to initialize the weights in a fully trainable recurrent net-\\nwork (with the hidden-to-hidden recurrent weights trained using back-propagation\\nthrough time), helping to learn long-term dependencies (Sutskever, 2012; Sutskever\\net al., 2013). In this setting, an initial spectral radius of 1.2 performs well, combined\\nwith the sparse initialization scheme described in section 8.4.\\n10.9 Leaky Units and Other Strategies for Multiple\\nTime Scales\\nOne way to deal with long-term dependencies is to design a model that operates\\nat multiple time scales, so that some parts of the model operate at fine-grained\\ntime scales and can handle small details, while other parts operate at coarse time'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='scales and transfer information from the distant past to the present more efficiently.\\nVarious strategies for building both fine and coarse time scales are possible. These\\ninclude the addition of skip connections across time, “leaky units” that integrate\\nsignals with different time constants, and the removal of some of the connections\\n406\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nused to model fine-grained time scales.\\n10.9.1 Adding Skip Connections through Time\\nOne way to obtain coarse time scales is to add direct connections from variables in\\nthe distant past to variables in the present. The idea of using such skip connections\\ndates back to Lin et al. (1996) and follows from the idea of incorporating delays in\\nfeedforward neural networks (Lang and Hinton, 1988). In an ordinary recurrent\\nnetwork, a recurrent connection goes from a unit at time t to a unit at time t+1.\\nIt is possible to construct recurrent networks with longer delays (Bengio, 1991).'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='As we have seen in section 8.2.5, gradients may vanish or explode exponentially\\nwith respect to the number of time steps. Lin et al. (1996) introduced recurrent\\nconnections with a time-delay of d to mitigate this problem. Gradients now\\ndiminish exponentially as a function of τ rather than τ. Since there are both\\nd\\ndelayed and single step connections, gradients may still explode exponentially in τ.\\nThis allows the learning algorithm to capture longer dependencies although not all\\nlong-term dependencies may be represented well in this way.\\n10.9.2 Leaky Units and a Spectrum of Different Time Scales\\nAnother way to obtain paths on which the product of derivatives is close to one is to\\nhave units with linear self-connections and a weight near one on these connections.\\nWhen we accumulate a running average µ(t) of some value v(t) by applying the\\nupdate µ(t) αµ(t 1) +(1 α)v(t) the α parameter is an example of a linear self-\\n−\\n← −'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='connection from µ(t 1) to µ(t). When α is near one, the running average remembers\\n−\\ninformation about the past for a long time, and when α is near zero, information\\nabout the past is rapidly discarded. Hidden units with linear self-connections can\\nbehave similarly to such running averages. Such hidden units are called leaky\\nunits.\\nSkip connections through d time steps are a way of ensuring that a unit can\\nalways learn to be influenced by a value from d time steps earlier. The use of a\\nlinear self-connection with a weight near one is a different way of ensuring that the\\nunit can access values from the past. The linear self-connection approach allows\\nthis effect to be adapted more smoothly and flexibly by adjusting the real-valued\\nα rather than by adjusting the integer-valued skip length.\\nThese ideas were proposed by Mozer (1992) and by El Hihi and Bengio (1996).\\nLeaky units were also found to be useful in the context of echo state networks\\n(Jaeger et al., 2007).\\n407'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nThere are two basic strategies for setting the time constants used by leaky\\nunits. One strategy is to manually fix them to values that remain constant, for\\nexample by sampling their values from some distribution once at initialization time.\\nAnother strategy is to make the time constants free parameters and learn them.\\nHaving such leaky units at different time scales appears to help with long-term\\ndependencies (Mozer, 1992; Pascanu et al., 2013).\\n10.9.3 Removing Connections\\nAnother approach to handle long-term dependencies is the idea of organizing\\nthe state of the RNN at multiple time-scales (El Hihi and Bengio, 1996), with\\ninformation flowing more easily through long distances at the slower time scales.\\nThis idea differs from the skip connections through time discussed earlier\\nbecause it involves actively removing length-one connections and replacing them'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='with longer connections. Units modified in such a way are forced to operate on a\\nlong time scale. Skip connections through time add edges. Units receiving such\\nnew connections may learn to operate on a long time scale but may also choose to\\nfocus on their other short-term connections.\\nThere are different ways in which a group of recurrent units can be forced to\\noperate at different time scales. One option is to make the recurrent units leaky,\\nbut to have different groups of units associated with different fixed time scales.\\nThis was the proposal in Mozer (1992) and has been successfully used in Pascanu\\net al. (2013). Another option is to have explicit and discrete updates taking place\\nat different times, with a different frequency for different groups of units. This is\\nthe approach of El Hihi and Bengio (1996) and Koutnik et al. (2014). It worked\\nwell on a number of benchmark datasets.\\n10.10 The Long Short-Term Memory and Other Gated\\nRNNs'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='As of this writing, the most effective sequence models used in practical applications\\nare called gated RNNs. These include the long short-term memory and\\nnetworks based on the gated recurrent unit.\\nLike leaky units, gated RNNs are based on the idea of creating paths through\\ntime that have derivatives that neither vanish nor explode. Leaky units did\\nthis with connection weights that were either manually chosen constants or were\\nparameters. Gated RNNs generalize this to connection weights that may change\\n408\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nat each time step.\\noutput\\n×\\nself-loop\\n+ ×\\nstate\\n×\\ninput input gate forget gate output gate\\nFigure 10.16: Block diagram of the LSTM recurrent network “cell.” Cells are connected\\nrecurrently to each other, replacing the usual hidden units of ordinary recurrent networks.\\nAn input feature is computed with a regular artificial neuron unit. Its value can be'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='accumulated into the state if the sigmoidal input gate allows it. The state unit has a\\nlinear self-loop whose weight is controlled by the forget gate. The output of the cell can\\nbe shut off by the output gate. All the gating units have a sigmoid nonlinearity, while the\\ninput unit can have any squashing nonlinearity. The state unit can also be used as an\\nextra input to the gating units. The black square indicates a delay of a single time step.\\nLeaky units allow the network to accumulate information (such as evidence\\nfor a particular feature or category) over a long duration. However, once that\\ninformation has been used, it might be useful for the neural network to forget the\\nold state. For example, if a sequence is made of sub-sequences and we want a leaky\\nunit to accumulate evidence inside each sub-subsequence, we need a mechanism to\\nforget the old state by setting it to zero. Instead of manually deciding when to'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='clear the state, we want the neural network to learn to decide when to do it. This\\n409\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nis what gated RNNs do.\\n10.10.1 LSTM\\nThe clever idea of introducing self-loops to produce paths where the gradient\\ncan flow for long durations is a core contribution of the initial long short-term\\nmemory (LSTM) model (Hochreiter and Schmidhuber, 1997). A crucial addition\\nhasbeen tomakethe weighton thisself-loopconditioned onthecontext,rather than\\nfixed (Gers et al., 2000). By making the weight of this self-loop gated (controlled\\nby another hidden unit), the time scale of integration can be changed dynamically.\\nIn this case, we mean that even for an LSTM with fixed parameters, the time scale\\nof integration can change based on the input sequence, because the time constants\\nare output by the model itself. The LSTM has been found extremely successful\\nin many applications, such as unconstrained handwriting recognition (Graves'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='et al., 2009), speech recognition (Graves et al., 2013; Graves and Jaitly, 2014),\\nhandwriting generation (Graves, 2013), machine translation (Sutskever et al., 2014),\\nimage captioning (Kiros et al., 2014b; Vinyals et al., 2014b; Xu et al., 2015) and\\nparsing (Vinyals et al., 2014a).\\nThe LSTM block diagram is illustrated in figure 10.16. The corresponding\\nforward propagation equations are given below, in the case of a shallow recurrent\\nnetwork architecture. Deeper architectureshave also been successfully used (Graves\\net al., 2013; Pascanu et al., 2014a). Instead of a unit that simply applies an element-\\nwise nonlinearity to the affine transformation of inputs and recurrent units, LSTM\\nrecurrent networks have “LSTM cells” that have an internal recurrence (a self-loop),\\nin addition to the outer recurrence of the RNN. Each cell has the same inputs\\nand outputs as an ordinary recurrent network, but has more parameters and a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='system of gating units that controls the flow of information. The most important\\n(t)\\ncomponent is the state unit s that has a linear self-loop similar to the leaky\\ni\\nunits described in the previous section. However, here, the self-loop weight (or the\\n(t)\\nassociated time constant) is controlled by a forget gate unit f (for time step t\\ni\\nand cell i), that sets this weight to a value between 0 and 1 via a sigmoid unit:\\n(t) f f (t) f (t 1)\\nf = σ b + U x + W h − , (10.40)\\ni \\uf8eb i i,j j i,j j \\uf8f6\\nj j\\n\\ue058 \\ue058\\n\\uf8ed \\uf8f8\\nwhere x(t) is the current input vector and h(t) is the current hidden layer vector,\\ncontaining the outputs of all the LSTM cells, and bf ,Uf, Wf are respectively\\nbiases, input weights and recurrent weights for the forget gates. The LSTM cell\\n410\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\ninternal state is thus updated as follows, but with a conditional self-loop weight\\n(t)\\nf :\\ni\\n(t) (t) (t 1) (t) (t) (t 1)\\ns\\ni\\n= f\\ni\\ns\\ni\\n− +g\\ni\\nσ \\uf8ebb i+ U i,jx\\nj\\n+ W i,jh\\nj\\n−\\n\\uf8f6\\n, (10.41)\\nj j'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='\\ue058 \\ue058\\n\\uf8ed \\uf8f8\\nwhere b, U and W respectively denote the biases, input weights and recurrent\\n(t)\\nweights into the LSTM cell. The external input gate unit g is computed\\ni\\nsimilarly to the forget gate (with a sigmoid unit to obtain a gating value between\\n0 and 1), but with its own parameters:\\n(t) g g (t) g (t 1)\\ng = σ b + U x + W h − . (10.42)\\ni \\uf8eb i i,j j i,j j \\uf8f6\\nj j\\n\\ue058 \\ue058\\n\\uf8ed \\uf8f8\\n(t) (t)\\nThe output h of the LSTM cell can also be shut off, via the output gate q ,\\ni i\\nwhich also uses a sigmoid unit for gating:\\n(t) (t) (t)\\nh = tanh s q (10.43)\\ni i i\\n\\ue010 \\ue011\\nq(t) = σ bo + Uo x(t) + Wo h(t −1) (10.44)\\ni \\uf8eb i i,j j i,j j \\uf8f6\\nj j\\n\\ue058 \\ue058\\n\\uf8ed \\uf8f8\\nwhich has parameters bo, Uo, W o for its biases, input weights and recurrent\\n(t)\\nweights, respectively. Among the variants, one can choose to use the cell state s\\ni\\nas an extra input (with its weight) into the three gates of the i-th unit, as shown\\nin figure 10.16. This would require three additional parameters.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='LSTM networks have been shown to learn long-term dependencies more easily\\nthan the simple recurrent architectures, first on artificial data sets designed for\\ntesting the ability to learn long-term dependencies (Bengio et al., 1994; Hochreiter\\nand Schmidhuber, 1997; Hochreiter et al., 2001), then on challenging sequence\\nprocessing tasks where state-of-the-art performance was obtained (Graves, 2012;\\nGraves et al., 2013; Sutskever et al., 2014). Variants and alternatives to the LSTM\\nhave been studied and used and are discussed next.\\n10.10.2 Other Gated RNNs\\nWhich pieces of the LSTM architecture are actually necessary? What other\\nsuccessful architectures could be designed that allow the network to dynamically\\ncontrol the time scale and forgetting behavior of different units?\\n411\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nSome answers to these questions are given with the recent work on gated RNNs,'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='whose units are also known as gated recurrent units or GRUs (Cho et al., 2014b;\\nChung et al., 2014, 2015a; Jozefowicz et al., 2015; Chrupala et al., 2015). The main\\ndifference with the LSTM is that a single gating unit simultaneously controls the\\nforgetting factor and the decision to update the state unit. The update equations\\nare the following:\\n(t) (t 1) (t 1) (t 1) (t 1) (t 1) (t 1)\\nh i = u i − h i − +(1 −u i − )σ \\uf8ebb i + U i,jx j − + W i,jr j − h j − \\uf8f6,\\nj j\\n\\ue058 \\ue058\\n\\uf8ed (10\\uf8f8.45)\\nwhere u stands for “update” gate and r for “reset” gate. Their value is defined as\\nusual:\\nu(t) = σ bu + Uu x(t) + Wu h(t) (10.46)\\ni \\uf8eb i i,j j i,j j \\uf8f6\\nj j\\n\\ue058 \\ue058\\n\\uf8ed \\uf8f8\\nand\\nr(t) = σ br+ Ur x(t) + Wr h(t) . (10.47)\\ni \\uf8eb i i,j j i,j j \\uf8f6\\nj j\\n\\ue058 \\ue058\\n\\uf8ed \\uf8f8\\nThe reset and updates gates can individually “ignore” parts of the state vector.\\nThe update gates act like conditional leaky integrators that can linearly gate any\\ndimension, thus choosing to copy it (at one extreme of the sigmoid) or completely'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='ignore it (at the other extreme) by replacing it by the new “target state” value\\n(towards which the leaky integrator wants to converge). The reset gates control\\nwhich parts of the state get used to compute the next target state, introducing an\\nadditional nonlinear effect in the relationship between past state and future state.\\nMany more variants around this theme can be designed. For example the\\nreset gate (or forget gate) output could be shared across multiple hidden units.\\nAlternately, the product of a global gate (covering a whole group of units, such as\\nan entire layer) and a local gate (per unit) could be used to combine global control\\nand local control. However, several investigations over architectural variations\\nof the LSTM and GRU found no variant that would clearly beat both of these\\nacross a wide range of tasks (Greff et al., 2015; Jozefowicz et al., 2015). Greff\\net al. (2015) found that a crucial ingredient is the forget gate, while Jozefowicz'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='et al. (2015) found that adding a bias of 1 to the LSTM forget gate, a practice\\nadvocated by Gers et al. (2000), makes the LSTM as strong as the best of the\\nexplored architectural variants.\\n412\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\n10.11 Optimization for Long-Term Dependencies\\nSection 8.2.5 and section 10.7 have described the vanishing and exploding gradient\\nproblems that occur when optimizing RNNs over many time steps.\\nAn interesting idea proposed by Martens and Sutskever (2011) is that second\\nderivatives may vanish at the same time that first derivatives vanish. Second-order\\noptimization algorithms may roughly be understood as dividing the first derivative\\nby the second derivative (in higher dimension, multiplying the gradient by the\\ninverse Hessian). If the second derivative shrinks at a similar rate to the first\\nderivative, then the ratio of first and second derivatives may remain relatively'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='constant. Unfortunately, second-order methods have many drawbacks, including\\nhigh computational cost, the need for a large minibatch, and a tendency to be\\nattracted to saddle points. Martens and Sutskever (2011) found promising results\\nusing second-order methods. Later, Sutskever et al. (2013) found that simpler\\nmethods such as Nesterov momentum with careful initialization could achieve\\nsimilar results. See Sutskever (2012) for more detail. Both of these approaches\\nhave largely been replaced by simply using SGD (even without momentum) applied\\nto LSTMs. This is part of a continuing theme in machine learning that it is often\\nmuch easier to design a model that is easy to optimize than it is to design a more\\npowerful optimization algorithm.\\n10.11.1 Clipping Gradients\\nAs discussed in section 8.2.4, strongly nonlinear functions such as those computed\\nby a recurrent net over many time steps tend to have derivatives that can be'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='either very large or very small in magnitude. This is illustrated in figure 8.3 and\\nfigure 10.17, in which we see that the objective function (as a function of the\\nparameters) has a “landscape” in which one finds “cliffs”: wide and rather flat\\nregions separated by tiny regions where the objective function changes quickly,\\nforming a kind of cliff.\\nThe difficulty that arises is that when the parameter gradient is very large, a\\ngradient descent parameter update could throw the parameters very far, into a\\nregion where the objective function is larger, undoing much of the work that had\\nbeen done to reach the current solution. The gradient tells us the direction that\\ncorresponds to the steepest descent within an infinitesimal region surrounding the\\ncurrent parameters. Outside of this infinitesimal region, the cost function may\\nbegin to curve back upwards. The update must be chosen to be small enough to\\navoid traversing too much upward curvature. We typically use learning rates that\\n413'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\ndecay slowly enough that consecutive steps have approximately the same learning\\nrate. A step size that is appropriate for a relatively linear part of the landscape is\\noften inappropriate and causes uphill motion if we enter a more curved part of the\\nlandscape on the next step.\\n\\ue057\\ue069\\ue074\\ue068\\ue06f\\ue075\\ue074\\ue020\\ue063\\ue06c\\ue069\\ue070\\ue070\\ue069\\ue06e\\ue067 \\ue057\\ue069\\ue074\\ue068\\ue020\\ue063\\ue06c\\ue069\\ue070\\ue070\\ue069\\ue06e\\ue067\\n\\ue029 \\ue029\\n\\ue062 \\ue062\\n\\ue077\\ue03b \\ue077\\ue03b\\n\\ue028 \\ue028\\n\\ue04a \\ue04a\\n\\ue077 \\ue077\\n\\ue062 \\ue062\\nFigure 10.17: Example of the effect of gradient clipping in a recurrent network with\\ntwo parameters w and b. Gradient clipping can make gradient descent perform more\\nreasonably in the vicinity of extremely steep cliffs. These steep cliffs commonly occur\\nin recurrent networks near where a recurrent network behaves approximately linearly.\\nThe cliff is exponentially steep in the number of time steps because the weight matrix\\nis multiplied by itself once for each time step. (Left)Gradient descent without gradient'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='clipping overshoots the bottom of this small ravine, then receives a very large gradient\\nfrom the cliff face. The large gradient catastrophically propels the parameters outside the\\naxes of the plot. (Right)Gradient descent with gradient clipping has a more moderate\\nreaction to the cliff. While it does ascend the cliff face, the step size is restricted so that\\nit cannot be propelled away from steep region near the solution. Figure adapted with\\npermission from Pascanu et al. (2013).\\nA simple type of solution has been in use by practitioners for many years:\\nclipping the gradient. There are different instances of this idea (Mikolov, 2012;\\nPascanu et al., 2013). One optionis to clip the parameter gradient from aminibatch\\nelement-wise (Mikolov, 2012) just before the parameter update. Another is to clip\\nthe norm g of the gradient g (Pascanu et al., 2013) just before the parameter\\n|| ||\\nupdate:\\nif g > v (10.48)\\n|| ||\\ngv\\ng (10.49)\\n← g\\n|| ||\\n414'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nwhere v is the norm threshold and g is used to update parameters. Because the\\ngradient of all the parameters (including different groups of parameters, such as\\nweights and biases) is renormalized jointly with a single scaling factor, the latter\\nmethod has the advantage that it guarantees that each step is still in the gradient\\ndirection, but experiments suggest that both forms work similarly. Although\\nthe parameter update has the same direction as the true gradient, with gradient\\nnorm clipping, the parameter update vector norm is now bounded. This bounded\\ngradient avoids performing a detrimental step when the gradient explodes. In\\nfact, even simply taking a random step when the gradient magnitude is above\\na threshold tends to work almost as well. If the explosion is so severe that the\\ngradient is numerically Inf or Nan (considered infinite or not-a-number), then'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a random step of size v can be taken and will typically move away from the\\nnumerically unstable configuration. Clipping the gradient norm per-minibatch will\\nnot change the direction of the gradient for an individual minibatch. However,\\ntaking the average of the norm-clipped gradient from many minibatches is not\\nequivalent to clipping the norm of the true gradient (the gradient formed from\\nusing all examples). Examples that have large gradient norm, as well as examples\\nthat appear in the same minibatch as such examples, will have their contribution\\nto the final direction diminished. This stands in contrast to traditional minibatch\\ngradient descent, where the true gradient direction is equal to the average over all\\nminibatch gradients. Put another way, traditional stochastic gradient descent uses\\nan unbiased estimate of the gradient, while gradient descent with norm clipping\\nintroduces a heuristic bias that we know empirically to be useful. With element-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='wise clipping, the direction of the update is not aligned with the true gradient\\nor the minibatch gradient, but it is still a descent direction. It has also been\\nproposed (Graves, 2013) to clip the back-propagated gradient (with respect to\\nhidden units) but no comparison has been published between these variants; we\\nconjecture that all these methods behave similarly.\\n10.11.2 Regularizing to Encourage Information Flow\\nGradient clipping helps to deal with exploding gradients, but it does not help with\\nvanishing gradients. To address vanishing gradients and better capture long-term\\ndependencies, we discussed the idea of creating paths in the computational graph of\\nthe unfolded recurrent architecture along which the product of gradients associated\\nwith arcs is near 1. One approach to achieve this is with LSTMs and other self-\\nloops and gating mechanisms, described above in section 10.10. Another idea is\\nto regularize or constrain the parameters so as to encourage “information flow.”'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='In particular, we would like the gradient vector L being back-propagated to\\nh(t)\\n∇\\n415\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nmaintain its magnitude, even if the loss function only penalizes the output at the\\nend of the sequence. Formally, we want\\n∂h(t)\\n( L) (10.50)\\n∇h(t)\\n∂h(t 1)\\n−\\nto be as large as\\nL. (10.51)\\nh(t)\\n∇\\nWith this objective, Pascanu et al. (2013) propose the following regularizer:\\n2\\n( L)\\n∂h(t)\\n| ∇h(t) ∂h(t 1) |\\nΩ = − 1 . (10.52)\\n\\uf8eb\\ue00c L \\ue00c − \\uf8f6\\nt \\ue00c ||∇h(t) || \\ue00c\\n\\ue058 \\ue00c \\ue00c\\n\\uf8ed \\uf8f8\\nComputing the gradient of this regularizer may appear difficult, but Pascanu\\net al. (2013) propose an approximation in which we consider the back-propagated\\nvectors L as if they were constants (for the purpose of this regularizer, so\\nh(t)\\n∇\\nthat there is no need to back-propagate through them). The experiments with\\nthis regularizer suggest that, if combined with the norm clipping heuristic (which\\nhandles gradient explosion), the regularizer can considerably increase the span of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the dependencies that an RNN can learn. Because it keeps the RNN dynamics\\non the edge of explosive gradients, the gradient clipping is particularly important.\\nWithout gradient clipping, gradient explosion prevents learning from succeeding.\\nA key weakness of this approach is that it is not as effective as the LSTM for\\ntasks where data is abundant, such as language modeling.\\n10.12 Explicit Memory\\nIntelligence requires knowledge and acquiring knowledge can be done via learning,\\nwhich has motivated the development of large-scale deep architectures. However,\\nthere are different kinds of knowledge. Some knowledge can be implicit, sub-\\nconscious, and difficult to verbalize—such as how to walk, or how a dog looks\\ndifferent from a cat. Other knowledge can be explicit, declarative, and relatively\\nstraightforward to put into words—every day commonsense knowledge, like “a cat\\nis a kind of animal,” or very specific facts that you need to know to accomplish'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='your current goals, like “the meeting with the sales team is at 3:00 PM in room\\n141.”\\nNeural networks excel at storing implicit knowledge. However, they struggle to\\nmemorize facts. Stochastic gradient descent requires many presentations of the\\n416\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nMemory cells\\nReading\\nWriting mechanism\\nmechanism\\nTask network,\\ncontrolling the memory\\nFigure 10.18: A schematic of an example of a network with an explicit memory, capturing\\nsome of the key design elements of the neural Turing machine. In this diagram we\\ndistinguish the “representation” part of the model (the “task network,” here a recurrent\\nnet in the bottom) from the “memory” part of the model (the set of cells), which can\\nstore facts. The task network learns to “control” the memory, deciding where to read from\\nand where to write to within the memory (through the reading and writing mechanisms,\\nindicated by bold arrows pointing at the reading and writing addresses).\\n417'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nsame input before it can be stored in a neural network parameters, and even then,\\nthatinput willnot be storedespecially precisely. Graves et al.(2014b) hypothesized\\nthat this is because neural networks lack the equivalent of the working memory\\nsystem that allows human beings to explicitly hold and manipulate pieces of\\ninformation that are relevant to achieving some goal. Such explicit memory\\ncomponents would allow our systems not only to rapidly and “intentionally” store\\nand retrieve specific facts but also to sequentially reason with them. The need\\nfor neural networks that can process information in a sequence of steps, changing\\nthe way the input is fed into the network at each step, has long been recognized\\nas important for the ability to reason rather than to make automatic, intuitive\\nresponses to the input (Hinton, 1990).\\nTo resolve this difficulty, Weston et al. (2014) introduced memory networks'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='that include a set of memory cells that can be accessed via an addressing mecha-\\nnism. Memory networks originally required a supervision signal instructing them\\nhow to use their memory cells. Graves et al. (2014b) introduced the neural\\nTuring machine, which is able to learn to read from and write arbitrary content\\nto memory cells without explicit supervision about which actions to undertake,\\nand allowed end-to-end training without this supervision signal, via the use of\\na content-based soft attention mechanism (see Bahdanau et al. (2015) and sec-\\ntion 12.4.5.1). This soft addressing mechanism has become standard with other\\nrelated architectures emulating algorithmic mechanisms in a way that still allows\\ngradient-based optimization (Sukhbaatar et al., 2015; Joulin and Mikolov, 2015;\\nKumar et al., 2015; Vinyals et al., 2015a; Grefenstette et al., 2015).\\nEach memory cell can be thought of as an extension of the memory cells in'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='LSTMs and GRUs. The difference is that the network outputs an internal state\\nthat chooses which cell to read from or write to, just as memory accesses in a\\ndigital computer read from or write to a specific address.\\nIt is difficult to optimize functions that produce exact, integer addresses. To\\nalleviate this problem, NTMs actually read to or write from many memory cells\\nsimultaneously. To read, they take a weighted average of many cells. To write, they\\nmodify multiple cells by different amounts. The coefficients for these operations\\nare chosen to be focused on a small number of cells, for example, by producing\\nthem via a softmax function. Using these weights with non-zero derivatives allows\\nthe functions controlling access to the memory to be optimized using gradient\\ndescent. The gradient on these coefficients indicates whether each of them should\\nbe increased or decreased, but the gradient will typically be large only for those\\nmemory addresses receiving a large coefficient.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='These memory cells are typically augmented to contain a vector, rather than\\n418\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nthe single scalar stored by an LSTM or GRU memory cell. There are two reasons\\nto increase the size of the memory cell. One reason is that we have increased the\\ncost of accessing a memory cell. We pay the computational cost of producing a\\ncoefficient for many cells, but we expect these coefficients to cluster around a small\\nnumber of cells. By reading a vector value, rather than a scalar value, we can\\noffset some of this cost. Another reason to use vector-valued memory cells is that\\nthey allow for content-based addressing, where the weight used to read to or\\nwrite from a cell is a function of that cell. Vector-valued cells allow us to retrieve a\\ncomplete vector-valued memory if we are able to produce a pattern that matches\\nsome but not all of its elements. This is analogous to the way that people can'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='recall the lyrics of a song based on a few words. We can think of a content-based\\nread instruction as saying, “Retrieve the lyrics of the song that has the chorus ‘We\\nall live in a yellow submarine.’” Content-based addressing is more useful when we\\nmake the objects to be retrieved large—if every letter of the song was stored in a\\nseparate memory cell, we would not be able to find them this way. By comparison,\\nlocation-based addressing is not allowed to refer to the content of the memory.\\nWe can think of a location-based read instruction as saying “Retrieve the lyrics of\\nthe song in slot 347.” Location-based addressing can often be a perfectly sensible\\nmechanism even when the memory cells are small.\\nIf the content of a memory cell is copied (not forgotten) at most time steps, then\\nthe information it contains can be propagated forward in time and the gradients\\npropagated backward in time without either vanishing or exploding.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The explicit memory approach is illustrated in figure 10.18, where we see that\\na “task neural network” is coupled with a memory. Although that task neural\\nnetwork could be feedforward or recurrent, the overall system is a recurrent network.\\nThe task network can choose to read from or write to specific memory addresses.\\nExplicit memory seems to allow models to learn tasks that ordinary RNNs or LSTM\\nRNNs cannot learn. One reason for this advantage may be because information and\\ngradients can be propagated (forward in time or backwards in time, respectively)\\nfor very long durations.\\nAs an alternative to back-propagation through weighted averages of memory\\ncells, we can interpret the memory addressing coefficients as probabilities and\\nstochastically read just one cell (Zaremba and Sutskever, 2015). Optimizing models\\nthat make discrete decisions requires specialized optimization algorithms, described\\nin section 20.9.1. So far, training these stochastic architectures that make discrete'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='decisions remains harder than training deterministic algorithms that make soft\\ndecisions.\\nWhether it is soft (allowing back-propagation) or stochastic and hard, the\\n419\\nCHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\\nmechanism for choosing an address is in its form identical to the attention\\nmechanism which had been previously introduced in the context of machine\\ntranslation (Bahdanau et al., 2015) and discussed in section 12.4.5.1. The idea\\nof attention mechanisms for neural networks was introduced even earlier, in the\\ncontext of handwriting generation (Graves, 2013), with an attention mechanism\\nthat was constrained to move only forward in time through the sequence. In\\nthe case of machine translation and memory networks, at each step, the focus of\\nattention can move to a completely different place, compared to the previous step.\\nRecurrent neural networks provide a way to extend deep learning to sequential'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='data. They are the last major tool in our deep learning toolbox. Our discussion now\\nmoves to how to choose and use these tools and how to apply them to real-world\\ntasks.\\n420\\nChapter 11\\nPractical Methodology\\nSuccessfully applying deep learning techniques requires more than just a good\\nknowledge of what algorithms exist and the principles that explain how they\\nwork. A good machine learning practitioner also needs to know how to choose an\\nalgorithm for a particular application and how to monitor and respond to feedback\\nobtained from experiments in order to improve a machine learning system. During\\nday to day development of machine learning systems, practitioners need to decide\\nwhether to gather more data, increase or decrease model capacity, add or remove\\nregularizing features, improve the optimization of a model, improve approximate\\ninference in a model, or debug the software implementation of the model. All of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='these operations are at the very least time-consuming to try out, so it is important\\nto be able to determine the right course of action rather than blindly guessing.\\nMost of this book is about different machine learning models, training algo-\\nrithms, and objective functions. This may give the impression that the most\\nimportant ingredient to being a machine learning expert is knowing a wide variety\\nof machine learning techniques and being good at different kinds of math. In prac-\\ntice, one can usually do much better with a correct application of a commonplace\\nalgorithm than by sloppily applying an obscure algorithm. Correct application of\\nan algorithm depends on mastering some fairly simple methodology. Many of the\\nrecommendations in this chapter are adapted from Ng (2015).\\nWe recommend the following practical design process:\\nDetermine your goals—what error metric to use, and your target value for\\n•\\nthis error metric. These goals and error metrics should be driven by the'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='problem that the application is intended to solve.\\nEstablish a working end-to-end pipeline as soon as possible, including the\\n•\\n421\\nCHAPTER 11. PRACTICAL METHODOLOGY\\nestimation of the appropriate performance metrics.\\nInstrument the system well to determine bottlenecks in performance. Diag-\\n•\\nnose which components are performing worse than expected and whether it\\nis due to overfitting, underfitting, or a defect in the data or software.\\nRepeatedly make incremental changes such as gathering new data, adjusting\\n•\\nhyperparameters, or changing algorithms, based on specific findings from\\nyour instrumentation.\\nAs a running example, we will use Street View address number transcription\\nsystem (Goodfellow et al., 2014d). The purpose of this application is to add\\nbuildings to Google Maps. Street View cars photograph the buildings and record\\nthe GPS coordinates associated with each photograph. A convolutional network\\nrecognizes the address number in each photograph, allowing the Google Maps'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='database to add that address in the correct location. The story of how this\\ncommercial application was developed gives an example of how to follow the design\\nmethodology we advocate.\\nWe now describe each of the steps in this process.\\n11.1 Performance Metrics\\nDetermining your goals, in terms of which error metric to use, is a necessary first\\nstep because your error metric will guide all of your future actions. You should\\nalso have an idea of what level of performance you desire.\\nKeep in mind that for most applications, it is impossible to achieve absolute\\nzero error. The Bayes error defines the minimum error rate that you can hope to\\nachieve, even if you have infinite training data and can recover the true probability\\ndistribution. This is because your input features may not contain complete\\ninformation about the output variable, or because the system might be intrinsically\\nstochastic. You will also be limited by having a finite amount of training data.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The amount of training data can be limited for a variety of reasons. When your\\ngoal is to build the best possible real-world product or service, you can typically\\ncollect more data but must determine the value of reducing error further and weigh\\nthis against the cost of collecting more data. Data collection can require time,\\nmoney, or human suffering (for example, if your data collection process involves\\nperforming invasive medical tests). When your goal is to answer a scientific question\\nabout which algorithm performs better on a fixed benchmark, the benchmark\\n422\\nCHAPTER 11. PRACTICAL METHODOLOGY\\nspecification usually determines the training set and you are not allowed to collect\\nmore data.\\nHow can one determine a reasonable level of performance to expect? Typically,\\nin the academic setting, we have some estimate of the error rate that is attainable\\nbased on previously published benchmark results. In the real-word setting, we'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='have some idea of the error rate that is necessary for an application to be safe,\\ncost-effective, or appealing to consumers. Once you have determined your realistic\\ndesired error rate, your design decisions will be guided by reaching this error rate.\\nAnother important consideration besides the target value of the performance\\nmetric is the choice of which metric to use. Several different performance metrics\\nmay be used to measure the effectiveness of a complete application that includes\\nmachine learning components. These performance metrics are usually different\\nfrom the cost function used to train the model. As described in section 5.1.2, it is\\ncommon to measure the accuracy, or equivalently, the error rate, of a system.\\nHowever, many applications require more advanced metrics.\\nSometimes it is much more costly to make one kind of a mistake than another.\\nFor example, an e-mail spam detection system can make two kinds of mistakes:'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='incorrectly classifying a legitimate message as spam, and incorrectly allowing a\\nspam message to appear in the inbox. It is much worse to block a legitimate\\nmessage than to allow a questionable message to pass through. Rather than\\nmeasuring the error rate of a spam classifier, we may wish to measure some form\\nof total cost, where the cost of blocking legitimate messages is higher than the cost\\nof allowing spam messages.\\nSometimes we wish to train a binary classifier that is intended to detect some\\nrare event. For example, we might design a medical test for a rare disease. Suppose\\nthat only one in every million people has this disease. We can easily achieve\\n99.9999% accuracy on the detection task, by simply hard-coding the classifier\\nto always report that the disease is absent. Clearly, accuracy is a poor way to\\ncharacterize the performance of such a system. One way to solve this problem is\\nto instead measure precision and recall. Precision is the fraction of detections'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='reported by the model that were correct, while recall is the fraction of true events\\nthat were detected. A detector that says no one has the disease would achieve\\nperfect precision, but zero recall. A detector that says everyone has the disease\\nwould achieve perfect recall, but precision equal to the percentage of people who\\nhave the disease (0.0001% in our example of a disease that only one people in a\\nmillion have). When using precision and recall, it is common to plot a PR curve,\\nwith precision on the y-axis and recall on the x-axis. The classifier generates a score\\nthat is higher if the event to be detected occurred. For example, a feedforward\\n423\\nCHAPTER 11. PRACTICAL METHODOLOGY\\nnetwork designed to detect a disease outputs yˆ = P(y = 1 x), estimating the\\n|\\nprobability that a person whose medical results are described by features x has\\nthe disease. We choose to report a detection whenever this score exceeds some'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='threshold. By varying the threshold, we can trade precision for recall. In many\\ncases, we wish to summarize the performance of the classifier with a single number\\nrather than a curve. To do so, we can convert precision p and recall r into an\\nF-score given by\\n2pr\\nF = . (11.1)\\np + r\\nAnother option is to report the total area lying beneath the PR curve.\\nIn some applications, it is possible for the machine learning system to refuse to\\nmake a decision. This is useful when the machine learning algorithm can estimate\\nhow confident it should be about a decision, especially if a wrong decision can\\nbe harmful and if a human operator is able to occasionally take over. The Street\\nView transcription system provides an example of this situation. The task is to\\ntranscribe the address number from a photograph in order to associate the location\\nwhere the photo was taken with the correct address in a map. Because the value'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='of the map degrades considerably if the map is inaccurate, it is important to add\\nan address only if the transcription is correct. If the machine learning system\\nthinks that it is less likely than a human being to obtain the correct transcription,\\nthen the best course of action is to allow a human to transcribe the photo instead.\\nOf course, the machine learning system is only useful if it is able to dramatically\\nreduce the amount of photos that the human operators must process. A natural\\nperformance metric to use in this situation is coverage. Coverage is the fraction\\nof examples for which the machine learning system is able to produce a response.\\nIt is possible to trade coverage for accuracy. One can always obtain 100% accuracy\\nby refusing to process any example, but this reduces the coverage to 0%. For the\\nStreet View task, the goal for the project was to reach human-level transcription\\naccuracy while maintaining 95% coverage. Human-level performance on this task\\nis 98% accuracy.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Many other metrics are possible. We can for example, measure click-through\\nrates, collect user satisfaction surveys, and so on. Many specialized application\\nareas have application-specific criteria as well.\\nWhat is important is to determine which performance metric to improve ahead\\nof time, then concentrate on improving this metric. Without clearly defined goals,\\nit can be difficult to tell whether changes to a machine learning system make\\nprogress or not.\\n424\\nCHAPTER 11. PRACTICAL METHODOLOGY\\n11.2 Default Baseline Models\\nAfter choosing performance metrics and goals, the next step in any practical\\napplication is to establish a reasonable end-to-end system as soon as possible. In\\nthis section, we provide recommendations for which algorithms to use as the first\\nbaseline approach in various situations. Keep in mind that deep learning research\\nprogresses quickly, so better default algorithms are likely to become available soon\\nafter this writing.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Depending on the complexity of your problem, you may even want to begin\\nwithout using deep learning. If your problem has a chance of being solved by\\njust choosing a few linear weights correctly, you may want to begin with a simple\\nstatistical model like logistic regression.\\nIf you know that your problem falls into an “AI-complete” category like object\\nrecognition, speech recognition, machine translation, and so on, then you are likely\\nto do well by beginning with an appropriate deep learning model.\\nFirst, choose the general category of model based on the structure of your\\ndata. If you want to perform supervised learning with fixed-size vectors as input,\\nuse a feedforward network with fully connected layers. If the input has known\\ntopological structure (for example, if the input is an image), use a convolutional\\nnetwork. In these cases, you should begin by using some kind of piecewise linear\\nunit (ReLUs or their generalizations like Leaky ReLUs, PreLus and maxout). If'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='your input or output is a sequence, use a gated recurrent net (LSTM or GRU).\\nA reasonable choice of optimization algorithm is SGD with momentum with a\\ndecaying learning rate (popular decay schemes that perform better or worse on\\ndifferent problems include decaying linearly until reaching a fixed minimum learning\\nrate, decaying exponentially, or decreasing the learning rate by a factor of 2-10\\neach time validation error plateaus). Another very reasonable alternative is Adam.\\nBatch normalization can have a dramatic effect on optimization performance,\\nespecially for convolutional networks and networks with sigmoidal nonlinearities.\\nWhile it is reasonable to omit batch normalization from the very first baseline, it\\nshould be introduced quickly if optimization appears to be problematic.\\nUnless your training set contains tens of millions of examples or more, you\\nshould include some mild forms of regularization from the start. Early stopping'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='should be used almost universally. Dropout is an excellent regularizer that is easy\\nto implement and compatible with many models and training algorithms. Batch\\nnormalization also sometimes reduces generalization error and allows dropout to\\nbe omitted, due to the noise in the estimate of the statistics used to normalize\\neach variable.\\n425\\nCHAPTER 11. PRACTICAL METHODOLOGY\\nIf your task is similar to another task that has been studied extensively, you\\nwill probably do well by first copying the model and algorithm that is already\\nknown to perform best on the previously studied task. You may even want to copy\\na trained model from that task. For example, it is common to use the features\\nfrom a convolutional network trained on ImageNet to solve other computer vision\\ntasks (Girshick et al., 2015).\\nA common question is whether to begin by using unsupervised learning, de-\\nscribed further in part III. This is somewhat domain specific. Some domains, such'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='as natural language processing, are known to benefit tremendously from unsuper-\\nvised learning techniques such as learning unsupervised word embeddings. In other\\ndomains, such as computer vision, current unsupervised learning techniques do\\nnot bring a benefit, except in the semi-supervised setting, when the number of\\nlabeled examples is very small (Kingma et al., 2014; Rasmus et al., 2015). If your\\napplication is in a context where unsupervised learning is known to be important,\\nthen include it in your first end-to-end baseline. Otherwise, only use unsupervised\\nlearning in your first attempt if the task you want to solve is unsupervised. You\\ncan always try adding unsupervised learning later if you observe that your initial\\nbaseline overfits.\\n11.3 Determining Whether to Gather More Data\\nAfter the first end-to-end system is established, it is time to measure the perfor-\\nmance of the algorithm and determine how to improve it. Many machine learning'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='novices are tempted to make improvements by trying out many different algorithms.\\nHowever, it is often much better to gather more data than to improve the learning\\nalgorithm.\\nHow does one decide whether to gather more data? First, determine whether\\nthe performance on the training set is acceptable. If performance on the training\\nset is poor, the learning algorithm is not using the training data that is already\\navailable, so there is no reason to gather more data. Instead, try increasing the\\nsize of the model by adding more layers or adding more hidden units to each layer.\\nAlso, try improving the learning algorithm, for example by tuning the learning\\nrate hyperparameter. If large models and carefully tuned optimization algorithms\\ndo not work well, then the problem might be the quality of the training data. The\\ndata may be too noisy or may not include the right inputs needed to predict the\\ndesired outputs. This suggests starting over, collecting cleaner data or collecting a'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='richer set of features.\\nIf the performance on the training set is acceptable, then measure the per-\\n426\\nCHAPTER 11. PRACTICAL METHODOLOGY\\nformance on a test set. If the performance on the test set is also acceptable,\\nthen there is nothing left to be done. If test set performance is much worse than\\ntraining set performance, then gathering more data is one of the most effective\\nsolutions. The key considerations are the cost and feasibility of gathering more\\ndata, the cost and feasibility of reducing the test error by other means, and the\\namount of data that is expected to be necessary to improve test set performance\\nsignificantly. At large internet companies with millions or billions of users, it is\\nfeasible to gather large datasets, and the expense of doing so can be considerably\\nless than the other alternatives, so the answer is almost always to gather more\\ntraining data. For example, the development of large labeled datasets was one of'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='the most important factors in solving object recognition. In other contexts, such as\\nmedical applications, it may be costly or infeasible to gather more data. A simple\\nalternative to gathering more data is to reduce the size of the model or improve\\nregularization, by adjusting hyperparameters such as weight decay coefficients,\\nor by adding regularization strategies such as dropout. If you find that the gap\\nbetween train and test performance is still unacceptable even after tuning the\\nregularization hyperparameters, then gathering more data is advisable.\\nWhen deciding whether to gather more data, it is also necessary to decide\\nhow much to gather. It is helpful to plot curves showing the relationship between\\ntraining set size and generalization error, like in figure 5.4. By extrapolating such\\ncurves, one can predict how much additional training data would be needed to\\nachieve a certain level of performance. Usually, adding a small fraction of the total'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='number of examples will not have a noticeable impact on generalization error. It is\\ntherefore recommended to experiment with training set sizes on a logarithmic scale,\\nfor example doubling the number of examples between consecutive experiments.\\nIf gathering much more data is not feasible, the only other way to improve\\ngeneralization error is to improve the learning algorithm itself. This becomes the\\ndomain of research and not the domain of advice for applied practitioners.\\n11.4 Selecting Hyperparameters\\nMost deep learning algorithms come with many hyperparameters that control many\\naspects of the algorithm’s behavior. Some of these hyperparameters affect the time\\nand memory cost of running the algorithm. Some of these hyperparameters affect\\nthe quality of the model recovered by the training process and its ability to infer\\ncorrect results when deployed on new inputs.\\nThere are two basic approaches to choosing these hyperparameters: choosing'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='them manually and choosing them automatically. Choosing the hyperparameters\\n427\\nCHAPTER 11. PRACTICAL METHODOLOGY\\nmanually requires understanding what the hyperparameters do and how machine\\nlearning models achieve good generalization. Automatic hyperparameter selection\\nalgorithms greatly reduce the need to understand these ideas, but they are often\\nmuch more computationally costly.\\n11.4.1 Manual Hyperparameter Tuning\\nTo set hyperparameters manually, one must understand the relationship between\\nhyperparameters, training error, generalization error and computational resources\\n(memory and runtime). This means establishing a solid foundation on the fun-\\ndamental ideas concerning the effective capacity of a learning algorithm from\\nchapter 5.\\nThe goal of manual hyperparameter search is usually to find the lowest general-\\nization error subject to some runtime and memory budget. We do not discuss how\\nto determine the runtime and memory impact of various hyperparameters here'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='because this is highly platform-dependent.\\nThe primary goal of manual hyperparameter search is to adjust the effective\\ncapacity of the model to match the complexity of the task. Effective capacity\\nis constrained by three factors: the representational capacity of the model, the\\nability of the learning algorithm to successfully minimize the cost function used to\\ntrain the model, and the degree to which the cost function and training procedure\\nregularize the model. A model with more layers and more hidden units per layer has\\nhigher representational capacity—it is capable of representing more complicated\\nfunctions. It can not necessarily actually learn all of these functions though, if\\nthe training algorithm cannot discover that certain functions do a good job of\\nminimizing the training cost, or if regularization terms such as weight decay forbid\\nsome of these functions.\\nThe generalization error typically follows a U-shaped curve when plotted as'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='a function of one of the hyperparameters, as in figure 5.3. At one extreme, the\\nhyperparameter value corresponds to low capacity, and generalization error is high\\nbecause training error is high. This is the underfitting regime. At the other extreme,\\nthe hyperparameter value corresponds to high capacity, and the generalization\\nerror is high because the gap between training and test error is high. Somewhere\\nin the middle lies the optimal model capacity, which achieves the lowest possible\\ngeneralization error, by adding a medium generalization gap to a medium amount\\nof training error.\\nFor some hyperparameters, overfitting occurs when the value of the hyper-\\nparameter is large. The number of hidden units in a layer is one such example,\\n428\\nCHAPTER 11. PRACTICAL METHODOLOGY\\nbecause increasing the number of hidden units increases the capacity of the model.\\nFor some hyperparameters, overfitting occurs when the value of the hyperparame-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='ter is small. For example, the smallest allowable weight decay coefficient of zero\\ncorresponds to the greatest effective capacity of the learning algorithm.\\nNot every hyperparameter will be able to explore the entire U-shaped curve.\\nMany hyperparameters are discrete, such as the number of units in a layer or the\\nnumber of linear pieces in a maxout unit, so it is only possible to visit a few points\\nalong the curve. Some hyperparameters are binary. Usually these hyperparameters\\nare switches that specify whether or not to use some optional component of\\nthe learning algorithm, such as a preprocessing step that normalizes the input\\nfeatures by subtracting their mean and dividing by their standard deviation. These\\nhyperparameters can only explore two points on the curve. Other hyperparameters\\nhave some minimum or maximum value that prevents them from exploring some\\npart of the curve. For example, the minimum weight decay coefficient is zero. This'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='means that if the model is underfitting when weight decay is zero, we can not enter\\nthe overfitting region by modifying the weight decay coefficient. In other words,\\nsome hyperparameters can only subtract capacity.\\nThe learning rate is perhaps the most important hyperparameter. If you\\nhave time to tune only one hyperparameter, tune the learning rate. It con-\\ntrols the effective capacity of the model in a more complicated way than other\\nhyperparameters—the effective capacity of the model is highest when the learning\\nrate is correct for the optimization problem, not when the learning rate is especially\\nlarge or especially small. The learning rate has a U-shaped curve for training error,\\nillustrated in figure 11.1. When the learning rate is too large, gradient descent\\ncan inadvertently increase rather than decrease the training error. In the idealized\\nquadratic case, this occurs if the learning rate is at least twice as large as its'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='optimal value (LeCun et al., 1998a). When the learning rate is too small, training\\nis not only slower, but may become permanently stuck with a high training error.\\nThis effect is poorly understood (it would not happen for a convex loss function).\\nTuning the parameters other than the learning rate requires monitoring both\\ntraining and test error to diagnose whether your model is overfitting or underfitting,\\nthen adjusting its capacity appropriately.\\nIf your error on the training set is higher than your target error rate, you have\\nno choice but to increase capacity. If you are not using regularization and you are\\nconfident that your optimization algorithm is performing correctly, then you must\\nadd more layers to your network or add more hidden units. Unfortunately, this\\nincreases the computational costs associated with the model.\\nIf your error on the test set is higher than than your target error rate, you can\\n429\\nCHAPTER 11. PRACTICAL METHODOLOGY\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\n0\\n10−2 10−1 100'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='Learning rate (logarithmic scale)\\nrorre\\ngniniarT\\nFigure 11.1: Typical relationship between the learning rate and the training error. Notice\\nthe sharp rise in error when the learning is above an optimal value. This is for a fixed\\ntraining time, as a smaller learning rate may sometimes only slow down training by a\\nfactor proportional to the learning rate reduction. Generalization error can follow this\\ncurve or be complicated by regularization effects arising out of having a too large or\\ntoo small learning rates, since poor optimization can, to some degree, reduce or prevent\\noverfitting, and even points with equivalent training error can have different generalization\\nerror.\\nnow take two kinds of actions. The test error is the sum of the training error and\\nthe gap between training and test error. The optimal test error is found by trading\\noff these quantities. Neural networks typically perform best when the training'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='error is very low (and thus, when capacity is high) and the test error is primarily\\ndriven by the gap between train and test error. Your goal is to reduce this gap\\nwithout increasing training error faster than the gap decreases. To reduce the gap,\\nchange regularization hyperparameters to reduce effective model capacity, such as\\nby adding dropout or weight decay. Usually the best performance comes from a\\nlarge model that is regularized well, for example by using dropout.\\nMost hyperparameters can be set by reasoning about whether they increase or\\ndecrease model capacity. Some examples are included in Table 11.1.\\nWhile manually tuning hyperparameters, do not lose sight of your end goal:\\ngood performance on the test set. Adding regularization is only one way to achieve\\nthis goal. As long as you have low training error, you can always reduce general-\\nization error by collecting more training data. The brute force way to practically'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='guarantee success is to continually increase model capacity and training set size\\nuntil the task is solved. This approach does of course increase the computational\\ncost of training and inference, so it is only feasible given appropriate resources. In\\n430\\nCHAPTER 11. PRACTICAL METHODOLOGY\\nHyperparameter Increases Reason Caveats\\ncapacity\\nwhen...\\nNumber of hid- increased Increasing the number of Increasing the number\\nden units hidden units increases the of hidden units increases\\nrepresentational capacity both the time and memory\\nof the model. cost of essentially every op-\\neration on the model.\\nLearning rate tuned op- An improper learning rate,\\ntimally whether too high or too\\nlow, results in a model\\nwith low effective capacity\\ndue to optimization failure\\nConvolution ker- increased Increasingthekernelwidth A wider kernel results in\\nnel width increases the number of pa- a narrower output dimen-\\nrameters in the model sion, reducing model ca-\\npacity unless you use im-'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='plicit zero padding to re-\\nduce this effect. Wider\\nkernels require more mem-\\nory for parameter storage\\nand increase runtime, but\\na narrower output reduces\\nmemory cost.\\nImplicit zero increased Adding implicit zeros be- Increased time and mem-\\npadding fore convolution keeps the ory cost of most opera-\\nrepresentation size large tions.\\nWeight decay co- decreased Decreasing the weight de-\\nefficient cay coefficient frees the\\nmodel parameters to be-\\ncome larger\\nDropout rate decreased Dropping units less often\\ngives the units more oppor-\\ntunities to “conspire” with\\neach other to fit the train-\\ning set\\nTable 11.1: The effect of various hyperparameters on model capacity.\\n431\\nCHAPTER 11. PRACTICAL METHODOLOGY\\nprinciple, this approach could fail due to optimization difficulties, but for many\\nproblems optimization does not seem to be a significant barrier, provided that the\\nmodel is chosen appropriately.\\n11.4.2 Automatic Hyperparameter Optimization Algorithms'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='The ideal learning algorithm just takes a dataset and outputs a function, without\\nrequiring hand-tuning of hyperparameters. The popularity of several learning\\nalgorithms such as logistic regression and SVMs stems in part from their ability to\\nperform well with only one or two tuned hyperparameters. Neural networks can\\nsometimes perform well with only a small number of tuned hyperparameters, but\\noften benefit significantly from tuning of forty or more hyperparameters. Manual\\nhyperparameter tuning can work very well when the user has a good starting point,\\nsuch as one determined by others having worked on the same type of application\\nand architecture, or when the user has months or years of experience in exploring\\nhyperparameter values for neural networks applied to similar tasks. However,\\nfor many applications, these starting points are not available. In these cases,\\nautomated algorithms can find useful values of the hyperparameters.'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='If we think about the way in which the user of a learning algorithm searches for\\ngood values of the hyperparameters, we realize that an optimization is taking place:\\nwe are trying to find a value of the hyperparameters that optimizes an objective\\nfunction, such as validation error, sometimes under constraints (such as a budget\\nfor training time, memory or recognition time). It is therefore possible, in principle,\\nto develop hyperparameter optimization algorithms that wrap a learning\\nalgorithm and choose its hyperparameters, thus hiding the hyperparameters of the\\nlearning algorithm from the user. Unfortunately, hyperparameter optimization\\nalgorithms often have their own hyperparameters, such as the range of values that\\nshould be explored for each of the learning algorithm’s hyperparameters. However,\\nthese secondary hyperparameters are usually easier to choose, in the sense that\\nacceptable performance may be achieved on a wide range of tasks using the same'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='secondary hyperparameters for all tasks.\\n11.4.3 Grid Search\\nWhen there are three or fewer hyperparameters, the common practice is to perform\\ngrid search. For each hyperparameter, the user selects a small finite set of\\nvalues to explore. The grid search algorithm then trains a model for every joint\\nspecification of hyperparameter values in the Cartesian product of the set of values\\nfor each individual hyperparameter. The experiment that yields the best validation\\n432\\nCHAPTER 11. PRACTICAL METHODOLOGY\\nGrid Random\\nFigure 11.2: Comparison of grid search and random search. For illustration purposes we\\ndisplay two hyperparameters but weare typicallyinterestedin having manymore. (Left)To\\nperform grid search, we provide a set of values for each hyperparameter. The search\\nalgorithm runs training for every joint hyperparameter setting in the cross product of these\\nsets. (Right)To perform random search, we provide a probability distribution over joint'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='hyperparameter configurations. Usually most of these hyperparameters are independent\\nfrom each other. Common choices for the distribution over a single hyperparameter include\\nuniform and log-uniform (to sample from a log-uniform distribution, take the exp of a\\nsample from a uniform distribution). The search algorithm then randomly samples joint\\nhyperparameter configurations and runs training with each of them. Both grid search\\nand random search evaluate the validation set error and return the best configuration.\\nThe figure illustrates the typical case where only some hyperparameters have a significant\\ninfluence on the result. In this illustration, only the hyperparameter on the horizontal axis\\nhas a significant effect. Grid search wastes an amount of computation that is exponential\\nin the number of non-influential hyperparameters, while random search tests a unique\\nvalue of every influential hyperparameter on nearly every trial. Figure reproduced with'),\n",
       " Document(metadata={'filename': 'dee.txt'}, page_content='permission from Bergstra and Bengio (2012).\\n433\\nCHAPTER 11. PRACTICAL METHODOLOGY\\nset error is then chosen as having found the best hyperparameters. See the left of\\nfigure 11.2 for an illustration of a grid of hyperparameter values.\\nHow should the lists of values to search over be chosen? In the case of numerical\\n(ordered) hyperparameters, the smallest and largest element of each list is chosen\\nconservatively, based on prior experience with similar experiments, to make sure\\nthat the optimal value is very likely to be in the selected range. Typically, a grid\\nsearch involves picking values approximately on a logarithmic scale, e.g., a learning\\nrate taken within the set .1,.01,10 3,10 4,10 5 , or a number of hidden units\\n− − −\\n{ }\\ntaken with the set 50,100,200,500,1000,2000 .\\n{ }\\nGrid search usually performs best when it is performed repeatedly. For example,\\nsuppose that we ran a grid searchover a hyperparameter α using valuesof 1,0,1 .\\n{− }'),\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textsplitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e6678-36a4-48ae-bb32-5cc97e8c1e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
